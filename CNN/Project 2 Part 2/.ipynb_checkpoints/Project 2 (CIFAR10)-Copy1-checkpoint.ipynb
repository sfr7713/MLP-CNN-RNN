{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Image classification using CNN\n",
    "\n",
    "In this exercise we'll be using convolutional neural networks to classify images. At the end of this exercise you will have implemented an \"end-to-end\" network, in which the raw data is passed in with no preprocessing. This is generally desirable for production, but hard to achieve with other deep learning techniques.\n",
    "\n",
    "\n",
    "The principles behind convolutional layers applied to images is exactly the same. Images are always going to be MxN(x3), so in the future you can substitute what you've learned here with an image or video of your choice.\n",
    "\n",
    "Goals:\n",
    "- Understand reshaping\n",
    "- Understand convolutional layers\n",
    "- Understand max pool\n",
    "- Understand dimensionality\n",
    "- Implement a convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\furon\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data if you don't already have it. \n",
    "# Otherwise, keras will load it in.\n",
    "# The data will be saved in ~/.keras/datasets\n",
    "(x, y), _ = cifar10.load_data()\n",
    "x = x.astype(\"float32\") / 255  # normalize to 0-1\n",
    "# x = x.reshape(-1, x.shape[1], x.shape[2], 1).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also split it into a train and dev set \n",
    "# to evaluate how we're doing\n",
    "x_train, x_dev, y_train, y_dev = train_test_split(\n",
    "    x, y, test_size=0.05, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_vectorized = to_categorical(y_train)\n",
    "y_dev_vectorized = to_categorical(y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Let's try to build a convolutional network. Complete build_convolutional_model() in exercise_2.py and test it below. You'll notice it's a lot slower to fit (hence why we don't want you to run too many training epochs until you're sure you're using the right model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_2 import build_convolutional_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to make sure your code compiles\n",
    "conv_model = build_convolutional_model(x_train.shape, len(np.unique(y_train, axis = 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "Even though we want to avoid overfitting our models for production, purposely overfitting our model on a subset of the data is a good way to test whether our model and code are working properly. The convolutional model you implemented will likely take a while to train on the complete dataset, so we simply select the first 20 samples and evaluate on the SAME samples below. \n",
    "\n",
    "You should see accuracy approach 100% by 30 epochs. That is because you are training and testing on the same data. You want to make sure this is the case before moving on to training your full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20 samples, validate on 20 samples\n",
      "Epoch 1/25\n",
      "20/20 [==============================] - 4s 191ms/step - loss: 4.7174 - acc: 0.1000 - val_loss: 2.5742 - val_acc: 0.3000\n",
      "Epoch 2/25\n",
      "20/20 [==============================] - 0s 20ms/step - loss: 2.4958 - acc: 0.2000 - val_loss: 2.3273 - val_acc: 0.3500\n",
      "Epoch 3/25\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 1.8936 - acc: 0.3500 - val_loss: 4.5489 - val_acc: 0.3000\n",
      "Epoch 4/25\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 2.4142 - acc: 0.2500 - val_loss: 4.6675 - val_acc: 0.3000\n",
      "Epoch 5/25\n",
      "20/20 [==============================] - 1s 25ms/step - loss: 1.5271 - acc: 0.5000 - val_loss: 4.8478 - val_acc: 0.3000\n",
      "Epoch 6/25\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 0.8746 - acc: 0.6000 - val_loss: 1.7227 - val_acc: 0.6000\n",
      "Epoch 7/25\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 0.9738 - acc: 0.6500 - val_loss: 4.1753 - val_acc: 0.4000\n",
      "Epoch 8/25\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 0.8450 - acc: 0.7500 - val_loss: 1.6943 - val_acc: 0.5500\n",
      "Epoch 9/25\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 0.5781 - acc: 0.7500 - val_loss: 3.0232 - val_acc: 0.4000\n",
      "Epoch 10/25\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 0.4301 - acc: 0.8000 - val_loss: 1.5648 - val_acc: 0.5500\n",
      "Epoch 11/25\n",
      "20/20 [==============================] - 0s 25ms/step - loss: 0.5493 - acc: 0.7500 - val_loss: 2.1898 - val_acc: 0.6000\n",
      "Epoch 12/25\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 0.2059 - acc: 0.9500 - val_loss: 0.8129 - val_acc: 0.7500\n",
      "Epoch 13/25\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 0.1731 - acc: 0.9000 - val_loss: 0.8056 - val_acc: 0.7000\n",
      "Epoch 14/25\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 0.3155 - acc: 0.9500 - val_loss: 1.1080 - val_acc: 0.8000\n",
      "Epoch 15/25\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 0.2967 - acc: 0.9000 - val_loss: 1.7149 - val_acc: 0.7500\n",
      "Epoch 16/25\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 0.4699 - acc: 0.9500 - val_loss: 1.3466 - val_acc: 0.8000\n",
      "Epoch 17/25\n",
      "20/20 [==============================] - 0s 24ms/step - loss: 0.5389 - acc: 0.9000 - val_loss: 0.3951 - val_acc: 0.9000\n",
      "Epoch 18/25\n",
      "20/20 [==============================] - 0s 25ms/step - loss: 0.6447 - acc: 0.8500 - val_loss: 1.5881 - val_acc: 0.8000\n",
      "Epoch 19/25\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 0.7186 - acc: 0.9000 - val_loss: 2.8640 - val_acc: 0.6500\n",
      "Epoch 20/25\n",
      "20/20 [==============================] - 0s 23ms/step - loss: 0.6029 - acc: 0.8000 - val_loss: 0.8584 - val_acc: 0.7500\n",
      "Epoch 21/25\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 0.5527 - acc: 0.9000 - val_loss: 1.2068 - val_acc: 0.7500\n",
      "Epoch 22/25\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 0.1880 - acc: 0.9000 - val_loss: 0.9389 - val_acc: 0.7500\n",
      "Epoch 23/25\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 0.5002 - acc: 0.9500 - val_loss: 0.5463 - val_acc: 0.8000\n",
      "Epoch 24/25\n",
      "20/20 [==============================] - 1s 26ms/step - loss: 0.5059 - acc: 0.8500 - val_loss: 0.0373 - val_acc: 1.0000\n",
      "Epoch 25/25\n",
      "20/20 [==============================] - 0s 22ms/step - loss: 0.3294 - acc: 0.8500 - val_loss: 0.1093 - val_acc: 0.9500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2462c0cdb38>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model = build_convolutional_model(x_train.shape, len(np.unique(y_train, axis = 0)))\n",
    "conv_model.fit(x_train[:20], y_train_vectorized[:20],\n",
    "               validation_data=(x_train[:20], \n",
    "                                y_train_vectorized[:20]),\n",
    "               batch_size=4, epochs=25, \n",
    "               verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can train on the entire dataset. Pay attention to how the val loss value changes over epochs. If it stagnates, you may want to break the training and revisit your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47500 samples, validate on 2500 samples\n",
      "Epoch 1/10\n",
      "47500/47500 [==============================] - 572s 12ms/step - loss: 1.5158 - acc: 0.4449 - val_loss: 1.9368 - val_acc: 0.3908\n",
      "Epoch 2/10\n",
      "47500/47500 [==============================] - 577s 12ms/step - loss: 1.1236 - acc: 0.5971 - val_loss: 1.0692 - val_acc: 0.6176\n",
      "Epoch 3/10\n",
      "47500/47500 [==============================] - 583s 12ms/step - loss: 0.9552 - acc: 0.6606 - val_loss: 1.2305 - val_acc: 0.5940\n",
      "Epoch 4/10\n",
      "47500/47500 [==============================] - 573s 12ms/step - loss: 0.8464 - acc: 0.7011 - val_loss: 0.8989 - val_acc: 0.6832\n",
      "Epoch 5/10\n",
      "47500/47500 [==============================] - 586s 12ms/step - loss: 0.7746 - acc: 0.7254 - val_loss: 0.8474 - val_acc: 0.7052\n",
      "Epoch 6/10\n",
      "47500/47500 [==============================] - 667s 14ms/step - loss: 0.7336 - acc: 0.7411 - val_loss: 0.9292 - val_acc: 0.6868\n",
      "Epoch 7/10\n",
      "47500/47500 [==============================] - 659s 14ms/step - loss: 0.6945 - acc: 0.7553 - val_loss: 0.6958 - val_acc: 0.7548\n",
      "Epoch 8/10\n",
      "47500/47500 [==============================] - 656s 14ms/step - loss: 0.6585 - acc: 0.7657 - val_loss: 0.6801 - val_acc: 0.7664\n",
      "Epoch 9/10\n",
      "47500/47500 [==============================] - 661s 14ms/step - loss: 0.6321 - acc: 0.7767 - val_loss: 0.8215 - val_acc: 0.7216\n",
      "Epoch 10/10\n",
      "47500/47500 [==============================] - 618s 13ms/step - loss: 0.6033 - acc: 0.7874 - val_loss: 0.7275 - val_acc: 0.7520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2462c0cd470>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_model = build_convolutional_model(x_train.shape, len(np.unique(y_train, axis = 0)))\n",
    "conv_model.fit(x_train, y_train_vectorized,\n",
    "                validation_data=(x_dev, y_dev_vectorized),\n",
    "                batch_size=128, epochs=10, \n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47500 samples, validate on 2500 samples\n",
      "Epoch 1/10\n",
      "47500/47500 [==============================] - 582s 12ms/step - loss: 0.5898 - acc: 0.7911 - val_loss: 0.7484 - val_acc: 0.7420\n",
      "Epoch 2/10\n",
      "47500/47500 [==============================] - 574s 12ms/step - loss: 0.5672 - acc: 0.7993 - val_loss: 0.7256 - val_acc: 0.7636\n",
      "Epoch 3/10\n",
      "47500/47500 [==============================] - 579s 12ms/step - loss: 0.5506 - acc: 0.8063 - val_loss: 0.6262 - val_acc: 0.7900\n",
      "Epoch 4/10\n",
      "47500/47500 [==============================] - 586s 12ms/step - loss: 0.5388 - acc: 0.8084 - val_loss: 0.6296 - val_acc: 0.7840\n",
      "Epoch 5/10\n",
      "47500/47500 [==============================] - 578s 12ms/step - loss: 0.5234 - acc: 0.8123 - val_loss: 0.6504 - val_acc: 0.7872\n",
      "Epoch 6/10\n",
      "47500/47500 [==============================] - 578s 12ms/step - loss: 0.5165 - acc: 0.8167 - val_loss: 0.6191 - val_acc: 0.7892\n",
      "Epoch 7/10\n",
      "47500/47500 [==============================] - 618s 13ms/step - loss: 0.4989 - acc: 0.8232 - val_loss: 0.6801 - val_acc: 0.7804\n",
      "Epoch 8/10\n",
      "47500/47500 [==============================] - 631s 13ms/step - loss: 0.4873 - acc: 0.8276 - val_loss: 0.7114 - val_acc: 0.7752\n",
      "Epoch 9/10\n",
      "47500/47500 [==============================] - 646s 14ms/step - loss: 0.4774 - acc: 0.8295 - val_loss: 0.5893 - val_acc: 0.8020\n",
      "Epoch 10/10\n",
      "47500/47500 [==============================] - 631s 13ms/step - loss: 0.4683 - acc: 0.8325 - val_loss: 0.6522 - val_acc: 0.7876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2462c0cd550>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run additional training here. \n",
    "# Feel free to modify this cell\n",
    "conv_model.fit(x_train, y_train_vectorized,\n",
    "                validation_data=(x_dev, y_dev_vectorized),\n",
    "                batch_size=128, epochs=10, \n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you choose an architecture that breaks 65% below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of exercise_2 failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\furon\\Anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 244, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\furon\\Anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 374, in superreload\n",
      "    module = reload(module)\n",
      "  File \"C:\\Users\\furon\\Anaconda3\\lib\\imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"C:\\Users\\furon\\Anaconda3\\lib\\importlib\\__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 674, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 781, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 741, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"C:\\Users\\furon\\Desktop\\Project 2 Part 2\\Project 2 Part 2\\exercise_2.py\", line 46\n",
      "    model.add(Dropout(dropouts[4]))\n",
      "        ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 8s 3ms/step\n",
      "Dev loss: 0.6522\n",
      "Dev acc: 78.76%\n"
     ]
    }
   ],
   "source": [
    "final_dev_accuracy = conv_model.evaluate(x_dev, y_dev_vectorized,\n",
    "                batch_size=128, verbose=1)\n",
    "print(\"Dev loss: {:.4f}\\nDev acc: {:.2f}%\".format(final_dev_accuracy[0],\n",
    "                                         100*final_dev_accuracy[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to generate your output\n",
    "_, (test, _) = cifar10.load_data()\n",
    "test = test.astype(\"float32\") / 255  # normalize to 0-1\n",
    "predicted = conv_model.predict_classes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7801"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(_,predicted  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'             precision    recall  f1-score   support\\n\\n          0       0.81      0.77      0.79      1000\\n          1       0.89      0.89      0.89      1000\\n          2       0.78      0.60      0.68      1000\\n          3       0.64      0.60      0.62      1000\\n          4       0.81      0.75      0.78      1000\\n          5       0.69      0.73      0.71      1000\\n          6       0.77      0.86      0.82      1000\\n          7       0.85      0.79      0.82      1000\\n          8       0.72      0.94      0.82      1000\\n          9       0.85      0.86      0.85      1000\\n\\navg / total       0.78      0.78      0.78     10000\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(_,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"exercise_2_output.txt\", \"w\") as f:\n",
    "    [f.write(\"{}\\n\".format(p)) for p in predicted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Please write up an explanation of your model. We would like you to explain:\n",
    "\n",
    "1. The architecture of your model (number of layers, the types of layers, etc) with a justification for each\n",
    "2. Any other techniques or layers you included and why\n",
    "3. How you might improve your model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(double click to edit) \n",
    "\n",
    "Submit your write up here or in a separate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
