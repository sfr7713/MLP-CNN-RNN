{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP -  Predicting diabetes using the NHANES dataset\n",
    "\n",
    "Dataset:\n",
    "The National Health and Nutrition Examination Survey (NHANES), administered annually by the National Center for Health Statistics, is designed to assess the general health and nutritional status of adults and children in the United States.\n",
    "\n",
    "\n",
    "Data: https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=2013\n",
    "\n",
    "\n",
    "Goals:\n",
    "- refresh neural network implementation\n",
    "- handle an imbalanced dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection as ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing the dataset\n",
    "read the SAS files into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import merge_xpt, get_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/Users/furon/Desktop/Project 1/Data/nhanes\"\n",
    "df = get_training_data(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2507, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Age</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>Race</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>D1Sugar</th>\n",
       "      <th>D1Carb</th>\n",
       "      <th>D2Sugar</th>\n",
       "      <th>D2Carb</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>SleepHours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2507.000000</td>\n",
       "      <td>2507.000000</td>\n",
       "      <td>2444.000000</td>\n",
       "      <td>2507.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2318.000000</td>\n",
       "      <td>2318.000000</td>\n",
       "      <td>2067.000000</td>\n",
       "      <td>2067.000000</td>\n",
       "      <td>1930.000000</td>\n",
       "      <td>2506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.919426</td>\n",
       "      <td>48.035899</td>\n",
       "      <td>75.344239</td>\n",
       "      <td>3.286797</td>\n",
       "      <td>187.024970</td>\n",
       "      <td>108.484292</td>\n",
       "      <td>251.030453</td>\n",
       "      <td>101.030982</td>\n",
       "      <td>237.012772</td>\n",
       "      <td>3.496891</td>\n",
       "      <td>6.922985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.364925</td>\n",
       "      <td>18.252254</td>\n",
       "      <td>113.087851</td>\n",
       "      <td>1.490164</td>\n",
       "      <td>41.496019</td>\n",
       "      <td>75.073624</td>\n",
       "      <td>126.535489</td>\n",
       "      <td>65.456716</td>\n",
       "      <td>116.708696</td>\n",
       "      <td>13.739268</td>\n",
       "      <td>1.403750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>8.670000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>34.305000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>57.390000</td>\n",
       "      <td>164.557500</td>\n",
       "      <td>54.570000</td>\n",
       "      <td>158.240000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>53.460000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>93.450000</td>\n",
       "      <td>231.080000</td>\n",
       "      <td>88.420000</td>\n",
       "      <td>220.880000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>86.280000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>141.022500</td>\n",
       "      <td>311.895000</td>\n",
       "      <td>132.055000</td>\n",
       "      <td>296.170000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4094.880000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>612.000000</td>\n",
       "      <td>719.580000</td>\n",
       "      <td>1300.720000</td>\n",
       "      <td>457.410000</td>\n",
       "      <td>1041.560000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Diabetes          Age      Insulin         Race  Cholesterol  \\\n",
       "count  2507.000000  2507.000000  2444.000000  2507.000000  2483.000000   \n",
       "mean      1.919426    48.035899    75.344239     3.286797   187.024970   \n",
       "std       0.364925    18.252254   113.087851     1.490164    41.496019   \n",
       "min       1.000000    18.000000     0.840000     1.000000    69.000000   \n",
       "25%       2.000000    33.000000    34.305000     3.000000   159.000000   \n",
       "50%       2.000000    48.000000    53.460000     3.000000   183.000000   \n",
       "75%       2.000000    63.000000    86.280000     4.000000   211.000000   \n",
       "max       3.000000    80.000000  4094.880000     7.000000   612.000000   \n",
       "\n",
       "           D1Sugar       D1Carb      D2Sugar       D2Carb      Alcohol  \\\n",
       "count  2318.000000  2318.000000  2067.000000  2067.000000  1930.000000   \n",
       "mean    108.484292   251.030453   101.030982   237.012772     3.496891   \n",
       "std      75.073624   126.535489    65.456716   116.708696    13.739268   \n",
       "min       0.130000     8.670000     0.000000     0.000000     0.000000   \n",
       "25%      57.390000   164.557500    54.570000   158.240000     1.000000   \n",
       "50%      93.450000   231.080000    88.420000   220.880000     2.000000   \n",
       "75%     141.022500   311.895000   132.055000   296.170000     4.000000   \n",
       "max     719.580000  1300.720000   457.410000  1041.560000   365.000000   \n",
       "\n",
       "        SleepHours  \n",
       "count  2506.000000  \n",
       "mean      6.922985  \n",
       "std       1.403750  \n",
       "min       2.000000  \n",
       "25%       6.000000  \n",
       "50%       7.000000  \n",
       "75%       8.000000  \n",
       "max      12.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    2157\n",
       "1.0     276\n",
       "3.0      74\n",
       "Name: Diabetes, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Diabetes\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Clean Data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_1 import clean_data_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_data_and_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(df.Diabetes < 3), \"Not all labels are < 3.0\"\n",
    "assert np.all(df.count() == len(df)), \"There are still NaNs in your data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_1 import build_model, split_x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from exercise_1 import preprocess_dataset\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Age</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>Race</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>D1Sugar</th>\n",
       "      <th>D1Carb</th>\n",
       "      <th>D2Sugar</th>\n",
       "      <th>D2Carb</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>SleepHours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1571.000000</td>\n",
       "      <td>1.571000e+03</td>\n",
       "      <td>1.571000e+03</td>\n",
       "      <td>1.571000e+03</td>\n",
       "      <td>1.571000e+03</td>\n",
       "      <td>1.571000e+03</td>\n",
       "      <td>1.571000e+03</td>\n",
       "      <td>1.571000e+03</td>\n",
       "      <td>1.571000e+03</td>\n",
       "      <td>1.571000e+03</td>\n",
       "      <td>1.571000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.886696</td>\n",
       "      <td>3.957510e-17</td>\n",
       "      <td>3.180142e-19</td>\n",
       "      <td>7.045782e-17</td>\n",
       "      <td>2.953027e-16</td>\n",
       "      <td>-5.144764e-17</td>\n",
       "      <td>-1.639540e-16</td>\n",
       "      <td>9.893776e-19</td>\n",
       "      <td>-3.136327e-16</td>\n",
       "      <td>9.178244e-17</td>\n",
       "      <td>3.083943e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.317065</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>1.000318e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.739497e+00</td>\n",
       "      <td>-5.745647e-01</td>\n",
       "      <td>-1.579568e+00</td>\n",
       "      <td>-2.635517e+00</td>\n",
       "      <td>-1.450440e+00</td>\n",
       "      <td>-1.914392e+00</td>\n",
       "      <td>-1.546781e+00</td>\n",
       "      <td>-2.011794e+00</td>\n",
       "      <td>-2.410468e-01</td>\n",
       "      <td>-3.524974e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>-8.323412e-01</td>\n",
       "      <td>-3.234003e-01</td>\n",
       "      <td>-1.484831e-01</td>\n",
       "      <td>-6.906737e-01</td>\n",
       "      <td>-6.755969e-01</td>\n",
       "      <td>-6.733057e-01</td>\n",
       "      <td>-7.154043e-01</td>\n",
       "      <td>-6.813619e-01</td>\n",
       "      <td>-1.749010e-01</td>\n",
       "      <td>-6.324515e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.811713e-02</td>\n",
       "      <td>-1.781568e-01</td>\n",
       "      <td>-1.484831e-01</td>\n",
       "      <td>-9.226017e-02</td>\n",
       "      <td>-2.055161e-01</td>\n",
       "      <td>-1.494444e-01</td>\n",
       "      <td>-1.953792e-01</td>\n",
       "      <td>-1.421857e-01</td>\n",
       "      <td>-1.087553e-01</td>\n",
       "      <td>9.067900e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.118782e-01</td>\n",
       "      <td>6.096074e-02</td>\n",
       "      <td>5.670596e-01</td>\n",
       "      <td>6.058889e-01</td>\n",
       "      <td>4.108687e-01</td>\n",
       "      <td>4.596095e-01</td>\n",
       "      <td>4.954747e-01</td>\n",
       "      <td>5.034671e-01</td>\n",
       "      <td>-9.536612e-03</td>\n",
       "      <td>8.138095e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.775731e+00</td>\n",
       "      <td>3.044435e+01</td>\n",
       "      <td>2.713688e+00</td>\n",
       "      <td>3.622890e+00</td>\n",
       "      <td>7.948563e+00</td>\n",
       "      <td>8.065041e+00</td>\n",
       "      <td>5.349847e+00</td>\n",
       "      <td>6.724060e+00</td>\n",
       "      <td>2.390216e+01</td>\n",
       "      <td>3.706332e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Diabetes           Age       Insulin          Race   Cholesterol  \\\n",
       "count  1571.000000  1.571000e+03  1.571000e+03  1.571000e+03  1.571000e+03   \n",
       "mean      1.886696  3.957510e-17  3.180142e-19  7.045782e-17  2.953027e-16   \n",
       "std       0.317065  1.000318e+00  1.000318e+00  1.000318e+00  1.000318e+00   \n",
       "min       1.000000 -1.739497e+00 -5.745647e-01 -1.579568e+00 -2.635517e+00   \n",
       "25%       2.000000 -8.323412e-01 -3.234003e-01 -1.484831e-01 -6.906737e-01   \n",
       "50%       2.000000  1.811713e-02 -1.781568e-01 -1.484831e-01 -9.226017e-02   \n",
       "75%       2.000000  8.118782e-01  6.096074e-02  5.670596e-01  6.058889e-01   \n",
       "max       2.000000  1.775731e+00  3.044435e+01  2.713688e+00  3.622890e+00   \n",
       "\n",
       "            D1Sugar        D1Carb       D2Sugar        D2Carb       Alcohol  \\\n",
       "count  1.571000e+03  1.571000e+03  1.571000e+03  1.571000e+03  1.571000e+03   \n",
       "mean  -5.144764e-17 -1.639540e-16  9.893776e-19 -3.136327e-16  9.178244e-17   \n",
       "std    1.000318e+00  1.000318e+00  1.000318e+00  1.000318e+00  1.000318e+00   \n",
       "min   -1.450440e+00 -1.914392e+00 -1.546781e+00 -2.011794e+00 -2.410468e-01   \n",
       "25%   -6.755969e-01 -6.733057e-01 -7.154043e-01 -6.813619e-01 -1.749010e-01   \n",
       "50%   -2.055161e-01 -1.494444e-01 -1.953792e-01 -1.421857e-01 -1.087553e-01   \n",
       "75%    4.108687e-01  4.596095e-01  4.954747e-01  5.034671e-01 -9.536612e-03   \n",
       "max    7.948563e+00  8.065041e+00  5.349847e+00  6.724060e+00  2.390216e+01   \n",
       "\n",
       "         SleepHours  \n",
       "count  1.571000e+03  \n",
       "mean   3.083943e-16  \n",
       "std    1.000318e+00  \n",
       "min   -3.524974e+00  \n",
       "25%   -6.324515e-01  \n",
       "50%    9.067900e-02  \n",
       "75%    8.138095e-01  \n",
       "max    3.706332e+00  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocess_dataset(df)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.shape[1] == 11, \\\n",
    "\"Number of features is off, make sure you didn't remove or add any\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cells below to fit your model and as scratch (perhaps to view the dataset distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    1393\n",
       "1.0     178\n",
       "Name: Diabetes, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Diabetes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_x_y(df)  \n",
    "X_train, X_vali, y_train, y_vali = train_test_split(X, y, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1256 samples, validate on 315 samples\n",
      "Epoch 1/200\n",
      "1256/1256 [==============================] - 1s 875us/step - loss: 1.2768 - acc: 0.5279 - val_loss: 0.6897 - val_acc: 0.6032\n",
      "Epoch 2/200\n",
      "1256/1256 [==============================] - 0s 82us/step - loss: 1.2738 - acc: 0.5231 - val_loss: 0.6916 - val_acc: 0.6127\n",
      "Epoch 3/200\n",
      "1256/1256 [==============================] - 0s 88us/step - loss: 1.2493 - acc: 0.5287 - val_loss: 0.6884 - val_acc: 0.6952\n",
      "Epoch 4/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.2502 - acc: 0.5119 - val_loss: 0.6906 - val_acc: 0.6730\n",
      "Epoch 5/200\n",
      "1256/1256 [==============================] - 0s 71us/step - loss: 1.2565 - acc: 0.5525 - val_loss: 0.6890 - val_acc: 0.6222\n",
      "Epoch 6/200\n",
      "1256/1256 [==============================] - 0s 72us/step - loss: 1.2632 - acc: 0.4904 - val_loss: 0.6996 - val_acc: 0.5270\n",
      "Epoch 7/200\n",
      "1256/1256 [==============================] - 0s 74us/step - loss: 1.2412 - acc: 0.5151 - val_loss: 0.6902 - val_acc: 0.6508\n",
      "Epoch 8/200\n",
      "1256/1256 [==============================] - 0s 77us/step - loss: 1.2242 - acc: 0.5311 - val_loss: 0.6829 - val_acc: 0.6698\n",
      "Epoch 9/200\n",
      "1256/1256 [==============================] - 0s 86us/step - loss: 1.2304 - acc: 0.5677 - val_loss: 0.6793 - val_acc: 0.6635\n",
      "Epoch 10/200\n",
      "1256/1256 [==============================] - 0s 78us/step - loss: 1.1885 - acc: 0.5796 - val_loss: 0.6718 - val_acc: 0.7079\n",
      "Epoch 11/200\n",
      "1256/1256 [==============================] - 0s 84us/step - loss: 1.2196 - acc: 0.6011 - val_loss: 0.6645 - val_acc: 0.7206\n",
      "Epoch 12/200\n",
      "1256/1256 [==============================] - 0s 88us/step - loss: 1.2339 - acc: 0.5772 - val_loss: 0.6621 - val_acc: 0.7333\n",
      "Epoch 13/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 1.2091 - acc: 0.5788 - val_loss: 0.6567 - val_acc: 0.7460\n",
      "Epoch 14/200\n",
      "1256/1256 [==============================] - 0s 91us/step - loss: 1.2272 - acc: 0.5725 - val_loss: 0.6599 - val_acc: 0.7429\n",
      "Epoch 15/200\n",
      "1256/1256 [==============================] - 0s 97us/step - loss: 1.2036 - acc: 0.5772 - val_loss: 0.6607 - val_acc: 0.7238\n",
      "Epoch 16/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.2296 - acc: 0.5892 - val_loss: 0.6542 - val_acc: 0.7270\n",
      "Epoch 17/200\n",
      "1256/1256 [==============================] - 0s 86us/step - loss: 1.2036 - acc: 0.5812 - val_loss: 0.6529 - val_acc: 0.7397\n",
      "Epoch 18/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 1.2173 - acc: 0.6178 - val_loss: 0.6445 - val_acc: 0.7492\n",
      "Epoch 19/200\n",
      "1256/1256 [==============================] - 0s 80us/step - loss: 1.2098 - acc: 0.6099 - val_loss: 0.6435 - val_acc: 0.7460\n",
      "Epoch 20/200\n",
      "1256/1256 [==============================] - 0s 81us/step - loss: 1.1606 - acc: 0.6194 - val_loss: 0.6267 - val_acc: 0.7683\n",
      "Epoch 21/200\n",
      "1256/1256 [==============================] - 0s 79us/step - loss: 1.1806 - acc: 0.6250 - val_loss: 0.6290 - val_acc: 0.7556\n",
      "Epoch 22/200\n",
      "1256/1256 [==============================] - 0s 76us/step - loss: 1.1627 - acc: 0.6242 - val_loss: 0.6225 - val_acc: 0.7460\n",
      "Epoch 23/200\n",
      "1256/1256 [==============================] - 0s 81us/step - loss: 1.2181 - acc: 0.6091 - val_loss: 0.6311 - val_acc: 0.7333\n",
      "Epoch 24/200\n",
      "1256/1256 [==============================] - 0s 73us/step - loss: 1.1503 - acc: 0.6282 - val_loss: 0.6278 - val_acc: 0.7206\n",
      "Epoch 25/200\n",
      "1256/1256 [==============================] - 0s 71us/step - loss: 1.1399 - acc: 0.6369 - val_loss: 0.6227 - val_acc: 0.7302\n",
      "Epoch 26/200\n",
      "1256/1256 [==============================] - 0s 77us/step - loss: 1.1468 - acc: 0.6226 - val_loss: 0.6165 - val_acc: 0.7460\n",
      "Epoch 27/200\n",
      "1256/1256 [==============================] - 0s 78us/step - loss: 1.1538 - acc: 0.6393 - val_loss: 0.6139 - val_acc: 0.7429\n",
      "Epoch 28/200\n",
      "1256/1256 [==============================] - 0s 75us/step - loss: 1.1949 - acc: 0.6298 - val_loss: 0.6254 - val_acc: 0.7365\n",
      "Epoch 29/200\n",
      "1256/1256 [==============================] - 0s 75us/step - loss: 1.1567 - acc: 0.6481 - val_loss: 0.6170 - val_acc: 0.7333\n",
      "Epoch 30/200\n",
      "1256/1256 [==============================] - 0s 77us/step - loss: 1.1462 - acc: 0.6592 - val_loss: 0.6074 - val_acc: 0.7492\n",
      "Epoch 31/200\n",
      "1256/1256 [==============================] - 0s 75us/step - loss: 1.1573 - acc: 0.6672 - val_loss: 0.6116 - val_acc: 0.7492\n",
      "Epoch 32/200\n",
      "1256/1256 [==============================] - 0s 72us/step - loss: 1.1406 - acc: 0.6537 - val_loss: 0.6069 - val_acc: 0.7524\n",
      "Epoch 33/200\n",
      "1256/1256 [==============================] - 0s 77us/step - loss: 1.1103 - acc: 0.6664 - val_loss: 0.6006 - val_acc: 0.7587\n",
      "Epoch 34/200\n",
      "1256/1256 [==============================] - 0s 77us/step - loss: 1.1183 - acc: 0.6576 - val_loss: 0.5996 - val_acc: 0.7460\n",
      "Epoch 35/200\n",
      "1256/1256 [==============================] - 0s 84us/step - loss: 1.1093 - acc: 0.6744 - val_loss: 0.5860 - val_acc: 0.7556\n",
      "Epoch 36/200\n",
      "1256/1256 [==============================] - 0s 82us/step - loss: 1.1220 - acc: 0.6584 - val_loss: 0.5811 - val_acc: 0.7619\n",
      "Epoch 37/200\n",
      "1256/1256 [==============================] - 0s 85us/step - loss: 1.1085 - acc: 0.6775 - val_loss: 0.5819 - val_acc: 0.7587\n",
      "Epoch 38/200\n",
      "1256/1256 [==============================] - 0s 84us/step - loss: 1.1166 - acc: 0.6775 - val_loss: 0.5823 - val_acc: 0.7460\n",
      "Epoch 39/200\n",
      "1256/1256 [==============================] - 0s 79us/step - loss: 1.1409 - acc: 0.6712 - val_loss: 0.5786 - val_acc: 0.7460\n",
      "Epoch 40/200\n",
      "1256/1256 [==============================] - 0s 91us/step - loss: 1.1144 - acc: 0.6592 - val_loss: 0.5762 - val_acc: 0.7460\n",
      "Epoch 41/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 1.1506 - acc: 0.6537 - val_loss: 0.5809 - val_acc: 0.7333\n",
      "Epoch 42/200\n",
      "1256/1256 [==============================] - 0s 75us/step - loss: 1.1268 - acc: 0.6481 - val_loss: 0.5731 - val_acc: 0.7556\n",
      "Epoch 43/200\n",
      "1256/1256 [==============================] - 0s 90us/step - loss: 1.1050 - acc: 0.6489 - val_loss: 0.5756 - val_acc: 0.7397\n",
      "Epoch 44/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 1.1160 - acc: 0.6457 - val_loss: 0.5731 - val_acc: 0.7429\n",
      "Epoch 45/200\n",
      "1256/1256 [==============================] - 0s 84us/step - loss: 1.0729 - acc: 0.6688 - val_loss: 0.5708 - val_acc: 0.7238\n",
      "Epoch 46/200\n",
      "1256/1256 [==============================] - 0s 84us/step - loss: 1.1115 - acc: 0.6568 - val_loss: 0.5738 - val_acc: 0.7175\n",
      "Epoch 47/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 1.0497 - acc: 0.6409 - val_loss: 0.5640 - val_acc: 0.7175\n",
      "Epoch 48/200\n",
      "1256/1256 [==============================] - 0s 91us/step - loss: 1.0814 - acc: 0.6783 - val_loss: 0.5676 - val_acc: 0.7143\n",
      "Epoch 49/200\n",
      "1256/1256 [==============================] - 0s 83us/step - loss: 1.1146 - acc: 0.6545 - val_loss: 0.5659 - val_acc: 0.7175\n",
      "Epoch 50/200\n",
      "1256/1256 [==============================] - 0s 85us/step - loss: 1.0697 - acc: 0.6807 - val_loss: 0.5775 - val_acc: 0.7016\n",
      "Epoch 51/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 1.1059 - acc: 0.6592 - val_loss: 0.5725 - val_acc: 0.6984\n",
      "Epoch 52/200\n",
      "1256/1256 [==============================] - 0s 92us/step - loss: 1.0733 - acc: 0.6680 - val_loss: 0.5639 - val_acc: 0.6984\n",
      "Epoch 53/200\n",
      "1256/1256 [==============================] - 0s 92us/step - loss: 1.0817 - acc: 0.6513 - val_loss: 0.5657 - val_acc: 0.7016\n",
      "Epoch 54/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 1.0719 - acc: 0.6449 - val_loss: 0.5643 - val_acc: 0.6952\n",
      "Epoch 55/200\n",
      "1256/1256 [==============================] - 0s 98us/step - loss: 1.0864 - acc: 0.6561 - val_loss: 0.5629 - val_acc: 0.6921\n",
      "Epoch 56/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 1.0332 - acc: 0.6688 - val_loss: 0.5551 - val_acc: 0.7016\n",
      "Epoch 57/200\n",
      "1256/1256 [==============================] - 0s 93us/step - loss: 1.0861 - acc: 0.6648 - val_loss: 0.5519 - val_acc: 0.7048\n",
      "Epoch 58/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 1.0508 - acc: 0.6656 - val_loss: 0.5568 - val_acc: 0.6921\n",
      "Epoch 59/200\n",
      "1256/1256 [==============================] - 0s 109us/step - loss: 1.0584 - acc: 0.6656 - val_loss: 0.5552 - val_acc: 0.6952\n",
      "Epoch 60/200\n",
      "1256/1256 [==============================] - 0s 85us/step - loss: 1.0472 - acc: 0.6688 - val_loss: 0.5425 - val_acc: 0.7048\n",
      "Epoch 61/200\n",
      "1256/1256 [==============================] - 0s 75us/step - loss: 1.0606 - acc: 0.6712 - val_loss: 0.5404 - val_acc: 0.7016\n",
      "Epoch 62/200\n",
      "1256/1256 [==============================] - 0s 72us/step - loss: 1.0657 - acc: 0.6688 - val_loss: 0.5447 - val_acc: 0.6952\n",
      "Epoch 63/200\n",
      "1256/1256 [==============================] - 0s 71us/step - loss: 1.0363 - acc: 0.6584 - val_loss: 0.5375 - val_acc: 0.6952\n",
      "Epoch 64/200\n",
      "1256/1256 [==============================] - 0s 75us/step - loss: 1.0880 - acc: 0.6521 - val_loss: 0.5426 - val_acc: 0.6921\n",
      "Epoch 65/200\n",
      "1256/1256 [==============================] - 0s 65us/step - loss: 1.0746 - acc: 0.6624 - val_loss: 0.5427 - val_acc: 0.6984\n",
      "Epoch 66/200\n",
      "1256/1256 [==============================] - 0s 74us/step - loss: 1.0587 - acc: 0.6760 - val_loss: 0.5462 - val_acc: 0.6984\n",
      "Epoch 67/200\n",
      "1256/1256 [==============================] - 0s 84us/step - loss: 1.0752 - acc: 0.6553 - val_loss: 0.5543 - val_acc: 0.6698\n",
      "Epoch 68/200\n",
      "1256/1256 [==============================] - 0s 79us/step - loss: 1.0494 - acc: 0.6807 - val_loss: 0.5415 - val_acc: 0.6952\n",
      "Epoch 69/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.0260 - acc: 0.6664 - val_loss: 0.5452 - val_acc: 0.6825\n",
      "Epoch 70/200\n",
      "1256/1256 [==============================] - 0s 156us/step - loss: 1.0312 - acc: 0.6783 - val_loss: 0.5350 - val_acc: 0.6921\n",
      "Epoch 71/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.0474 - acc: 0.6664 - val_loss: 0.5341 - val_acc: 0.6921\n",
      "Epoch 72/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 1.0415 - acc: 0.6584 - val_loss: 0.5439 - val_acc: 0.6730\n",
      "Epoch 73/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 1.0036 - acc: 0.6799 - val_loss: 0.5366 - val_acc: 0.6857\n",
      "Epoch 74/200\n",
      "1256/1256 [==============================] - 0s 80us/step - loss: 1.0892 - acc: 0.6672 - val_loss: 0.5341 - val_acc: 0.6857\n",
      "Epoch 75/200\n",
      "1256/1256 [==============================] - 0s 86us/step - loss: 1.0133 - acc: 0.6744 - val_loss: 0.5369 - val_acc: 0.6825\n",
      "Epoch 76/200\n",
      "1256/1256 [==============================] - 0s 103us/step - loss: 1.0263 - acc: 0.6648 - val_loss: 0.5307 - val_acc: 0.7048\n",
      "Epoch 77/200\n",
      "1256/1256 [==============================] - 0s 116us/step - loss: 1.0568 - acc: 0.6768 - val_loss: 0.5308 - val_acc: 0.6952\n",
      "Epoch 78/200\n",
      "1256/1256 [==============================] - 0s 123us/step - loss: 1.0533 - acc: 0.6537 - val_loss: 0.5400 - val_acc: 0.6921\n",
      "Epoch 79/200\n",
      "1256/1256 [==============================] - 0s 126us/step - loss: 1.0142 - acc: 0.6855 - val_loss: 0.5309 - val_acc: 0.6889\n",
      "Epoch 80/200\n",
      "1256/1256 [==============================] - 0s 101us/step - loss: 1.0341 - acc: 0.6728 - val_loss: 0.5415 - val_acc: 0.6730\n",
      "Epoch 81/200\n",
      "1256/1256 [==============================] - 0s 75us/step - loss: 1.0108 - acc: 0.6720 - val_loss: 0.5328 - val_acc: 0.6952\n",
      "Epoch 82/200\n",
      "1256/1256 [==============================] - 0s 71us/step - loss: 0.9989 - acc: 0.6736 - val_loss: 0.5274 - val_acc: 0.6952\n",
      "Epoch 83/200\n",
      "1256/1256 [==============================] - 0s 73us/step - loss: 1.0092 - acc: 0.6831 - val_loss: 0.5376 - val_acc: 0.6698\n",
      "Epoch 84/200\n",
      "1256/1256 [==============================] - 0s 72us/step - loss: 0.9916 - acc: 0.6839 - val_loss: 0.5300 - val_acc: 0.6825\n",
      "Epoch 85/200\n",
      "1256/1256 [==============================] - 0s 74us/step - loss: 1.0142 - acc: 0.6696 - val_loss: 0.5358 - val_acc: 0.6635\n",
      "Epoch 86/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 1.0262 - acc: 0.6624 - val_loss: 0.5377 - val_acc: 0.6540\n",
      "Epoch 87/200\n",
      "1256/1256 [==============================] - 0s 84us/step - loss: 1.0479 - acc: 0.6505 - val_loss: 0.5324 - val_acc: 0.6603\n",
      "Epoch 88/200\n",
      "1256/1256 [==============================] - 0s 79us/step - loss: 1.0205 - acc: 0.6736 - val_loss: 0.5343 - val_acc: 0.6444\n",
      "Epoch 89/200\n",
      "1256/1256 [==============================] - 0s 82us/step - loss: 1.0493 - acc: 0.6592 - val_loss: 0.5278 - val_acc: 0.6540\n",
      "Epoch 90/200\n",
      "1256/1256 [==============================] - 0s 83us/step - loss: 1.0439 - acc: 0.6656 - val_loss: 0.5249 - val_acc: 0.6667\n",
      "Epoch 91/200\n",
      "1256/1256 [==============================] - 0s 78us/step - loss: 1.0519 - acc: 0.6664 - val_loss: 0.5256 - val_acc: 0.6762\n",
      "Epoch 92/200\n",
      "1256/1256 [==============================] - 0s 79us/step - loss: 1.0428 - acc: 0.6616 - val_loss: 0.5325 - val_acc: 0.6540\n",
      "Epoch 93/200\n",
      "1256/1256 [==============================] - 0s 77us/step - loss: 1.0342 - acc: 0.6791 - val_loss: 0.5349 - val_acc: 0.6476\n",
      "Epoch 94/200\n",
      "1256/1256 [==============================] - 0s 103us/step - loss: 1.0337 - acc: 0.6648 - val_loss: 0.5320 - val_acc: 0.6444\n",
      "Epoch 95/200\n",
      "1256/1256 [==============================] - 0s 111us/step - loss: 1.0392 - acc: 0.6529 - val_loss: 0.5316 - val_acc: 0.6476\n",
      "Epoch 96/200\n",
      "1256/1256 [==============================] - 0s 80us/step - loss: 1.0385 - acc: 0.6672 - val_loss: 0.5307 - val_acc: 0.6508\n",
      "Epoch 97/200\n",
      "1256/1256 [==============================] - 0s 72us/step - loss: 1.0481 - acc: 0.6768 - val_loss: 0.5239 - val_acc: 0.6762\n",
      "Epoch 98/200\n",
      "1256/1256 [==============================] - 0s 72us/step - loss: 1.0061 - acc: 0.6791 - val_loss: 0.5200 - val_acc: 0.6730\n",
      "Epoch 99/200\n",
      "1256/1256 [==============================] - 0s 74us/step - loss: 1.0248 - acc: 0.6648 - val_loss: 0.5219 - val_acc: 0.6762\n",
      "Epoch 100/200\n",
      "1256/1256 [==============================] - 0s 69us/step - loss: 0.9977 - acc: 0.6768 - val_loss: 0.5085 - val_acc: 0.7111\n",
      "Epoch 101/200\n",
      "1256/1256 [==============================] - 0s 68us/step - loss: 1.0036 - acc: 0.6855 - val_loss: 0.5232 - val_acc: 0.6698\n",
      "Epoch 102/200\n",
      "1256/1256 [==============================] - 0s 67us/step - loss: 1.0175 - acc: 0.7006 - val_loss: 0.5160 - val_acc: 0.6984\n",
      "Epoch 103/200\n",
      "1256/1256 [==============================] - 0s 71us/step - loss: 0.9835 - acc: 0.6815 - val_loss: 0.5141 - val_acc: 0.7016\n",
      "Epoch 104/200\n",
      "1256/1256 [==============================] - 0s 70us/step - loss: 0.9860 - acc: 0.6879 - val_loss: 0.5131 - val_acc: 0.6984\n",
      "Epoch 105/200\n",
      "1256/1256 [==============================] - 0s 76us/step - loss: 0.9961 - acc: 0.6919 - val_loss: 0.5143 - val_acc: 0.6952\n",
      "Epoch 106/200\n",
      "1256/1256 [==============================] - 0s 98us/step - loss: 0.9988 - acc: 0.7158 - val_loss: 0.5099 - val_acc: 0.7079\n",
      "Epoch 107/200\n",
      "1256/1256 [==============================] - 0s 107us/step - loss: 0.9852 - acc: 0.6982 - val_loss: 0.5119 - val_acc: 0.6825\n",
      "Epoch 108/200\n",
      "1256/1256 [==============================] - 0s 92us/step - loss: 0.9869 - acc: 0.6863 - val_loss: 0.5081 - val_acc: 0.7206\n",
      "Epoch 109/200\n",
      "1256/1256 [==============================] - 0s 91us/step - loss: 0.9654 - acc: 0.6783 - val_loss: 0.5031 - val_acc: 0.7333\n",
      "Epoch 110/200\n",
      "1256/1256 [==============================] - 0s 86us/step - loss: 0.9793 - acc: 0.6982 - val_loss: 0.5003 - val_acc: 0.7333\n",
      "Epoch 111/200\n",
      "1256/1256 [==============================] - 0s 84us/step - loss: 1.0349 - acc: 0.6815 - val_loss: 0.5172 - val_acc: 0.6730\n",
      "Epoch 112/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.0294 - acc: 0.6672 - val_loss: 0.5120 - val_acc: 0.6889\n",
      "Epoch 113/200\n",
      "1256/1256 [==============================] - 0s 83us/step - loss: 1.0352 - acc: 0.6879 - val_loss: 0.5182 - val_acc: 0.6698\n",
      "Epoch 114/200\n",
      "1256/1256 [==============================] - 0s 75us/step - loss: 1.0095 - acc: 0.6696 - val_loss: 0.5232 - val_acc: 0.6540\n",
      "Epoch 115/200\n",
      "1256/1256 [==============================] - 0s 81us/step - loss: 0.9616 - acc: 0.6935 - val_loss: 0.5137 - val_acc: 0.6730\n",
      "Epoch 116/200\n",
      "1256/1256 [==============================] - 0s 76us/step - loss: 1.0249 - acc: 0.6656 - val_loss: 0.5175 - val_acc: 0.6730\n",
      "Epoch 117/200\n",
      "1256/1256 [==============================] - 0s 71us/step - loss: 1.0244 - acc: 0.6760 - val_loss: 0.5149 - val_acc: 0.6667\n",
      "Epoch 118/200\n",
      "1256/1256 [==============================] - 0s 69us/step - loss: 0.9517 - acc: 0.6879 - val_loss: 0.5105 - val_acc: 0.6698\n",
      "Epoch 119/200\n",
      "1256/1256 [==============================] - 0s 76us/step - loss: 0.9989 - acc: 0.6911 - val_loss: 0.5101 - val_acc: 0.6698\n",
      "Epoch 120/200\n",
      "1256/1256 [==============================] - 0s 69us/step - loss: 1.0165 - acc: 0.6815 - val_loss: 0.5186 - val_acc: 0.6540\n",
      "Epoch 121/200\n",
      "1256/1256 [==============================] - 0s 75us/step - loss: 0.9740 - acc: 0.6768 - val_loss: 0.5152 - val_acc: 0.6698\n",
      "Epoch 122/200\n",
      "1256/1256 [==============================] - 0s 80us/step - loss: 0.9834 - acc: 0.6752 - val_loss: 0.5213 - val_acc: 0.6571\n",
      "Epoch 123/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 0.9907 - acc: 0.6831 - val_loss: 0.5197 - val_acc: 0.6540\n",
      "Epoch 124/200\n",
      "1256/1256 [==============================] - 0s 82us/step - loss: 0.9872 - acc: 0.6720 - val_loss: 0.5108 - val_acc: 0.6730\n",
      "Epoch 125/200\n",
      "1256/1256 [==============================] - 0s 83us/step - loss: 1.0179 - acc: 0.6704 - val_loss: 0.5137 - val_acc: 0.6635\n",
      "Epoch 126/200\n",
      "1256/1256 [==============================] - 0s 112us/step - loss: 1.0249 - acc: 0.6768 - val_loss: 0.5251 - val_acc: 0.6254\n",
      "Epoch 127/200\n",
      "1256/1256 [==============================] - 0s 82us/step - loss: 1.0128 - acc: 0.6576 - val_loss: 0.5332 - val_acc: 0.6000\n",
      "Epoch 128/200\n",
      "1256/1256 [==============================] - 0s 83us/step - loss: 0.9981 - acc: 0.6584 - val_loss: 0.5251 - val_acc: 0.6190\n",
      "Epoch 129/200\n",
      "1256/1256 [==============================] - 0s 75us/step - loss: 1.0114 - acc: 0.6712 - val_loss: 0.5216 - val_acc: 0.6349\n",
      "Epoch 130/200\n",
      "1256/1256 [==============================] - 0s 73us/step - loss: 1.0128 - acc: 0.6584 - val_loss: 0.5161 - val_acc: 0.6444\n",
      "Epoch 131/200\n",
      "1256/1256 [==============================] - 0s 73us/step - loss: 1.0124 - acc: 0.6553 - val_loss: 0.5170 - val_acc: 0.6444\n",
      "Epoch 132/200\n",
      "1256/1256 [==============================] - 0s 72us/step - loss: 1.0700 - acc: 0.6744 - val_loss: 0.5243 - val_acc: 0.6190\n",
      "Epoch 133/200\n",
      "1256/1256 [==============================] - 0s 72us/step - loss: 1.0142 - acc: 0.6664 - val_loss: 0.5270 - val_acc: 0.6254\n",
      "Epoch 134/200\n",
      "1256/1256 [==============================] - 0s 71us/step - loss: 0.9973 - acc: 0.6831 - val_loss: 0.5219 - val_acc: 0.6349\n",
      "Epoch 135/200\n",
      "1256/1256 [==============================] - 0s 71us/step - loss: 1.0097 - acc: 0.6752 - val_loss: 0.5222 - val_acc: 0.6190\n",
      "Epoch 136/200\n",
      "1256/1256 [==============================] - 0s 67us/step - loss: 1.0512 - acc: 0.6584 - val_loss: 0.5231 - val_acc: 0.6127\n",
      "Epoch 137/200\n",
      "1256/1256 [==============================] - 0s 67us/step - loss: 0.9951 - acc: 0.6624 - val_loss: 0.5281 - val_acc: 0.5968\n",
      "Epoch 138/200\n",
      "1256/1256 [==============================] - 0s 77us/step - loss: 0.9893 - acc: 0.6696 - val_loss: 0.5332 - val_acc: 0.5937\n",
      "Epoch 139/200\n",
      "1256/1256 [==============================] - 0s 80us/step - loss: 0.9916 - acc: 0.6529 - val_loss: 0.5283 - val_acc: 0.5968\n",
      "Epoch 140/200\n",
      "1256/1256 [==============================] - 0s 71us/step - loss: 1.0137 - acc: 0.6584 - val_loss: 0.5324 - val_acc: 0.5873\n",
      "Epoch 141/200\n",
      "1256/1256 [==============================] - 0s 72us/step - loss: 1.0420 - acc: 0.6576 - val_loss: 0.5293 - val_acc: 0.5968\n",
      "Epoch 142/200\n",
      "1256/1256 [==============================] - 0s 69us/step - loss: 1.0323 - acc: 0.6680 - val_loss: 0.5292 - val_acc: 0.6095\n",
      "Epoch 143/200\n",
      "1256/1256 [==============================] - 0s 79us/step - loss: 1.0005 - acc: 0.6401 - val_loss: 0.5242 - val_acc: 0.6190\n",
      "Epoch 144/200\n",
      "1256/1256 [==============================] - 0s 75us/step - loss: 0.9868 - acc: 0.6712 - val_loss: 0.5132 - val_acc: 0.6381\n",
      "Epoch 145/200\n",
      "1256/1256 [==============================] - 0s 74us/step - loss: 0.9840 - acc: 0.6768 - val_loss: 0.5089 - val_acc: 0.6508\n",
      "Epoch 146/200\n",
      "1256/1256 [==============================] - 0s 76us/step - loss: 1.0051 - acc: 0.6688 - val_loss: 0.5188 - val_acc: 0.6222\n",
      "Epoch 147/200\n",
      "1256/1256 [==============================] - 0s 79us/step - loss: 0.9695 - acc: 0.6656 - val_loss: 0.5095 - val_acc: 0.6476\n",
      "Epoch 148/200\n",
      "1256/1256 [==============================] - 0s 79us/step - loss: 0.9985 - acc: 0.6688 - val_loss: 0.5156 - val_acc: 0.6286\n",
      "Epoch 149/200\n",
      "1256/1256 [==============================] - 0s 77us/step - loss: 0.9760 - acc: 0.6775 - val_loss: 0.5220 - val_acc: 0.6127\n",
      "Epoch 150/200\n",
      "1256/1256 [==============================] - 0s 76us/step - loss: 0.9675 - acc: 0.6688 - val_loss: 0.5174 - val_acc: 0.6254\n",
      "Epoch 151/200\n",
      "1256/1256 [==============================] - 0s 71us/step - loss: 0.9629 - acc: 0.6752 - val_loss: 0.5155 - val_acc: 0.6317\n",
      "Epoch 152/200\n",
      "1256/1256 [==============================] - 0s 69us/step - loss: 0.9691 - acc: 0.6895 - val_loss: 0.5080 - val_acc: 0.6635\n",
      "Epoch 153/200\n",
      "1256/1256 [==============================] - 0s 74us/step - loss: 1.0253 - acc: 0.6823 - val_loss: 0.5152 - val_acc: 0.6444\n",
      "Epoch 154/200\n",
      "1256/1256 [==============================] - 0s 105us/step - loss: 1.0039 - acc: 0.6656 - val_loss: 0.5141 - val_acc: 0.6349\n",
      "Epoch 155/200\n",
      "1256/1256 [==============================] - 0s 82us/step - loss: 1.0135 - acc: 0.6664 - val_loss: 0.5153 - val_acc: 0.6286\n",
      "Epoch 156/200\n",
      "1256/1256 [==============================] - 0s 86us/step - loss: 0.9697 - acc: 0.6887 - val_loss: 0.5093 - val_acc: 0.6603\n",
      "Epoch 157/200\n",
      "1256/1256 [==============================] - 0s 86us/step - loss: 1.0375 - acc: 0.6640 - val_loss: 0.5090 - val_acc: 0.6444\n",
      "Epoch 158/200\n",
      "1256/1256 [==============================] - 0s 86us/step - loss: 0.9758 - acc: 0.6728 - val_loss: 0.5148 - val_acc: 0.6159\n",
      "Epoch 159/200\n",
      "1256/1256 [==============================] - 0s 83us/step - loss: 0.9990 - acc: 0.6521 - val_loss: 0.5244 - val_acc: 0.5905\n",
      "Epoch 160/200\n",
      "1256/1256 [==============================] - 0s 76us/step - loss: 1.0133 - acc: 0.6513 - val_loss: 0.5157 - val_acc: 0.6190\n",
      "Epoch 161/200\n",
      "1256/1256 [==============================] - 0s 68us/step - loss: 0.9833 - acc: 0.6696 - val_loss: 0.5067 - val_acc: 0.6508\n",
      "Epoch 162/200\n",
      "1256/1256 [==============================] - 0s 69us/step - loss: 0.9804 - acc: 0.6728 - val_loss: 0.5098 - val_acc: 0.6540\n",
      "Epoch 163/200\n",
      "1256/1256 [==============================] - 0s 85us/step - loss: 0.9444 - acc: 0.6847 - val_loss: 0.4971 - val_acc: 0.6952\n",
      "Epoch 164/200\n",
      "1256/1256 [==============================] - 0s 75us/step - loss: 0.9875 - acc: 0.6791 - val_loss: 0.4897 - val_acc: 0.7048\n",
      "Epoch 165/200\n",
      "1256/1256 [==============================] - 0s 82us/step - loss: 0.9850 - acc: 0.6895 - val_loss: 0.4938 - val_acc: 0.7016\n",
      "Epoch 166/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 0.9707 - acc: 0.6863 - val_loss: 0.4914 - val_acc: 0.7111\n",
      "Epoch 167/200\n",
      "1256/1256 [==============================] - 0s 76us/step - loss: 0.9951 - acc: 0.6879 - val_loss: 0.5029 - val_acc: 0.6857\n",
      "Epoch 168/200\n",
      "1256/1256 [==============================] - 0s 77us/step - loss: 0.9934 - acc: 0.6815 - val_loss: 0.5095 - val_acc: 0.6540\n",
      "Epoch 169/200\n",
      "1256/1256 [==============================] - 0s 77us/step - loss: 0.9658 - acc: 0.6664 - val_loss: 0.5166 - val_acc: 0.6190\n",
      "Epoch 170/200\n",
      "1256/1256 [==============================] - 0s 80us/step - loss: 0.9857 - acc: 0.6592 - val_loss: 0.5132 - val_acc: 0.6286\n",
      "Epoch 171/200\n",
      "1256/1256 [==============================] - 0s 79us/step - loss: 0.9484 - acc: 0.6871 - val_loss: 0.5025 - val_acc: 0.6857\n",
      "Epoch 172/200\n",
      "1256/1256 [==============================] - 0s 70us/step - loss: 0.9682 - acc: 0.6871 - val_loss: 0.4983 - val_acc: 0.7143\n",
      "Epoch 173/200\n",
      "1256/1256 [==============================] - 0s 70us/step - loss: 0.9753 - acc: 0.6927 - val_loss: 0.5003 - val_acc: 0.7111\n",
      "Epoch 174/200\n",
      "1256/1256 [==============================] - 0s 73us/step - loss: 0.9796 - acc: 0.6990 - val_loss: 0.4956 - val_acc: 0.7175\n",
      "Epoch 175/200\n",
      "1256/1256 [==============================] - 0s 68us/step - loss: 0.9721 - acc: 0.6871 - val_loss: 0.5025 - val_acc: 0.6984\n",
      "Epoch 176/200\n",
      "1256/1256 [==============================] - 0s 73us/step - loss: 1.0085 - acc: 0.6807 - val_loss: 0.5094 - val_acc: 0.6698\n",
      "Epoch 177/200\n",
      "1256/1256 [==============================] - 0s 73us/step - loss: 0.9381 - acc: 0.6879 - val_loss: 0.5076 - val_acc: 0.6794\n",
      "Epoch 178/200\n",
      "1256/1256 [==============================] - 0s 72us/step - loss: 0.9803 - acc: 0.6855 - val_loss: 0.5084 - val_acc: 0.6730\n",
      "Epoch 179/200\n",
      "1256/1256 [==============================] - 0s 73us/step - loss: 0.9877 - acc: 0.6863 - val_loss: 0.5044 - val_acc: 0.6921\n",
      "Epoch 180/200\n",
      "1256/1256 [==============================] - 0s 74us/step - loss: 0.9386 - acc: 0.6791 - val_loss: 0.5017 - val_acc: 0.6889\n",
      "Epoch 181/200\n",
      "1256/1256 [==============================] - 0s 78us/step - loss: 0.9562 - acc: 0.6935 - val_loss: 0.5066 - val_acc: 0.6698\n",
      "Epoch 182/200\n",
      "1256/1256 [==============================] - 0s 79us/step - loss: 1.0001 - acc: 0.6847 - val_loss: 0.5081 - val_acc: 0.6794\n",
      "Epoch 183/200\n",
      "1256/1256 [==============================] - 0s 83us/step - loss: 1.0005 - acc: 0.6831 - val_loss: 0.5099 - val_acc: 0.6667\n",
      "Epoch 184/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 0.9832 - acc: 0.6951 - val_loss: 0.5065 - val_acc: 0.6825\n",
      "Epoch 185/200\n",
      "1256/1256 [==============================] - 0s 91us/step - loss: 0.9669 - acc: 0.6935 - val_loss: 0.5075 - val_acc: 0.6921\n",
      "Epoch 186/200\n",
      "1256/1256 [==============================] - 0s 93us/step - loss: 0.9792 - acc: 0.6791 - val_loss: 0.5102 - val_acc: 0.6730\n",
      "Epoch 187/200\n",
      "1256/1256 [==============================] - 0s 91us/step - loss: 0.9452 - acc: 0.6760 - val_loss: 0.5082 - val_acc: 0.6635\n",
      "Epoch 188/200\n",
      "1256/1256 [==============================] - 0s 79us/step - loss: 1.0044 - acc: 0.6791 - val_loss: 0.5109 - val_acc: 0.6635\n",
      "Epoch 189/200\n",
      "1256/1256 [==============================] - 0s 73us/step - loss: 0.9675 - acc: 0.6839 - val_loss: 0.5071 - val_acc: 0.6794\n",
      "Epoch 190/200\n",
      "1256/1256 [==============================] - 0s 72us/step - loss: 0.9543 - acc: 0.6815 - val_loss: 0.5109 - val_acc: 0.6603\n",
      "Epoch 191/200\n",
      "1256/1256 [==============================] - 0s 71us/step - loss: 0.9667 - acc: 0.6871 - val_loss: 0.5089 - val_acc: 0.6794\n",
      "Epoch 192/200\n",
      "1256/1256 [==============================] - 0s 80us/step - loss: 1.0002 - acc: 0.6847 - val_loss: 0.5064 - val_acc: 0.6762\n",
      "Epoch 193/200\n",
      "1256/1256 [==============================] - 0s 79us/step - loss: 0.9456 - acc: 0.6815 - val_loss: 0.5144 - val_acc: 0.6667\n",
      "Epoch 194/200\n",
      "1256/1256 [==============================] - 0s 63us/step - loss: 1.0001 - acc: 0.6616 - val_loss: 0.5145 - val_acc: 0.6540\n",
      "Epoch 195/200\n",
      "1256/1256 [==============================] - 0s 67us/step - loss: 0.9621 - acc: 0.6823 - val_loss: 0.5155 - val_acc: 0.6571\n",
      "Epoch 196/200\n",
      "1256/1256 [==============================] - 0s 79us/step - loss: 0.9336 - acc: 0.6903 - val_loss: 0.5090 - val_acc: 0.6762\n",
      "Epoch 197/200\n",
      "1256/1256 [==============================] - 0s 71us/step - loss: 0.9526 - acc: 0.6871 - val_loss: 0.5017 - val_acc: 0.6794\n",
      "Epoch 198/200\n",
      "1256/1256 [==============================] - 0s 68us/step - loss: 0.9646 - acc: 0.6935 - val_loss: 0.5047 - val_acc: 0.6825\n",
      "Epoch 199/200\n",
      "1256/1256 [==============================] - 0s 67us/step - loss: 0.9296 - acc: 0.6911 - val_loss: 0.5016 - val_acc: 0.6825\n",
      "Epoch 200/200\n",
      "1256/1256 [==============================] - 0s 69us/step - loss: 0.9253 - acc: 0.7046 - val_loss: 0.5032 - val_acc: 0.6825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x290b7084a20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = { 0: 1, 1: 8.4}\n",
    "model.fit(X_train, y_train, batch_size= 32, epochs= 200, verbose=1, validation_data=(X_vali, y_vali), class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_testing_data\n",
    "test_data, _ = get_testing_data(data_root)\n",
    "predicted = model.predict_classes(test_data)\n",
    "with open(\"exercise_1_output.txt\", \"w\") as f:\n",
    "    [f.write(\"{}\\n\".format(p)) for p in predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5892857142857143"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy2 = accuracy_score(pd.get_dummies(_).iloc[:,0], predicted)\n",
    "accuracy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Any relationship between Age and whether a person has Diabetes or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24729294365805007, 2.5529994557745299e-23)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "pearsonr(df['Age'], y)\n",
    "\n",
    "## The pearsonr correlation between age and Diabetes is possitive(0.24729294365805007)\n",
    "## and it is statistically significant, with p-value 2.5529994557745299e-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = get_training_data(data_root)\n",
    "df2 = clean_data_and_labels(df2).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEaCAYAAAAL7cBuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFAsEERFJsUAQEZEUCwQREUn9fyJnbqagvTq0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x290acf482b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "plt.scatter(df2['Age'],df2['Diabetes'],alpha=0.5)\n",
    "plt.title('Diabetes vs. Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Diabetes ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict whether the person sleeps for less than mean sleeping hours across the dataset or more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Activation, Input, BatchNormalization\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 =  pd.get_dummies(df2,prefix=[\"Diabetes\",\"Race\"],columns=[\"Diabetes\",\"Race\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>D1Sugar</th>\n",
       "      <th>D1Carb</th>\n",
       "      <th>D2Sugar</th>\n",
       "      <th>D2Carb</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>SleepHours</th>\n",
       "      <th>Diabetes_1.0</th>\n",
       "      <th>Diabetes_2.0</th>\n",
       "      <th>Race_1.0</th>\n",
       "      <th>Race_2.0</th>\n",
       "      <th>Race_3.0</th>\n",
       "      <th>Race_4.0</th>\n",
       "      <th>Race_6.0</th>\n",
       "      <th>Race_7.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72.0</td>\n",
       "      <td>34.98</td>\n",
       "      <td>126.0</td>\n",
       "      <td>102.90</td>\n",
       "      <td>224.39</td>\n",
       "      <td>131.36</td>\n",
       "      <td>286.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73.0</td>\n",
       "      <td>36.72</td>\n",
       "      <td>201.0</td>\n",
       "      <td>87.78</td>\n",
       "      <td>178.20</td>\n",
       "      <td>99.26</td>\n",
       "      <td>195.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.0</td>\n",
       "      <td>89.46</td>\n",
       "      <td>168.0</td>\n",
       "      <td>128.23</td>\n",
       "      <td>300.16</td>\n",
       "      <td>61.84</td>\n",
       "      <td>182.41</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>168.0</td>\n",
       "      <td>83.91</td>\n",
       "      <td>227.63</td>\n",
       "      <td>86.13</td>\n",
       "      <td>208.11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.0</td>\n",
       "      <td>36.48</td>\n",
       "      <td>202.0</td>\n",
       "      <td>161.91</td>\n",
       "      <td>534.92</td>\n",
       "      <td>94.61</td>\n",
       "      <td>300.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Insulin  Cholesterol  D1Sugar  D1Carb  D2Sugar  D2Carb  Alcohol  \\\n",
       "0  72.0    34.98        126.0   102.90  224.39   131.36  286.47      0.0   \n",
       "1  73.0    36.72        201.0    87.78  178.20    99.26  195.95      0.0   \n",
       "2  61.0    89.46        168.0   128.23  300.16    61.84  182.41      2.0   \n",
       "3  26.0    23.10        168.0    83.91  227.63    86.13  208.11      2.0   \n",
       "4  50.0    36.48        202.0   161.91  534.92    94.61  300.01      0.0   \n",
       "\n",
       "   SleepHours  Diabetes_1.0  Diabetes_2.0  Race_1.0  Race_2.0  Race_3.0  \\\n",
       "0         8.0             1             0         0         0         1   \n",
       "1         9.0             0             1         0         0         1   \n",
       "2         9.0             0             1         0         0         1   \n",
       "3         8.0             0             1         0         0         1   \n",
       "4         7.0             0             1         0         0         0   \n",
       "\n",
       "   Race_4.0  Race_6.0  Race_7.0  \n",
       "0         0         0         0  \n",
       "1         0         0         0  \n",
       "2         0         0         0  \n",
       "3         0         0         0  \n",
       "4         0         1         0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_mean = df2['SleepHours'].mean() \n",
    "y_s = pd.get_dummies((df2['SleepHours'] > sleep_mean )*1)\n",
    "X_s = df2.drop(\"SleepHours\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaler = preprocessing.StandardScaler()\n",
    "X_s.iloc[:,0:8]  = Scaler.fit_transform(X_s.iloc[:,0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sleep = Sequential()\n",
    "\n",
    "model_sleep.add(Dense(input_dim= 16 , units= 32, activation=\"relu\"))  \n",
    "model_sleep.add(Dropout(0.5))\n",
    "\n",
    "model_sleep.add(Dense( units= 20, activation=\"relu\"))  \n",
    "model_sleep.add(Dropout(0.5))\n",
    "\n",
    "model_sleep.add(Dense( units= 10, activation=\"relu\"))  \n",
    "model_sleep.add(Dropout(0.5))\n",
    "\n",
    "model_sleep.add(Dense(units= 2, activation=\"softmax\"))\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-7, momentum=0.9) \n",
    "model_sleep.compile(loss=\"categorical_crossentropy\", optimizer= sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight2 = {1:1, 0:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x290b7242e10>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_s_train, X_s_vali, y_s_train, y_s_vali = train_test_split(X_s, y_s, test_size=0.3, random_state=42)\n",
    "model_sleep.fit(X_s_train, y_s_train, batch_size= 32, epochs= 100, verbose=0,  validation_split=0.2, class_weight = class_weight2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 [==============================] - 0s 44us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.66623163930440354, 0.61440677966101698]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sleep.evaluate(X_s_vali,y_s_vali)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the alcohol consumption of the person?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>D1Sugar</th>\n",
       "      <th>D1Carb</th>\n",
       "      <th>D2Sugar</th>\n",
       "      <th>D2Carb</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>SleepHours</th>\n",
       "      <th>Diabetes_1.0</th>\n",
       "      <th>Diabetes_2.0</th>\n",
       "      <th>Race_1.0</th>\n",
       "      <th>Race_2.0</th>\n",
       "      <th>Race_3.0</th>\n",
       "      <th>Race_4.0</th>\n",
       "      <th>Race_6.0</th>\n",
       "      <th>Race_7.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72.0</td>\n",
       "      <td>34.98</td>\n",
       "      <td>126.0</td>\n",
       "      <td>102.90</td>\n",
       "      <td>224.39</td>\n",
       "      <td>131.36</td>\n",
       "      <td>286.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73.0</td>\n",
       "      <td>36.72</td>\n",
       "      <td>201.0</td>\n",
       "      <td>87.78</td>\n",
       "      <td>178.20</td>\n",
       "      <td>99.26</td>\n",
       "      <td>195.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.0</td>\n",
       "      <td>89.46</td>\n",
       "      <td>168.0</td>\n",
       "      <td>128.23</td>\n",
       "      <td>300.16</td>\n",
       "      <td>61.84</td>\n",
       "      <td>182.41</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>168.0</td>\n",
       "      <td>83.91</td>\n",
       "      <td>227.63</td>\n",
       "      <td>86.13</td>\n",
       "      <td>208.11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.0</td>\n",
       "      <td>36.48</td>\n",
       "      <td>202.0</td>\n",
       "      <td>161.91</td>\n",
       "      <td>534.92</td>\n",
       "      <td>94.61</td>\n",
       "      <td>300.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Insulin  Cholesterol  D1Sugar  D1Carb  D2Sugar  D2Carb  Alcohol  \\\n",
       "0  72.0    34.98        126.0   102.90  224.39   131.36  286.47      0.0   \n",
       "1  73.0    36.72        201.0    87.78  178.20    99.26  195.95      0.0   \n",
       "2  61.0    89.46        168.0   128.23  300.16    61.84  182.41      2.0   \n",
       "3  26.0    23.10        168.0    83.91  227.63    86.13  208.11      2.0   \n",
       "4  50.0    36.48        202.0   161.91  534.92    94.61  300.01      0.0   \n",
       "\n",
       "   SleepHours  Diabetes_1.0  Diabetes_2.0  Race_1.0  Race_2.0  Race_3.0  \\\n",
       "0         8.0             1             0         0         0         1   \n",
       "1         9.0             0             1         0         0         1   \n",
       "2         9.0             0             1         0         0         1   \n",
       "3         8.0             0             1         0         0         1   \n",
       "4         7.0             0             1         0         0         0   \n",
       "\n",
       "   Race_4.0  Race_6.0  Race_7.0  \n",
       "0         0         0         0  \n",
       "1         0         0         0  \n",
       "2         0         0         0  \n",
       "3         0         0         0  \n",
       "4         0         1         0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_a = df2['Alcohol']\n",
    "X_a = df2.drop(\"Alcohol\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaler = preprocessing.StandardScaler()\n",
    "X_a.iloc[:,0:8]  = Scaler.fit_transform(X_a.iloc[:,0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = Sequential()\n",
    "\n",
    "model_a.add(Dense(input_dim= 16 , units= 32, activation=\"relu\"))  \n",
    "model_a.add(Dropout(0.5))\n",
    "\n",
    "model_a.add(Dense( units= 16, activation=\"relu\"))  \n",
    "model_a.add(Dropout(0.5))\n",
    "\n",
    "model_a.add(Dense( units= 8, activation=\"relu\"))  \n",
    "model_a.add(Dropout(0.5))\n",
    "\n",
    "model_a.add(Dense(units= 1, activation=\"softmax\"))\n",
    "\n",
    "sgd = SGD(lr=0.001, decay=1e-7, momentum=0.9) \n",
    "model_a.compile(loss=\"mean_squared_error\", optimizer= sgd, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x290b816aba8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_a_train, X_a_test, y_a_train, y_a_test = train_test_split(X_a, y_a, test_size=0.3, random_state= 101)\n",
    "model_a.fit(X_a_train, y_a_train, batch_size= 32, epochs= 100, verbose=0,  validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 [==============================] - 0s 25us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[116.80084746570911, 2.9745762691659441]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a.evaluate(X_a_test, y_a_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
