{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "\n",
    "Predicting diabetes using the NHANES dataset\n",
    "\n",
    "About our dataset\n",
    "The National Health and Nutrition Examination Survey (NHANES), administered annually by the National Center for Health Statistics, is designed to assess the general health and nutritional status of adults and children in the United States.\n",
    "\n",
    "\n",
    "Data:\n",
    "\n",
    "https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=2013\n",
    "\n",
    "\n",
    "Goals:\n",
    "\n",
    "- refresh general machine learning principles (train/dev/test)\n",
    "- refresh neural network implementation\n",
    "- handle an imbalanced dataset\n",
    "\n",
    "Keras:\n",
    "\n",
    "The Python Deep Learning Library Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.core import Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook supports automated reloading of packages. So once you import a file as a module, any saved changes you make to that file will be automatically changed in this notebook. For example, go to exercise_1.py and toggle the comment the second \"print\" statement in helloworld(), rerunning the next cell several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_1 import helloworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "helloworld()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing the dataset\n",
    "We provide a helper function for you to read the SAS files into a pandas dataframe. \n",
    "\n",
    "To load it, you will need to install xport (a SAS interface to Python) \n",
    "\n",
    "```pip install xport```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import merge_xpt, get_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFY THIS CELL BY POINTING IT TO WHERE YOU EXTRACTED YOUR DATA ###\n",
    "data_root = \"/Users/liang/UCLA/course/2018Fall/Adv Machine Learning/Project 1/Data/nhanes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_training_data(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2507, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Age</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>Race</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>D1Sugar</th>\n",
       "      <th>D1Carb</th>\n",
       "      <th>D2Sugar</th>\n",
       "      <th>D2Carb</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>SleepHours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2507.000000</td>\n",
       "      <td>2507.000000</td>\n",
       "      <td>2444.000000</td>\n",
       "      <td>2507.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2318.000000</td>\n",
       "      <td>2318.000000</td>\n",
       "      <td>2067.000000</td>\n",
       "      <td>2067.000000</td>\n",
       "      <td>1930.000000</td>\n",
       "      <td>2506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.919426</td>\n",
       "      <td>48.035899</td>\n",
       "      <td>75.344239</td>\n",
       "      <td>3.286797</td>\n",
       "      <td>187.024970</td>\n",
       "      <td>108.484292</td>\n",
       "      <td>251.030453</td>\n",
       "      <td>101.030982</td>\n",
       "      <td>237.012772</td>\n",
       "      <td>3.496891</td>\n",
       "      <td>6.922985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.364925</td>\n",
       "      <td>18.252254</td>\n",
       "      <td>113.087851</td>\n",
       "      <td>1.490164</td>\n",
       "      <td>41.496019</td>\n",
       "      <td>75.073624</td>\n",
       "      <td>126.535489</td>\n",
       "      <td>65.456716</td>\n",
       "      <td>116.708696</td>\n",
       "      <td>13.739268</td>\n",
       "      <td>1.403750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>8.670000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>34.305000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>57.390000</td>\n",
       "      <td>164.557500</td>\n",
       "      <td>54.570000</td>\n",
       "      <td>158.240000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>53.460000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>93.450000</td>\n",
       "      <td>231.080000</td>\n",
       "      <td>88.420000</td>\n",
       "      <td>220.880000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>86.280000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>141.022500</td>\n",
       "      <td>311.895000</td>\n",
       "      <td>132.055000</td>\n",
       "      <td>296.170000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4094.880000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>612.000000</td>\n",
       "      <td>719.580000</td>\n",
       "      <td>1300.720000</td>\n",
       "      <td>457.410000</td>\n",
       "      <td>1041.560000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Diabetes          Age      Insulin         Race  Cholesterol  \\\n",
       "count  2507.000000  2507.000000  2444.000000  2507.000000  2483.000000   \n",
       "mean      1.919426    48.035899    75.344239     3.286797   187.024970   \n",
       "std       0.364925    18.252254   113.087851     1.490164    41.496019   \n",
       "min       1.000000    18.000000     0.840000     1.000000    69.000000   \n",
       "25%       2.000000    33.000000    34.305000     3.000000   159.000000   \n",
       "50%       2.000000    48.000000    53.460000     3.000000   183.000000   \n",
       "75%       2.000000    63.000000    86.280000     4.000000   211.000000   \n",
       "max       3.000000    80.000000  4094.880000     7.000000   612.000000   \n",
       "\n",
       "           D1Sugar       D1Carb      D2Sugar       D2Carb      Alcohol  \\\n",
       "count  2318.000000  2318.000000  2067.000000  2067.000000  1930.000000   \n",
       "mean    108.484292   251.030453   101.030982   237.012772     3.496891   \n",
       "std      75.073624   126.535489    65.456716   116.708696    13.739268   \n",
       "min       0.130000     8.670000     0.000000     0.000000     0.000000   \n",
       "25%      57.390000   164.557500    54.570000   158.240000     1.000000   \n",
       "50%      93.450000   231.080000    88.420000   220.880000     2.000000   \n",
       "75%     141.022500   311.895000   132.055000   296.170000     4.000000   \n",
       "max     719.580000  1300.720000   457.410000  1041.560000   365.000000   \n",
       "\n",
       "        SleepHours  \n",
       "count  2506.000000  \n",
       "mean      6.922985  \n",
       "std       1.403750  \n",
       "min       2.000000  \n",
       "25%       6.000000  \n",
       "50%       7.000000  \n",
       "75%       8.000000  \n",
       "max      12.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two commands above are useful for quickly getting a feel for the dataset. We definitely want to know the shape of the dataframe so we know how many features we're dealing with, and we can see the number of missing values in each column, as well as a few descriptive statistics.\n",
    "\n",
    "# Exercise 1: \n",
    "\n",
    "The Diabetes column is coded as 1.0: yes and 2.0: no, and for some reason there are a few rows with a 3.0. We don't know what 3.0 means, so we will drop it. Also, we will remove all of the samples that have a NaN (for easy training later- also it seems like we'll still have enough data). \n",
    "\n",
    "As most of you are familiar with SQL, these references may be helpful:\n",
    "https://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html\n",
    "\n",
    "Use it to complete clean_data_and_labels() in exercise_1.py(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_1 import clean_data_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_data_and_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check your code. \n",
    "# If you see no output, you have completed the exercise correctly.\n",
    "assert np.all(df.Diabetes < 3), \"Not all labels are < 3.0\"\n",
    "assert np.all(df.count() == len(df)), \"There are still NaNs in your data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2:\n",
    "\n",
    "With the clean dataset we are ready to build and train the model.\n",
    "\n",
    "Please build your neural network by completing build_model() in exercise_1.py. The architecture choice is up to you, but please only use fully connected (Dense) layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_1 import build_model, split_x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 2.3918 - acc: 0.8000 - auc: 0.6425   \n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 1.6118 - acc: 0.9000 - auc: 0.8562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10b0d10f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if the model works. If it runs your model is functioning properly\n",
    "x_sm, y_sm = split_x_y(df[0:10])  # load the first 10 samples\n",
    "model = build_model(x_sm.shape[1])\n",
    "model.fit(x_sm, y_sm, batch_size=1, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3:\n",
    "Model tuning\n",
    "\n",
    "Optimize your neural network by modifying the cells below. Here are a few hints:\n",
    "\n",
    "- You are not provided with a validation set, so it may be helpful to make your own. \n",
    "- Check the distribution of the labels using df.Diabetes.hist(). What should you do about this? (make any changes to preprocess_dataset() in exercise_1.py- OPTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from exercise_1 import preprocess_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.shape[1] == 11, \\\n",
    "\"Number of features is off, make sure you didn't remove or add any\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cells below to fit your model and as scratch (perhaps to view the dataset distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,scaler = preprocess_dataset(df,scaler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1256 samples, validate on 315 samples\n",
      "Epoch 1/200\n",
      "1256/1256 [==============================] - 1s 695us/step - loss: 1.7536 - acc: 0.3917 - auc: 0.5264 - val_loss: 1.6514 - val_acc: 0.1238 - val_auc: 0.3591\n",
      "Epoch 2/200\n",
      "1256/1256 [==============================] - 0s 104us/step - loss: 1.5649 - acc: 0.2150 - auc: 0.3020 - val_loss: 1.6221 - val_acc: 0.1238 - val_auc: 0.2610\n",
      "Epoch 3/200\n",
      "1256/1256 [==============================] - 0s 105us/step - loss: 1.5545 - acc: 0.1903 - auc: 0.2387 - val_loss: 1.6144 - val_acc: 0.1238 - val_auc: 0.2241\n",
      "Epoch 4/200\n",
      "1256/1256 [==============================] - 0s 106us/step - loss: 1.6252 - acc: 0.1537 - auc: 0.2108 - val_loss: 1.6138 - val_acc: 0.1238 - val_auc: 0.1980\n",
      "Epoch 5/200\n",
      "1256/1256 [==============================] - 0s 104us/step - loss: 1.5753 - acc: 0.1393 - auc: 0.1899 - val_loss: 1.6151 - val_acc: 0.1238 - val_auc: 0.1823\n",
      "Epoch 6/200\n",
      "1256/1256 [==============================] - 0s 119us/step - loss: 1.5670 - acc: 0.1584 - auc: 0.1771 - val_loss: 1.6092 - val_acc: 0.1238 - val_auc: 0.1726\n",
      "Epoch 7/200\n",
      "1256/1256 [==============================] - 0s 107us/step - loss: 1.5535 - acc: 0.1433 - auc: 0.1691 - val_loss: 1.6080 - val_acc: 0.1238 - val_auc: 0.1649\n",
      "Epoch 8/200\n",
      "1256/1256 [==============================] - 0s 105us/step - loss: 1.5312 - acc: 0.1401 - auc: 0.1625 - val_loss: 1.6061 - val_acc: 0.1238 - val_auc: 0.1606\n",
      "Epoch 9/200\n",
      "1256/1256 [==============================] - 0s 92us/step - loss: 1.5654 - acc: 0.1489 - auc: 0.1584 - val_loss: 1.6030 - val_acc: 0.1238 - val_auc: 0.1566\n",
      "Epoch 10/200\n",
      "1256/1256 [==============================] - 0s 98us/step - loss: 1.5569 - acc: 0.1385 - auc: 0.1555 - val_loss: 1.6009 - val_acc: 0.1238 - val_auc: 0.1532\n",
      "Epoch 11/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 1.5471 - acc: 0.1433 - auc: 0.1524 - val_loss: 1.5995 - val_acc: 0.1238 - val_auc: 0.1512\n",
      "Epoch 12/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.5273 - acc: 0.1489 - auc: 0.1508 - val_loss: 1.5967 - val_acc: 0.1238 - val_auc: 0.1505\n",
      "Epoch 13/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 1.5350 - acc: 0.1465 - auc: 0.1503 - val_loss: 1.5923 - val_acc: 0.1238 - val_auc: 0.1499\n",
      "Epoch 14/200\n",
      "1256/1256 [==============================] - 0s 103us/step - loss: 1.5541 - acc: 0.1497 - auc: 0.1499 - val_loss: 1.5896 - val_acc: 0.1238 - val_auc: 0.1492\n",
      "Epoch 15/200\n",
      "1256/1256 [==============================] - 0s 98us/step - loss: 1.5302 - acc: 0.1385 - auc: 0.1488 - val_loss: 1.5869 - val_acc: 0.1238 - val_auc: 0.1484\n",
      "Epoch 16/200\n",
      "1256/1256 [==============================] - 0s 91us/step - loss: 1.5405 - acc: 0.1497 - auc: 0.1486 - val_loss: 1.5844 - val_acc: 0.1238 - val_auc: 0.1483\n",
      "Epoch 17/200\n",
      "1256/1256 [==============================] - 0s 93us/step - loss: 1.5271 - acc: 0.1385 - auc: 0.1483 - val_loss: 1.5857 - val_acc: 0.1238 - val_auc: 0.1478\n",
      "Epoch 18/200\n",
      "1256/1256 [==============================] - 0s 109us/step - loss: 1.5391 - acc: 0.1409 - auc: 0.1475 - val_loss: 1.5872 - val_acc: 0.1238 - val_auc: 0.1474\n",
      "Epoch 19/200\n",
      "1256/1256 [==============================] - 0s 114us/step - loss: 1.5422 - acc: 0.1497 - auc: 0.1474 - val_loss: 1.5844 - val_acc: 0.1270 - val_auc: 0.1471\n",
      "Epoch 20/200\n",
      "1256/1256 [==============================] - 0s 105us/step - loss: 1.5429 - acc: 0.1529 - auc: 0.1473 - val_loss: 1.5808 - val_acc: 0.1270 - val_auc: 0.1474\n",
      "Epoch 21/200\n",
      "1256/1256 [==============================] - 0s 109us/step - loss: 1.5289 - acc: 0.1600 - auc: 0.1475 - val_loss: 1.5798 - val_acc: 0.1270 - val_auc: 0.1476\n",
      "Epoch 22/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.5133 - acc: 0.1632 - auc: 0.1481 - val_loss: 1.5731 - val_acc: 0.1270 - val_auc: 0.1480\n",
      "Epoch 23/200\n",
      "1256/1256 [==============================] - 0s 103us/step - loss: 1.5125 - acc: 0.1656 - auc: 0.1483 - val_loss: 1.5659 - val_acc: 0.1270 - val_auc: 0.1487\n",
      "Epoch 24/200\n",
      "1256/1256 [==============================] - 0s 108us/step - loss: 1.5045 - acc: 0.1831 - auc: 0.1492 - val_loss: 1.5564 - val_acc: 0.1302 - val_auc: 0.1497\n",
      "Epoch 25/200\n",
      "1256/1256 [==============================] - 0s 111us/step - loss: 1.5069 - acc: 0.1895 - auc: 0.1501 - val_loss: 1.5462 - val_acc: 0.1333 - val_auc: 0.1508\n",
      "Epoch 26/200\n",
      "1256/1256 [==============================] - 0s 108us/step - loss: 1.4835 - acc: 0.2126 - auc: 0.1517 - val_loss: 1.5344 - val_acc: 0.1333 - val_auc: 0.1526\n",
      "Epoch 27/200\n",
      "1256/1256 [==============================] - 0s 109us/step - loss: 1.5121 - acc: 0.2261 - auc: 0.1539 - val_loss: 1.5245 - val_acc: 0.1397 - val_auc: 0.1551\n",
      "Epoch 28/200\n",
      "1256/1256 [==============================] - 0s 107us/step - loss: 1.4980 - acc: 0.2333 - auc: 0.1563 - val_loss: 1.5146 - val_acc: 0.1492 - val_auc: 0.1574\n",
      "Epoch 29/200\n",
      "1256/1256 [==============================] - 0s 107us/step - loss: 1.4922 - acc: 0.2365 - auc: 0.1586 - val_loss: 1.5035 - val_acc: 0.1460 - val_auc: 0.1595\n",
      "Epoch 30/200\n",
      "1256/1256 [==============================] - 0s 116us/step - loss: 1.5149 - acc: 0.2357 - auc: 0.1603 - val_loss: 1.5044 - val_acc: 0.1492 - val_auc: 0.1611\n",
      "Epoch 31/200\n",
      "1256/1256 [==============================] - 0s 130us/step - loss: 1.4712 - acc: 0.2627 - auc: 0.1627 - val_loss: 1.4954 - val_acc: 0.1619 - val_auc: 0.1639\n",
      "Epoch 32/200\n",
      "1256/1256 [==============================] - 0s 103us/step - loss: 1.4660 - acc: 0.2596 - auc: 0.1655 - val_loss: 1.4829 - val_acc: 0.2063 - val_auc: 0.1668\n",
      "Epoch 33/200\n",
      "1256/1256 [==============================] - 0s 107us/step - loss: 1.4634 - acc: 0.2818 - auc: 0.1684 - val_loss: 1.4699 - val_acc: 0.2635 - val_auc: 0.1701\n",
      "Epoch 34/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.4396 - acc: 0.3177 - auc: 0.1719 - val_loss: 1.4600 - val_acc: 0.2794 - val_auc: 0.1745\n",
      "Epoch 35/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 1.4903 - acc: 0.3041 - auc: 0.1766 - val_loss: 1.4561 - val_acc: 0.2667 - val_auc: 0.1782\n",
      "Epoch 36/200\n",
      "1256/1256 [==============================] - 0s 91us/step - loss: 1.4220 - acc: 0.3065 - auc: 0.1802 - val_loss: 1.4441 - val_acc: 0.3111 - val_auc: 0.1822\n",
      "Epoch 37/200\n",
      "1256/1256 [==============================] - 0s 90us/step - loss: 1.4144 - acc: 0.3193 - auc: 0.1843 - val_loss: 1.4271 - val_acc: 0.3429 - val_auc: 0.1864\n",
      "Epoch 38/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 1.4259 - acc: 0.3400 - auc: 0.1889 - val_loss: 1.4167 - val_acc: 0.3524 - val_auc: 0.1916\n",
      "Epoch 39/200\n",
      "1256/1256 [==============================] - 0s 104us/step - loss: 1.3618 - acc: 0.3670 - auc: 0.1945 - val_loss: 1.3919 - val_acc: 0.4222 - val_auc: 0.1976\n",
      "Epoch 40/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 1.3972 - acc: 0.3798 - auc: 0.2008 - val_loss: 1.3856 - val_acc: 0.4127 - val_auc: 0.2040\n",
      "Epoch 41/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.3607 - acc: 0.3957 - auc: 0.2071 - val_loss: 1.3719 - val_acc: 0.4730 - val_auc: 0.2107\n",
      "Epoch 42/200\n",
      "1256/1256 [==============================] - 0s 121us/step - loss: 1.4158 - acc: 0.3869 - auc: 0.2140 - val_loss: 1.3685 - val_acc: 0.4571 - val_auc: 0.2170\n",
      "Epoch 43/200\n",
      "1256/1256 [==============================] - 0s 155us/step - loss: 1.3660 - acc: 0.4013 - auc: 0.2201 - val_loss: 1.3611 - val_acc: 0.4762 - val_auc: 0.2235\n",
      "Epoch 44/200\n",
      "1256/1256 [==============================] - 0s 130us/step - loss: 1.3466 - acc: 0.4212 - auc: 0.2269 - val_loss: 1.3498 - val_acc: 0.4825 - val_auc: 0.2305\n",
      "Epoch 45/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.3970 - acc: 0.3838 - auc: 0.2336 - val_loss: 1.3524 - val_acc: 0.4444 - val_auc: 0.2360\n",
      "Epoch 46/200\n",
      "1256/1256 [==============================] - 0s 106us/step - loss: 1.3556 - acc: 0.3861 - auc: 0.2388 - val_loss: 1.3472 - val_acc: 0.4984 - val_auc: 0.2415\n",
      "Epoch 47/200\n",
      "1256/1256 [==============================] - 0s 123us/step - loss: 1.4083 - acc: 0.3822 - auc: 0.2442 - val_loss: 1.3574 - val_acc: 0.4571 - val_auc: 0.2468\n",
      "Epoch 48/200\n",
      "1256/1256 [==============================] - 0s 127us/step - loss: 1.3683 - acc: 0.3861 - auc: 0.2494 - val_loss: 1.3472 - val_acc: 0.4794 - val_auc: 0.2516\n",
      "Epoch 49/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1256/1256 [==============================] - 0s 136us/step - loss: 1.3305 - acc: 0.3973 - auc: 0.2541 - val_loss: 1.3377 - val_acc: 0.4730 - val_auc: 0.2566\n",
      "Epoch 50/200\n",
      "1256/1256 [==============================] - 0s 142us/step - loss: 1.4383 - acc: 0.3885 - auc: 0.2586 - val_loss: 1.3472 - val_acc: 0.4349 - val_auc: 0.2603\n",
      "Epoch 51/200\n",
      "1256/1256 [==============================] - 0s 112us/step - loss: 1.3551 - acc: 0.3742 - auc: 0.2620 - val_loss: 1.3365 - val_acc: 0.4762 - val_auc: 0.2638\n",
      "Epoch 52/200\n",
      "1256/1256 [==============================] - 0s 112us/step - loss: 1.3351 - acc: 0.4188 - auc: 0.2660 - val_loss: 1.3253 - val_acc: 0.5175 - val_auc: 0.2683\n",
      "Epoch 53/200\n",
      "1256/1256 [==============================] - 0s 123us/step - loss: 1.3380 - acc: 0.4323 - auc: 0.2707 - val_loss: 1.3232 - val_acc: 0.5333 - val_auc: 0.2735\n",
      "Epoch 54/200\n",
      "1256/1256 [==============================] - 0s 108us/step - loss: 1.3096 - acc: 0.4204 - auc: 0.2761 - val_loss: 1.3083 - val_acc: 0.5238 - val_auc: 0.2786\n",
      "Epoch 55/200\n",
      "1256/1256 [==============================] - 0s 93us/step - loss: 1.3413 - acc: 0.3901 - auc: 0.2805 - val_loss: 1.3159 - val_acc: 0.5079 - val_auc: 0.2825\n",
      "Epoch 56/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 1.3437 - acc: 0.4076 - auc: 0.2846 - val_loss: 1.3114 - val_acc: 0.5333 - val_auc: 0.2869\n",
      "Epoch 57/200\n",
      "1256/1256 [==============================] - 0s 131us/step - loss: 1.3475 - acc: 0.4156 - auc: 0.2893 - val_loss: 1.3151 - val_acc: 0.5270 - val_auc: 0.2914\n",
      "Epoch 58/200\n",
      "1256/1256 [==============================] - 0s 122us/step - loss: 1.3185 - acc: 0.4578 - auc: 0.2938 - val_loss: 1.3109 - val_acc: 0.5333 - val_auc: 0.2962\n",
      "Epoch 59/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 1.3689 - acc: 0.4053 - auc: 0.2984 - val_loss: 1.3223 - val_acc: 0.4540 - val_auc: 0.2998\n",
      "Epoch 60/200\n",
      "1256/1256 [==============================] - 0s 101us/step - loss: 1.3196 - acc: 0.3965 - auc: 0.3013 - val_loss: 1.3193 - val_acc: 0.5111 - val_auc: 0.3029\n",
      "Epoch 61/200\n",
      "1256/1256 [==============================] - 0s 138us/step - loss: 1.3281 - acc: 0.4260 - auc: 0.3047 - val_loss: 1.3169 - val_acc: 0.5365 - val_auc: 0.3067\n",
      "Epoch 62/200\n",
      "1256/1256 [==============================] - 0s 108us/step - loss: 1.2812 - acc: 0.4506 - auc: 0.3089 - val_loss: 1.3030 - val_acc: 0.5460 - val_auc: 0.3110\n",
      "Epoch 63/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.2640 - acc: 0.4602 - auc: 0.3134 - val_loss: 1.2944 - val_acc: 0.5683 - val_auc: 0.3156\n",
      "Epoch 64/200\n",
      "1256/1256 [==============================] - 0s 103us/step - loss: 1.2898 - acc: 0.4554 - auc: 0.3178 - val_loss: 1.2886 - val_acc: 0.5524 - val_auc: 0.3200\n",
      "Epoch 65/200\n",
      "1256/1256 [==============================] - 0s 129us/step - loss: 1.2918 - acc: 0.4697 - auc: 0.3223 - val_loss: 1.2863 - val_acc: 0.5619 - val_auc: 0.3244\n",
      "Epoch 66/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.3097 - acc: 0.4498 - auc: 0.3264 - val_loss: 1.2831 - val_acc: 0.5619 - val_auc: 0.3283\n",
      "Epoch 67/200\n",
      "1256/1256 [==============================] - 0s 107us/step - loss: 1.2758 - acc: 0.4498 - auc: 0.3303 - val_loss: 1.2804 - val_acc: 0.5683 - val_auc: 0.3324\n",
      "Epoch 68/200\n",
      "1256/1256 [==============================] - 0s 107us/step - loss: 1.2567 - acc: 0.4825 - auc: 0.3344 - val_loss: 1.2708 - val_acc: 0.5524 - val_auc: 0.3367\n",
      "Epoch 69/200\n",
      "1256/1256 [==============================] - 0s 105us/step - loss: 1.2659 - acc: 0.4705 - auc: 0.3388 - val_loss: 1.2705 - val_acc: 0.5587 - val_auc: 0.3408\n",
      "Epoch 70/200\n",
      "1256/1256 [==============================] - 0s 114us/step - loss: 1.2758 - acc: 0.4761 - auc: 0.3427 - val_loss: 1.2766 - val_acc: 0.5587 - val_auc: 0.3447\n",
      "Epoch 71/200\n",
      "1256/1256 [==============================] - 0s 119us/step - loss: 1.2769 - acc: 0.4713 - auc: 0.3468 - val_loss: 1.2717 - val_acc: 0.5397 - val_auc: 0.3487\n",
      "Epoch 72/200\n",
      "1256/1256 [==============================] - 0s 103us/step - loss: 1.3947 - acc: 0.4602 - auc: 0.3505 - val_loss: 1.2738 - val_acc: 0.5302 - val_auc: 0.3520\n",
      "Epoch 73/200\n",
      "1256/1256 [==============================] - 0s 97us/step - loss: 1.2584 - acc: 0.4554 - auc: 0.3535 - val_loss: 1.2761 - val_acc: 0.5460 - val_auc: 0.3549\n",
      "Epoch 74/200\n",
      "1256/1256 [==============================] - 0s 101us/step - loss: 1.2681 - acc: 0.4602 - auc: 0.3565 - val_loss: 1.2694 - val_acc: 0.5556 - val_auc: 0.3582\n",
      "Epoch 75/200\n",
      "1256/1256 [==============================] - 0s 103us/step - loss: 1.2530 - acc: 0.4745 - auc: 0.3600 - val_loss: 1.2650 - val_acc: 0.5524 - val_auc: 0.3618\n",
      "Epoch 76/200\n",
      "1256/1256 [==============================] - 0s 104us/step - loss: 1.2309 - acc: 0.4857 - auc: 0.3637 - val_loss: 1.2631 - val_acc: 0.5587 - val_auc: 0.3654\n",
      "Epoch 77/200\n",
      "1256/1256 [==============================] - 0s 97us/step - loss: 1.2755 - acc: 0.4825 - auc: 0.3673 - val_loss: 1.2665 - val_acc: 0.5587 - val_auc: 0.3688\n",
      "Epoch 78/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 1.2682 - acc: 0.4721 - auc: 0.3705 - val_loss: 1.2652 - val_acc: 0.5556 - val_auc: 0.3720\n",
      "Epoch 79/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 1.2798 - acc: 0.4889 - auc: 0.3736 - val_loss: 1.2649 - val_acc: 0.5524 - val_auc: 0.3752\n",
      "Epoch 80/200\n",
      "1256/1256 [==============================] - 0s 101us/step - loss: 1.2816 - acc: 0.4968 - auc: 0.3767 - val_loss: 1.2725 - val_acc: 0.5556 - val_auc: 0.3784\n",
      "Epoch 81/200\n",
      "1256/1256 [==============================] - 0s 104us/step - loss: 1.2548 - acc: 0.4896 - auc: 0.3800 - val_loss: 1.2639 - val_acc: 0.5365 - val_auc: 0.3815\n",
      "Epoch 82/200\n",
      "1256/1256 [==============================] - 0s 105us/step - loss: 1.2304 - acc: 0.4865 - auc: 0.3830 - val_loss: 1.2603 - val_acc: 0.5429 - val_auc: 0.3844\n",
      "Epoch 83/200\n",
      "1256/1256 [==============================] - 0s 97us/step - loss: 1.2255 - acc: 0.4992 - auc: 0.3860 - val_loss: 1.2615 - val_acc: 0.5619 - val_auc: 0.3873\n",
      "Epoch 84/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.2001 - acc: 0.5000 - auc: 0.3888 - val_loss: 1.2623 - val_acc: 0.5714 - val_auc: 0.3903\n",
      "Epoch 85/200\n",
      "1256/1256 [==============================] - 0s 93us/step - loss: 1.3039 - acc: 0.4761 - auc: 0.3916 - val_loss: 1.2677 - val_acc: 0.5841 - val_auc: 0.3930\n",
      "Epoch 86/200\n",
      "1256/1256 [==============================] - 0s 89us/step - loss: 1.2487 - acc: 0.5096 - auc: 0.3945 - val_loss: 1.2597 - val_acc: 0.5746 - val_auc: 0.3959\n",
      "Epoch 87/200\n",
      "1256/1256 [==============================] - 0s 90us/step - loss: 1.1743 - acc: 0.5008 - auc: 0.3974 - val_loss: 1.2660 - val_acc: 0.5968 - val_auc: 0.3988\n",
      "Epoch 88/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.2082 - acc: 0.5350 - auc: 0.4006 - val_loss: 1.2675 - val_acc: 0.5937 - val_auc: 0.4022\n",
      "Epoch 89/200\n",
      "1256/1256 [==============================] - 0s 99us/step - loss: 1.1967 - acc: 0.5239 - auc: 0.4038 - val_loss: 1.2704 - val_acc: 0.6000 - val_auc: 0.4053\n",
      "Epoch 90/200\n",
      "1256/1256 [==============================] - 0s 101us/step - loss: 1.2765 - acc: 0.5183 - auc: 0.4067 - val_loss: 1.2631 - val_acc: 0.5746 - val_auc: 0.4082\n",
      "Epoch 91/200\n",
      "1256/1256 [==============================] - 0s 97us/step - loss: 1.1924 - acc: 0.5215 - auc: 0.4096 - val_loss: 1.2666 - val_acc: 0.5841 - val_auc: 0.4111\n",
      "Epoch 92/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 1.2780 - acc: 0.5191 - auc: 0.4125 - val_loss: 1.2664 - val_acc: 0.5746 - val_auc: 0.4137\n",
      "Epoch 93/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 1.1605 - acc: 0.5088 - auc: 0.4150 - val_loss: 1.2796 - val_acc: 0.6063 - val_auc: 0.4164\n",
      "Epoch 94/200\n",
      "1256/1256 [==============================] - 0s 99us/step - loss: 1.2152 - acc: 0.5366 - auc: 0.4180 - val_loss: 1.2661 - val_acc: 0.5873 - val_auc: 0.4194\n",
      "Epoch 95/200\n",
      "1256/1256 [==============================] - 0s 98us/step - loss: 1.2124 - acc: 0.5183 - auc: 0.4207 - val_loss: 1.2558 - val_acc: 0.5810 - val_auc: 0.4220\n",
      "Epoch 96/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 1.2772 - acc: 0.4833 - auc: 0.4231 - val_loss: 1.2436 - val_acc: 0.5619 - val_auc: 0.4242\n",
      "Epoch 97/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1256/1256 [==============================] - 0s 98us/step - loss: 1.2171 - acc: 0.4984 - auc: 0.4253 - val_loss: 1.2520 - val_acc: 0.5873 - val_auc: 0.4263\n",
      "Epoch 98/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.2561 - acc: 0.5159 - auc: 0.4275 - val_loss: 1.2547 - val_acc: 0.5841 - val_auc: 0.4288\n",
      "Epoch 99/200\n",
      "1256/1256 [==============================] - 0s 93us/step - loss: 1.2112 - acc: 0.5048 - auc: 0.4299 - val_loss: 1.2540 - val_acc: 0.5873 - val_auc: 0.4310\n",
      "Epoch 100/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 1.2458 - acc: 0.5048 - auc: 0.4321 - val_loss: 1.2498 - val_acc: 0.5841 - val_auc: 0.4332\n",
      "Epoch 101/200\n",
      "1256/1256 [==============================] - 0s 91us/step - loss: 1.2705 - acc: 0.4825 - auc: 0.4342 - val_loss: 1.2435 - val_acc: 0.5429 - val_auc: 0.4350\n",
      "Epoch 102/200\n",
      "1256/1256 [==============================] - 0s 92us/step - loss: 1.1886 - acc: 0.5016 - auc: 0.4360 - val_loss: 1.2554 - val_acc: 0.5841 - val_auc: 0.4369\n",
      "Epoch 103/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 1.2004 - acc: 0.4976 - auc: 0.4379 - val_loss: 1.2567 - val_acc: 0.5841 - val_auc: 0.4390\n",
      "Epoch 104/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.2551 - acc: 0.5056 - auc: 0.4399 - val_loss: 1.2526 - val_acc: 0.5619 - val_auc: 0.4409\n",
      "Epoch 105/200\n",
      "1256/1256 [==============================] - 0s 97us/step - loss: 1.1542 - acc: 0.5119 - auc: 0.4418 - val_loss: 1.2538 - val_acc: 0.5841 - val_auc: 0.4429\n",
      "Epoch 106/200\n",
      "1256/1256 [==============================] - 0s 98us/step - loss: 1.3091 - acc: 0.5080 - auc: 0.4438 - val_loss: 1.2551 - val_acc: 0.5841 - val_auc: 0.4448\n",
      "Epoch 107/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.2576 - acc: 0.5000 - auc: 0.4458 - val_loss: 1.2541 - val_acc: 0.5587 - val_auc: 0.4465\n",
      "Epoch 108/200\n",
      "1256/1256 [==============================] - 0s 99us/step - loss: 1.1855 - acc: 0.5159 - auc: 0.4474 - val_loss: 1.2563 - val_acc: 0.5905 - val_auc: 0.4483\n",
      "Epoch 109/200\n",
      "1256/1256 [==============================] - 0s 99us/step - loss: 1.1630 - acc: 0.5104 - auc: 0.4492 - val_loss: 1.2605 - val_acc: 0.6000 - val_auc: 0.4502\n",
      "Epoch 110/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 1.2044 - acc: 0.5287 - auc: 0.4513 - val_loss: 1.2703 - val_acc: 0.6063 - val_auc: 0.4524\n",
      "Epoch 111/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.1863 - acc: 0.5318 - auc: 0.4534 - val_loss: 1.2644 - val_acc: 0.6000 - val_auc: 0.4544\n",
      "Epoch 112/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.1385 - acc: 0.5406 - auc: 0.4555 - val_loss: 1.2710 - val_acc: 0.6063 - val_auc: 0.4566\n",
      "Epoch 113/200\n",
      "1256/1256 [==============================] - 0s 88us/step - loss: 1.2080 - acc: 0.5478 - auc: 0.4578 - val_loss: 1.2734 - val_acc: 0.5968 - val_auc: 0.4588\n",
      "Epoch 114/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.1755 - acc: 0.5342 - auc: 0.4598 - val_loss: 1.2764 - val_acc: 0.5937 - val_auc: 0.4607\n",
      "Epoch 115/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 1.1973 - acc: 0.5167 - auc: 0.4616 - val_loss: 1.2678 - val_acc: 0.5873 - val_auc: 0.4626\n",
      "Epoch 116/200\n",
      "1256/1256 [==============================] - 0s 92us/step - loss: 1.2003 - acc: 0.5303 - auc: 0.4635 - val_loss: 1.2748 - val_acc: 0.5810 - val_auc: 0.4644\n",
      "Epoch 117/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 1.1245 - acc: 0.5303 - auc: 0.4653 - val_loss: 1.2945 - val_acc: 0.6032 - val_auc: 0.4663\n",
      "Epoch 118/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 1.1870 - acc: 0.5303 - auc: 0.4672 - val_loss: 1.2891 - val_acc: 0.6000 - val_auc: 0.4681\n",
      "Epoch 119/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 1.1426 - acc: 0.5406 - auc: 0.4691 - val_loss: 1.2887 - val_acc: 0.6095 - val_auc: 0.4700\n",
      "Epoch 120/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 1.1695 - acc: 0.5470 - auc: 0.4710 - val_loss: 1.2853 - val_acc: 0.6000 - val_auc: 0.4719\n",
      "Epoch 121/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.1689 - acc: 0.5438 - auc: 0.4729 - val_loss: 1.2899 - val_acc: 0.5968 - val_auc: 0.4738\n",
      "Epoch 122/200\n",
      "1256/1256 [==============================] - 0s 97us/step - loss: 1.1571 - acc: 0.5462 - auc: 0.4748 - val_loss: 1.2924 - val_acc: 0.6032 - val_auc: 0.4757\n",
      "Epoch 123/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.2049 - acc: 0.5143 - auc: 0.4765 - val_loss: 1.2591 - val_acc: 0.5714 - val_auc: 0.4772\n",
      "Epoch 124/200\n",
      "1256/1256 [==============================] - 0s 91us/step - loss: 1.1631 - acc: 0.5223 - auc: 0.4780 - val_loss: 1.2696 - val_acc: 0.5873 - val_auc: 0.4788\n",
      "Epoch 125/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 1.1695 - acc: 0.5119 - auc: 0.4795 - val_loss: 1.2641 - val_acc: 0.5841 - val_auc: 0.4802\n",
      "Epoch 126/200\n",
      "1256/1256 [==============================] - 0s 91us/step - loss: 1.1892 - acc: 0.5199 - auc: 0.4811 - val_loss: 1.2635 - val_acc: 0.5937 - val_auc: 0.4818\n",
      "Epoch 127/200\n",
      "1256/1256 [==============================] - 0s 85us/step - loss: 1.1205 - acc: 0.5358 - auc: 0.4826 - val_loss: 1.2708 - val_acc: 0.5968 - val_auc: 0.4834\n",
      "Epoch 128/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 1.1484 - acc: 0.5557 - auc: 0.4842 - val_loss: 1.2945 - val_acc: 0.6095 - val_auc: 0.4851\n",
      "Epoch 129/200\n",
      "1256/1256 [==============================] - 0s 92us/step - loss: 1.2373 - acc: 0.5390 - auc: 0.4859 - val_loss: 1.2674 - val_acc: 0.5778 - val_auc: 0.4866\n",
      "Epoch 130/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.2011 - acc: 0.5271 - auc: 0.4873 - val_loss: 1.2671 - val_acc: 0.5810 - val_auc: 0.4880\n",
      "Epoch 131/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 1.2172 - acc: 0.5175 - auc: 0.4887 - val_loss: 1.2635 - val_acc: 0.5841 - val_auc: 0.4893\n",
      "Epoch 132/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 1.1602 - acc: 0.5398 - auc: 0.4900 - val_loss: 1.2690 - val_acc: 0.5937 - val_auc: 0.4907\n",
      "Epoch 133/200\n",
      "1256/1256 [==============================] - 0s 92us/step - loss: 1.1669 - acc: 0.5342 - auc: 0.4915 - val_loss: 1.2836 - val_acc: 0.6000 - val_auc: 0.4922\n",
      "Epoch 134/200\n",
      "1256/1256 [==============================] - 0s 91us/step - loss: 1.1539 - acc: 0.5589 - auc: 0.4931 - val_loss: 1.2941 - val_acc: 0.6000 - val_auc: 0.4939\n",
      "Epoch 135/200\n",
      "1256/1256 [==============================] - 0s 92us/step - loss: 1.1940 - acc: 0.5271 - auc: 0.4946 - val_loss: 1.2700 - val_acc: 0.5810 - val_auc: 0.4952\n",
      "Epoch 136/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 1.1185 - acc: 0.5255 - auc: 0.4959 - val_loss: 1.2805 - val_acc: 0.5937 - val_auc: 0.4966\n",
      "Epoch 137/200\n",
      "1256/1256 [==============================] - 0s 89us/step - loss: 1.2153 - acc: 0.5454 - auc: 0.4973 - val_loss: 1.2611 - val_acc: 0.5905 - val_auc: 0.4980\n",
      "Epoch 138/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.2067 - acc: 0.5056 - auc: 0.4985 - val_loss: 1.2593 - val_acc: 0.5778 - val_auc: 0.4990\n",
      "Epoch 139/200\n",
      "1256/1256 [==============================] - 0s 84us/step - loss: 1.1228 - acc: 0.5311 - auc: 0.4996 - val_loss: 1.2761 - val_acc: 0.6032 - val_auc: 0.5002\n",
      "Epoch 140/200\n",
      "1256/1256 [==============================] - 0s 91us/step - loss: 1.1011 - acc: 0.5510 - auc: 0.5010 - val_loss: 1.3126 - val_acc: 0.6190 - val_auc: 0.5016\n",
      "Epoch 141/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.1619 - acc: 0.5581 - auc: 0.5024 - val_loss: 1.3114 - val_acc: 0.6159 - val_auc: 0.5032\n",
      "Epoch 142/200\n",
      "1256/1256 [==============================] - 0s 89us/step - loss: 1.1614 - acc: 0.5326 - auc: 0.5039 - val_loss: 1.2925 - val_acc: 0.6095 - val_auc: 0.5045\n",
      "Epoch 143/200\n",
      "1256/1256 [==============================] - 0s 97us/step - loss: 1.1472 - acc: 0.5629 - auc: 0.5053 - val_loss: 1.2787 - val_acc: 0.5841 - val_auc: 0.5060\n",
      "Epoch 144/200\n",
      "1256/1256 [==============================] - 0s 92us/step - loss: 1.1270 - acc: 0.5549 - auc: 0.5067 - val_loss: 1.2990 - val_acc: 0.6095 - val_auc: 0.5074\n",
      "Epoch 145/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1256/1256 [==============================] - 0s 85us/step - loss: 1.2099 - acc: 0.5438 - auc: 0.5081 - val_loss: 1.2575 - val_acc: 0.5714 - val_auc: 0.5086\n",
      "Epoch 146/200\n",
      "1256/1256 [==============================] - 0s 99us/step - loss: 1.2263 - acc: 0.5366 - auc: 0.5092 - val_loss: 1.2626 - val_acc: 0.5778 - val_auc: 0.5098\n",
      "Epoch 147/200\n",
      "1256/1256 [==============================] - 0s 99us/step - loss: 1.2188 - acc: 0.5390 - auc: 0.5103 - val_loss: 1.2644 - val_acc: 0.5683 - val_auc: 0.5109\n",
      "Epoch 148/200\n",
      "1256/1256 [==============================] - 0s 92us/step - loss: 1.1898 - acc: 0.5215 - auc: 0.5114 - val_loss: 1.2646 - val_acc: 0.5714 - val_auc: 0.5119\n",
      "Epoch 149/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.1328 - acc: 0.5151 - auc: 0.5123 - val_loss: 1.2818 - val_acc: 0.5968 - val_auc: 0.5128\n",
      "Epoch 150/200\n",
      "1256/1256 [==============================] - 0s 98us/step - loss: 1.0991 - acc: 0.5661 - auc: 0.5135 - val_loss: 1.2987 - val_acc: 0.6063 - val_auc: 0.5142\n",
      "Epoch 151/200\n",
      "1256/1256 [==============================] - 0s 99us/step - loss: 1.1061 - acc: 0.5573 - auc: 0.5148 - val_loss: 1.3193 - val_acc: 0.6159 - val_auc: 0.5155\n",
      "Epoch 152/200\n",
      "1256/1256 [==============================] - 0s 93us/step - loss: 1.1458 - acc: 0.5549 - auc: 0.5161 - val_loss: 1.2886 - val_acc: 0.5841 - val_auc: 0.5167\n",
      "Epoch 153/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.1348 - acc: 0.5541 - auc: 0.5173 - val_loss: 1.3120 - val_acc: 0.6127 - val_auc: 0.5179\n",
      "Epoch 154/200\n",
      "1256/1256 [==============================] - 0s 87us/step - loss: 1.1202 - acc: 0.5605 - auc: 0.5186 - val_loss: 1.3040 - val_acc: 0.5905 - val_auc: 0.5192\n",
      "Epoch 155/200\n",
      "1256/1256 [==============================] - 0s 97us/step - loss: 1.1376 - acc: 0.5478 - auc: 0.5198 - val_loss: 1.3128 - val_acc: 0.6063 - val_auc: 0.5204\n",
      "Epoch 156/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 1.1259 - acc: 0.5398 - auc: 0.5209 - val_loss: 1.3132 - val_acc: 0.6032 - val_auc: 0.5215\n",
      "Epoch 157/200\n",
      "1256/1256 [==============================] - 0s 89us/step - loss: 1.2437 - acc: 0.5478 - auc: 0.5220 - val_loss: 1.2715 - val_acc: 0.5524 - val_auc: 0.5225\n",
      "Epoch 158/200\n",
      "1256/1256 [==============================] - 0s 90us/step - loss: 1.1900 - acc: 0.5104 - auc: 0.5229 - val_loss: 1.2643 - val_acc: 0.5524 - val_auc: 0.5233\n",
      "Epoch 159/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 1.2026 - acc: 0.5199 - auc: 0.5238 - val_loss: 1.2638 - val_acc: 0.5492 - val_auc: 0.5242\n",
      "Epoch 160/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 1.1159 - acc: 0.5318 - auc: 0.5246 - val_loss: 1.2801 - val_acc: 0.5714 - val_auc: 0.5250\n",
      "Epoch 161/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.1526 - acc: 0.5358 - auc: 0.5255 - val_loss: 1.2738 - val_acc: 0.5746 - val_auc: 0.5260\n",
      "Epoch 162/200\n",
      "1256/1256 [==============================] - 0s 93us/step - loss: 1.1346 - acc: 0.5326 - auc: 0.5265 - val_loss: 1.3037 - val_acc: 0.6000 - val_auc: 0.5270\n",
      "Epoch 163/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.2342 - acc: 0.5175 - auc: 0.5275 - val_loss: 1.2757 - val_acc: 0.5714 - val_auc: 0.5279\n",
      "Epoch 164/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.1437 - acc: 0.5279 - auc: 0.5283 - val_loss: 1.2995 - val_acc: 0.5810 - val_auc: 0.5287\n",
      "Epoch 165/200\n",
      "1256/1256 [==============================] - 0s 93us/step - loss: 1.1404 - acc: 0.5573 - auc: 0.5292 - val_loss: 1.3115 - val_acc: 0.6159 - val_auc: 0.5297\n",
      "Epoch 166/200\n",
      "1256/1256 [==============================] - 0s 93us/step - loss: 1.1943 - acc: 0.5486 - auc: 0.5302 - val_loss: 1.3102 - val_acc: 0.6032 - val_auc: 0.5307\n",
      "Epoch 167/200\n",
      "1256/1256 [==============================] - 0s 90us/step - loss: 1.1753 - acc: 0.5446 - auc: 0.5311 - val_loss: 1.3043 - val_acc: 0.5937 - val_auc: 0.5316\n",
      "Epoch 168/200\n",
      "1256/1256 [==============================] - 0s 89us/step - loss: 1.0747 - acc: 0.5573 - auc: 0.5321 - val_loss: 1.3008 - val_acc: 0.5968 - val_auc: 0.5327\n",
      "Epoch 169/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.0732 - acc: 0.5430 - auc: 0.5331 - val_loss: 1.3049 - val_acc: 0.5810 - val_auc: 0.5336\n",
      "Epoch 170/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.1563 - acc: 0.5334 - auc: 0.5340 - val_loss: 1.3134 - val_acc: 0.5968 - val_auc: 0.5344\n",
      "Epoch 171/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.0859 - acc: 0.5677 - auc: 0.5349 - val_loss: 1.3330 - val_acc: 0.6159 - val_auc: 0.5355\n",
      "Epoch 172/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.1067 - acc: 0.5796 - auc: 0.5360 - val_loss: 1.3499 - val_acc: 0.6159 - val_auc: 0.5366\n",
      "Epoch 173/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.0854 - acc: 0.5725 - auc: 0.5372 - val_loss: 1.3564 - val_acc: 0.6095 - val_auc: 0.5377\n",
      "Epoch 174/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 1.1261 - acc: 0.5709 - auc: 0.5383 - val_loss: 1.3508 - val_acc: 0.6063 - val_auc: 0.5388\n",
      "Epoch 175/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.1104 - acc: 0.5422 - auc: 0.5393 - val_loss: 1.3167 - val_acc: 0.5873 - val_auc: 0.5397\n",
      "Epoch 176/200\n",
      "1256/1256 [==============================] - 0s 99us/step - loss: 1.1048 - acc: 0.5541 - auc: 0.5402 - val_loss: 1.3360 - val_acc: 0.5968 - val_auc: 0.5406\n",
      "Epoch 177/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 1.1186 - acc: 0.5557 - auc: 0.5411 - val_loss: 1.3637 - val_acc: 0.6159 - val_auc: 0.5416\n",
      "Epoch 178/200\n",
      "1256/1256 [==============================] - 0s 92us/step - loss: 1.0902 - acc: 0.5565 - auc: 0.5420 - val_loss: 1.3875 - val_acc: 0.6190 - val_auc: 0.5425\n",
      "Epoch 179/200\n",
      "1256/1256 [==============================] - 0s 99us/step - loss: 1.1450 - acc: 0.5645 - auc: 0.5431 - val_loss: 1.3338 - val_acc: 0.5810 - val_auc: 0.5436\n",
      "Epoch 180/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.0907 - acc: 0.5334 - auc: 0.5440 - val_loss: 1.3651 - val_acc: 0.5968 - val_auc: 0.5444\n",
      "Epoch 181/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.1427 - acc: 0.5661 - auc: 0.5449 - val_loss: 1.3405 - val_acc: 0.5905 - val_auc: 0.5453\n",
      "Epoch 182/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 1.0955 - acc: 0.5748 - auc: 0.5458 - val_loss: 1.3363 - val_acc: 0.5937 - val_auc: 0.5463\n",
      "Epoch 183/200\n",
      "1256/1256 [==============================] - 0s 98us/step - loss: 1.0559 - acc: 0.5860 - auc: 0.5468 - val_loss: 1.3704 - val_acc: 0.6063 - val_auc: 0.5473\n",
      "Epoch 184/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.1013 - acc: 0.5629 - auc: 0.5478 - val_loss: 1.3660 - val_acc: 0.6095 - val_auc: 0.5483\n",
      "Epoch 185/200\n",
      "1256/1256 [==============================] - 0s 103us/step - loss: 1.1137 - acc: 0.5820 - auc: 0.5488 - val_loss: 1.4030 - val_acc: 0.6190 - val_auc: 0.5493\n",
      "Epoch 186/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.0785 - acc: 0.5764 - auc: 0.5498 - val_loss: 1.3793 - val_acc: 0.6095 - val_auc: 0.5503\n",
      "Epoch 187/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.1746 - acc: 0.5764 - auc: 0.5508 - val_loss: 1.3624 - val_acc: 0.6127 - val_auc: 0.5512\n",
      "Epoch 188/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 1.1762 - acc: 0.5390 - auc: 0.5516 - val_loss: 1.2971 - val_acc: 0.5683 - val_auc: 0.5520\n",
      "Epoch 189/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.0756 - acc: 0.5398 - auc: 0.5523 - val_loss: 1.3112 - val_acc: 0.5937 - val_auc: 0.5527\n",
      "Epoch 190/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.0622 - acc: 0.5701 - auc: 0.5531 - val_loss: 1.3454 - val_acc: 0.6159 - val_auc: 0.5535\n",
      "Epoch 191/200\n",
      "1256/1256 [==============================] - 0s 90us/step - loss: 1.0561 - acc: 0.5924 - auc: 0.5540 - val_loss: 1.3599 - val_acc: 0.6190 - val_auc: 0.5545\n",
      "Epoch 192/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 1.1020 - acc: 0.5780 - auc: 0.5550 - val_loss: 1.3341 - val_acc: 0.6063 - val_auc: 0.5554\n",
      "Epoch 193/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1256/1256 [==============================] - 0s 92us/step - loss: 1.1300 - acc: 0.5796 - auc: 0.5559 - val_loss: 1.3527 - val_acc: 0.6032 - val_auc: 0.5563\n",
      "Epoch 194/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 1.1145 - acc: 0.5645 - auc: 0.5567 - val_loss: 1.3556 - val_acc: 0.6095 - val_auc: 0.5571\n",
      "Epoch 195/200\n",
      "1256/1256 [==============================] - 0s 92us/step - loss: 1.1296 - acc: 0.5621 - auc: 0.5575 - val_loss: 1.3630 - val_acc: 0.6063 - val_auc: 0.5580\n",
      "Epoch 196/200\n",
      "1256/1256 [==============================] - 0s 93us/step - loss: 1.1150 - acc: 0.5573 - auc: 0.5584 - val_loss: 1.3624 - val_acc: 0.6000 - val_auc: 0.5587\n",
      "Epoch 197/200\n",
      "1256/1256 [==============================] - 0s 91us/step - loss: 1.1049 - acc: 0.5717 - auc: 0.5591 - val_loss: 1.3448 - val_acc: 0.5841 - val_auc: 0.5595\n",
      "Epoch 198/200\n",
      "1256/1256 [==============================] - 0s 90us/step - loss: 1.0603 - acc: 0.5533 - auc: 0.5599 - val_loss: 1.3637 - val_acc: 0.5968 - val_auc: 0.5603\n",
      "Epoch 199/200\n",
      "1256/1256 [==============================] - 0s 93us/step - loss: 1.1869 - acc: 0.5533 - auc: 0.5607 - val_loss: 1.3247 - val_acc: 0.5651 - val_auc: 0.5609\n",
      "Epoch 200/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.1185 - acc: 0.5621 - auc: 0.5613 - val_loss: 1.3384 - val_acc: 0.5778 - val_auc: 0.5616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1236d05c0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_x, df_train_y = split_x_y(df_train)\n",
    "model = build_model(input_dim=df_train_x.shape[1])\n",
    "model.fit(df_train_x, df_train_y,validation_split=0.2,\n",
    "          batch_size=32, epochs=200, verbose=1,\n",
    "         class_weight={0:13,1:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are happy with your model training, run the below cell to generate the final labels for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_testing_data\n",
    "test_data, _,y_test = get_testing_data(data_root)\n",
    "test_data = preprocess_dataset(test_data,scaler=scaler)\n",
    "predicted = model.predict_classes(test_data)\n",
    "with open(\"exercise_1_output.txt\", \"w\") as f:\n",
    "    [f.write(\"{}\\n\".format(p)) for p in predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7120535714285714"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum(predicted==np.argmax(y_test,axis=1))/len(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Exercise 4:\n",
    "(OPTIONAL)\n",
    "\n",
    "Do you find any relationship between Age and whether a person has Diabetes or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11b060c50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b06e358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x=df.Diabetes,y=df.Age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**:\n",
    "The greater the age, the higher chance of having Diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Exercise 5:\n",
    "(OPTIONAL)\n",
    "\n",
    "Predict whether the person sleeps for less than mean sleeping hours across the dataset or more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sleep_less\"] = df[\"SleepHours\"]<0\n",
    "df[\"Sleep_less\"] = df[\"Sleep_less\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "df = pd.get_dummies(df,prefix=[\"Diabetes\",\"Race\"],columns=[\"Diabetes\",\"Race\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df,train_size=0.7)\n",
    "df_train_x,df_train_y = split_x_y(df_train,\"Sleep_less\")\n",
    "df_test_x,df_test_y = split_x_y(df_test,\"Sleep_less\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 879 samples, validate on 220 samples\n",
      "Epoch 1/100\n",
      "879/879 [==============================] - 1s 1ms/step - loss: 0.9960 - acc: 0.3959 - auc: 0.3846 - val_loss: 0.6936 - val_acc: 0.5364 - val_auc: 0.4068\n",
      "Epoch 2/100\n",
      "879/879 [==============================] - 0s 102us/step - loss: 0.7584 - acc: 0.5142 - auc: 0.4210 - val_loss: 0.6582 - val_acc: 0.6636 - val_auc: 0.4505\n",
      "Epoch 3/100\n",
      "879/879 [==============================] - 0s 105us/step - loss: 0.6880 - acc: 0.5927 - auc: 0.4777 - val_loss: 0.6570 - val_acc: 0.6227 - val_auc: 0.5015\n",
      "Epoch 4/100\n",
      "879/879 [==============================] - 0s 103us/step - loss: 0.6579 - acc: 0.6280 - auc: 0.5201 - val_loss: 0.6578 - val_acc: 0.6136 - val_auc: 0.5386\n",
      "Epoch 5/100\n",
      "879/879 [==============================] - 0s 111us/step - loss: 0.6464 - acc: 0.6621 - auc: 0.5533 - val_loss: 0.6555 - val_acc: 0.6136 - val_auc: 0.5672\n",
      "Epoch 6/100\n",
      "879/879 [==============================] - 0s 131us/step - loss: 0.6543 - acc: 0.6325 - auc: 0.5765 - val_loss: 0.6509 - val_acc: 0.6136 - val_auc: 0.5840\n",
      "Epoch 7/100\n",
      "879/879 [==============================] - 0s 128us/step - loss: 0.6411 - acc: 0.6610 - auc: 0.5919 - val_loss: 0.6472 - val_acc: 0.6091 - val_auc: 0.5995\n",
      "Epoch 8/100\n",
      "879/879 [==============================] - 0s 118us/step - loss: 0.6210 - acc: 0.6837 - auc: 0.6079 - val_loss: 0.6401 - val_acc: 0.6091 - val_auc: 0.6143\n",
      "Epoch 9/100\n",
      "879/879 [==============================] - 0s 102us/step - loss: 0.6340 - acc: 0.6769 - auc: 0.6196 - val_loss: 0.6310 - val_acc: 0.6136 - val_auc: 0.6259\n",
      "Epoch 10/100\n",
      "879/879 [==============================] - 0s 88us/step - loss: 0.6123 - acc: 0.6849 - auc: 0.6317 - val_loss: 0.6222 - val_acc: 0.6136 - val_auc: 0.6371\n",
      "Epoch 11/100\n",
      "879/879 [==============================] - 0s 100us/step - loss: 0.6211 - acc: 0.6769 - auc: 0.6412 - val_loss: 0.6123 - val_acc: 0.6273 - val_auc: 0.6458\n",
      "Epoch 12/100\n",
      "879/879 [==============================] - 0s 132us/step - loss: 0.6061 - acc: 0.6985 - auc: 0.6501 - val_loss: 0.6002 - val_acc: 0.6364 - val_auc: 0.6546\n",
      "Epoch 13/100\n",
      "879/879 [==============================] - 0s 128us/step - loss: 0.5861 - acc: 0.7133 - auc: 0.6599 - val_loss: 0.5864 - val_acc: 0.6636 - val_auc: 0.6642\n",
      "Epoch 14/100\n",
      "879/879 [==============================] - 0s 103us/step - loss: 0.5871 - acc: 0.6985 - auc: 0.6691 - val_loss: 0.5724 - val_acc: 0.6682 - val_auc: 0.6730\n",
      "Epoch 15/100\n",
      "879/879 [==============================] - 0s 93us/step - loss: 0.5667 - acc: 0.7167 - auc: 0.6774 - val_loss: 0.5531 - val_acc: 0.6864 - val_auc: 0.6821\n",
      "Epoch 16/100\n",
      "879/879 [==============================] - 0s 88us/step - loss: 0.5711 - acc: 0.7156 - auc: 0.6863 - val_loss: 0.5341 - val_acc: 0.6955 - val_auc: 0.6905\n",
      "Epoch 17/100\n",
      "879/879 [==============================] - 0s 97us/step - loss: 0.5675 - acc: 0.7156 - auc: 0.6943 - val_loss: 0.5190 - val_acc: 0.7091 - val_auc: 0.6979\n",
      "Epoch 18/100\n",
      "879/879 [==============================] - 0s 132us/step - loss: 0.5385 - acc: 0.7452 - auc: 0.7020 - val_loss: 0.4962 - val_acc: 0.7455 - val_auc: 0.7066\n",
      "Epoch 19/100\n",
      "879/879 [==============================] - 0s 121us/step - loss: 0.5319 - acc: 0.7554 - auc: 0.7111 - val_loss: 0.4744 - val_acc: 0.7818 - val_auc: 0.7149\n",
      "Epoch 20/100\n",
      "879/879 [==============================] - 0s 96us/step - loss: 0.5329 - acc: 0.7565 - auc: 0.7185 - val_loss: 0.4539 - val_acc: 0.8045 - val_auc: 0.7223\n",
      "Epoch 21/100\n",
      "879/879 [==============================] - 0s 91us/step - loss: 0.5208 - acc: 0.7691 - auc: 0.7263 - val_loss: 0.4326 - val_acc: 0.8182 - val_auc: 0.7302\n",
      "Epoch 22/100\n",
      "879/879 [==============================] - 0s 89us/step - loss: 0.4943 - acc: 0.7895 - auc: 0.7348 - val_loss: 0.4136 - val_acc: 0.8500 - val_auc: 0.7386\n",
      "Epoch 23/100\n",
      "879/879 [==============================] - 0s 100us/step - loss: 0.4949 - acc: 0.7736 - auc: 0.7419 - val_loss: 0.3944 - val_acc: 0.8727 - val_auc: 0.7458\n",
      "Epoch 24/100\n",
      "879/879 [==============================] - 0s 124us/step - loss: 0.4811 - acc: 0.7793 - auc: 0.7493 - val_loss: 0.3769 - val_acc: 0.8909 - val_auc: 0.7527\n",
      "Epoch 25/100\n",
      "879/879 [==============================] - 0s 122us/step - loss: 0.4489 - acc: 0.8146 - auc: 0.7569 - val_loss: 0.3558 - val_acc: 0.9091 - val_auc: 0.7607\n",
      "Epoch 26/100\n",
      "879/879 [==============================] - 0s 92us/step - loss: 0.4555 - acc: 0.8077 - auc: 0.7643 - val_loss: 0.3299 - val_acc: 0.9227 - val_auc: 0.7675\n",
      "Epoch 27/100\n",
      "879/879 [==============================] - 0s 101us/step - loss: 0.4362 - acc: 0.8123 - auc: 0.7710 - val_loss: 0.3101 - val_acc: 0.9318 - val_auc: 0.7744\n",
      "Epoch 28/100\n",
      "879/879 [==============================] - 0s 101us/step - loss: 0.4346 - acc: 0.7975 - auc: 0.7774 - val_loss: 0.2918 - val_acc: 0.9409 - val_auc: 0.7806\n",
      "Epoch 29/100\n",
      "879/879 [==============================] - 0s 107us/step - loss: 0.4293 - acc: 0.8180 - auc: 0.7837 - val_loss: 0.2749 - val_acc: 0.9500 - val_auc: 0.7867\n",
      "Epoch 30/100\n",
      "879/879 [==============================] - 0s 131us/step - loss: 0.3983 - acc: 0.8407 - auc: 0.7901 - val_loss: 0.2559 - val_acc: 0.9545 - val_auc: 0.7931\n",
      "Epoch 31/100\n",
      "879/879 [==============================] - 0s 105us/step - loss: 0.4140 - acc: 0.8111 - auc: 0.7959 - val_loss: 0.2442 - val_acc: 0.9591 - val_auc: 0.7984\n",
      "Epoch 32/100\n",
      "879/879 [==============================] - 0s 101us/step - loss: 0.3967 - acc: 0.8350 - auc: 0.8014 - val_loss: 0.2316 - val_acc: 0.9636 - val_auc: 0.8039\n",
      "Epoch 33/100\n",
      "879/879 [==============================] - 0s 99us/step - loss: 0.3690 - acc: 0.8521 - auc: 0.8069 - val_loss: 0.2157 - val_acc: 0.9773 - val_auc: 0.8096\n",
      "Epoch 34/100\n",
      "879/879 [==============================] - 0s 92us/step - loss: 0.3781 - acc: 0.8476 - auc: 0.8123 - val_loss: 0.1993 - val_acc: 0.9909 - val_auc: 0.8149\n",
      "Epoch 35/100\n",
      "879/879 [==============================] - 0s 91us/step - loss: 0.3675 - acc: 0.8441 - auc: 0.8175 - val_loss: 0.1866 - val_acc: 0.9909 - val_auc: 0.8199\n",
      "Epoch 36/100\n",
      "879/879 [==============================] - 0s 98us/step - loss: 0.3343 - acc: 0.8737 - auc: 0.8226 - val_loss: 0.1754 - val_acc: 0.9909 - val_auc: 0.8252\n",
      "Epoch 37/100\n",
      "879/879 [==============================] - 0s 107us/step - loss: 0.3305 - acc: 0.8567 - auc: 0.8277 - val_loss: 0.1599 - val_acc: 0.9909 - val_auc: 0.8300\n",
      "Epoch 38/100\n",
      "879/879 [==============================] - 0s 100us/step - loss: 0.3172 - acc: 0.8908 - auc: 0.8327 - val_loss: 0.1468 - val_acc: 0.9955 - val_auc: 0.8351\n",
      "Epoch 39/100\n",
      "879/879 [==============================] - 0s 98us/step - loss: 0.2981 - acc: 0.8862 - auc: 0.8376 - val_loss: 0.1345 - val_acc: 0.9955 - val_auc: 0.8400\n",
      "Epoch 40/100\n",
      "879/879 [==============================] - 0s 93us/step - loss: 0.3218 - acc: 0.8578 - auc: 0.8421 - val_loss: 0.1279 - val_acc: 0.9955 - val_auc: 0.8440\n",
      "Epoch 41/100\n",
      "879/879 [==============================] - 0s 92us/step - loss: 0.3022 - acc: 0.8794 - auc: 0.8462 - val_loss: 0.1197 - val_acc: 0.9955 - val_auc: 0.8481\n",
      "Epoch 42/100\n",
      "879/879 [==============================] - 0s 94us/step - loss: 0.2846 - acc: 0.8805 - auc: 0.8502 - val_loss: 0.1091 - val_acc: 1.0000 - val_auc: 0.8522\n",
      "Epoch 43/100\n",
      "879/879 [==============================] - 0s 96us/step - loss: 0.2916 - acc: 0.8840 - auc: 0.8542 - val_loss: 0.1027 - val_acc: 1.0000 - val_auc: 0.8559\n",
      "Epoch 44/100\n",
      "879/879 [==============================] - 0s 95us/step - loss: 0.2541 - acc: 0.8987 - auc: 0.8580 - val_loss: 0.0958 - val_acc: 1.0000 - val_auc: 0.8600\n",
      "Epoch 45/100\n",
      "879/879 [==============================] - 0s 92us/step - loss: 0.2511 - acc: 0.9033 - auc: 0.8621 - val_loss: 0.0870 - val_acc: 1.0000 - val_auc: 0.8639\n",
      "Epoch 46/100\n",
      "879/879 [==============================] - 0s 91us/step - loss: 0.2509 - acc: 0.9067 - auc: 0.8657 - val_loss: 0.0796 - val_acc: 1.0000 - val_auc: 0.8675\n",
      "Epoch 47/100\n",
      "879/879 [==============================] - 0s 89us/step - loss: 0.2502 - acc: 0.9022 - auc: 0.8691 - val_loss: 0.0736 - val_acc: 1.0000 - val_auc: 0.8709\n",
      "Epoch 48/100\n",
      "879/879 [==============================] - 0s 98us/step - loss: 0.2562 - acc: 0.9010 - auc: 0.8725 - val_loss: 0.0682 - val_acc: 1.0000 - val_auc: 0.8740\n",
      "Epoch 49/100\n",
      "879/879 [==============================] - 0s 131us/step - loss: 0.2227 - acc: 0.9215 - auc: 0.8758 - val_loss: 0.0632 - val_acc: 1.0000 - val_auc: 0.8774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "879/879 [==============================] - 0s 115us/step - loss: 0.2212 - acc: 0.9078 - auc: 0.8790 - val_loss: 0.0586 - val_acc: 1.0000 - val_auc: 0.8804\n",
      "Epoch 51/100\n",
      "879/879 [==============================] - 0s 96us/step - loss: 0.2476 - acc: 0.8987 - auc: 0.8818 - val_loss: 0.0560 - val_acc: 1.0000 - val_auc: 0.8831\n",
      "Epoch 52/100\n",
      "879/879 [==============================] - 0s 90us/step - loss: 0.2198 - acc: 0.9124 - auc: 0.8847 - val_loss: 0.0539 - val_acc: 1.0000 - val_auc: 0.8859\n",
      "Epoch 53/100\n",
      "879/879 [==============================] - 0s 100us/step - loss: 0.1891 - acc: 0.9283 - auc: 0.8874 - val_loss: 0.0507 - val_acc: 1.0000 - val_auc: 0.8889\n",
      "Epoch 54/100\n",
      "879/879 [==============================] - 0s 88us/step - loss: 0.2039 - acc: 0.9261 - auc: 0.8903 - val_loss: 0.0471 - val_acc: 1.0000 - val_auc: 0.8915\n",
      "Epoch 55/100\n",
      "879/879 [==============================] - 0s 94us/step - loss: 0.1922 - acc: 0.9306 - auc: 0.8929 - val_loss: 0.0436 - val_acc: 1.0000 - val_auc: 0.8942\n",
      "Epoch 56/100\n",
      "879/879 [==============================] - 0s 99us/step - loss: 0.1822 - acc: 0.9352 - auc: 0.8956 - val_loss: 0.0404 - val_acc: 1.0000 - val_auc: 0.8968\n",
      "Epoch 57/100\n",
      "879/879 [==============================] - 0s 110us/step - loss: 0.1770 - acc: 0.9420 - auc: 0.8981 - val_loss: 0.0378 - val_acc: 1.0000 - val_auc: 0.8993\n",
      "Epoch 58/100\n",
      "879/879 [==============================] - 0s 109us/step - loss: 0.1713 - acc: 0.9408 - auc: 0.9005 - val_loss: 0.0352 - val_acc: 1.0000 - val_auc: 0.9017\n",
      "Epoch 59/100\n",
      "879/879 [==============================] - 0s 103us/step - loss: 0.1851 - acc: 0.9397 - auc: 0.9029 - val_loss: 0.0330 - val_acc: 1.0000 - val_auc: 0.9040\n",
      "Epoch 60/100\n",
      "879/879 [==============================] - 0s 97us/step - loss: 0.1636 - acc: 0.9465 - auc: 0.9052 - val_loss: 0.0305 - val_acc: 1.0000 - val_auc: 0.9063\n",
      "Epoch 61/100\n",
      "879/879 [==============================] - 0s 99us/step - loss: 0.1690 - acc: 0.9443 - auc: 0.9074 - val_loss: 0.0288 - val_acc: 1.0000 - val_auc: 0.9084\n",
      "Epoch 62/100\n",
      "879/879 [==============================] - 0s 96us/step - loss: 0.1507 - acc: 0.9454 - auc: 0.9096 - val_loss: 0.0273 - val_acc: 1.0000 - val_auc: 0.9106\n",
      "Epoch 63/100\n",
      "879/879 [==============================] - 0s 98us/step - loss: 0.1467 - acc: 0.9545 - auc: 0.9116 - val_loss: 0.0262 - val_acc: 1.0000 - val_auc: 0.9127\n",
      "Epoch 64/100\n",
      "879/879 [==============================] - 0s 90us/step - loss: 0.1553 - acc: 0.9454 - auc: 0.9136 - val_loss: 0.0246 - val_acc: 1.0000 - val_auc: 0.9146\n",
      "Epoch 65/100\n",
      "879/879 [==============================] - 0s 92us/step - loss: 0.1387 - acc: 0.9590 - auc: 0.9156 - val_loss: 0.0230 - val_acc: 1.0000 - val_auc: 0.9166\n",
      "Epoch 66/100\n",
      "879/879 [==============================] - 0s 99us/step - loss: 0.1569 - acc: 0.9454 - auc: 0.9175 - val_loss: 0.0215 - val_acc: 1.0000 - val_auc: 0.9184\n",
      "Epoch 67/100\n",
      "879/879 [==============================] - 0s 97us/step - loss: 0.1480 - acc: 0.9454 - auc: 0.9193 - val_loss: 0.0204 - val_acc: 1.0000 - val_auc: 0.9201\n",
      "Epoch 68/100\n",
      "879/879 [==============================] - 0s 98us/step - loss: 0.1432 - acc: 0.9545 - auc: 0.9210 - val_loss: 0.0194 - val_acc: 1.0000 - val_auc: 0.9218\n",
      "Epoch 69/100\n",
      "879/879 [==============================] - 0s 99us/step - loss: 0.1248 - acc: 0.9579 - auc: 0.9226 - val_loss: 0.0183 - val_acc: 1.0000 - val_auc: 0.9235\n",
      "Epoch 70/100\n",
      "879/879 [==============================] - 0s 98us/step - loss: 0.1374 - acc: 0.9534 - auc: 0.9243 - val_loss: 0.0173 - val_acc: 1.0000 - val_auc: 0.9251\n",
      "Epoch 71/100\n",
      "879/879 [==============================] - 0s 99us/step - loss: 0.1085 - acc: 0.9670 - auc: 0.9260 - val_loss: 0.0162 - val_acc: 1.0000 - val_auc: 0.9268\n",
      "Epoch 72/100\n",
      "879/879 [==============================] - 0s 97us/step - loss: 0.1333 - acc: 0.9545 - auc: 0.9275 - val_loss: 0.0156 - val_acc: 1.0000 - val_auc: 0.9282\n",
      "Epoch 73/100\n",
      "879/879 [==============================] - 0s 87us/step - loss: 0.1171 - acc: 0.9602 - auc: 0.9290 - val_loss: 0.0148 - val_acc: 1.0000 - val_auc: 0.9297\n",
      "Epoch 74/100\n",
      "879/879 [==============================] - 0s 91us/step - loss: 0.1135 - acc: 0.9636 - auc: 0.9304 - val_loss: 0.0142 - val_acc: 1.0000 - val_auc: 0.9312\n",
      "Epoch 75/100\n",
      "879/879 [==============================] - 0s 99us/step - loss: 0.1190 - acc: 0.9613 - auc: 0.9319 - val_loss: 0.0136 - val_acc: 1.0000 - val_auc: 0.9325\n",
      "Epoch 76/100\n",
      "879/879 [==============================] - 0s 93us/step - loss: 0.1331 - acc: 0.9568 - auc: 0.9332 - val_loss: 0.0130 - val_acc: 1.0000 - val_auc: 0.9338\n",
      "Epoch 77/100\n",
      "879/879 [==============================] - 0s 104us/step - loss: 0.1047 - acc: 0.9670 - auc: 0.9345 - val_loss: 0.0126 - val_acc: 1.0000 - val_auc: 0.9352\n",
      "Epoch 78/100\n",
      "879/879 [==============================] - 0s 100us/step - loss: 0.1175 - acc: 0.9590 - auc: 0.9358 - val_loss: 0.0120 - val_acc: 1.0000 - val_auc: 0.9364\n",
      "Epoch 79/100\n",
      "879/879 [==============================] - 0s 98us/step - loss: 0.0956 - acc: 0.9670 - auc: 0.9371 - val_loss: 0.0113 - val_acc: 1.0000 - val_auc: 0.9377\n",
      "Epoch 80/100\n",
      "879/879 [==============================] - 0s 89us/step - loss: 0.0916 - acc: 0.9681 - auc: 0.9383 - val_loss: 0.0107 - val_acc: 1.0000 - val_auc: 0.9389\n",
      "Epoch 81/100\n",
      "879/879 [==============================] - 0s 97us/step - loss: 0.0921 - acc: 0.9750 - auc: 0.9396 - val_loss: 0.0102 - val_acc: 1.0000 - val_auc: 0.9401\n",
      "Epoch 82/100\n",
      "879/879 [==============================] - 0s 105us/step - loss: 0.0823 - acc: 0.9727 - auc: 0.9408 - val_loss: 0.0097 - val_acc: 1.0000 - val_auc: 0.9413\n",
      "Epoch 83/100\n",
      "879/879 [==============================] - 0s 120us/step - loss: 0.0909 - acc: 0.9727 - auc: 0.9419 - val_loss: 0.0092 - val_acc: 1.0000 - val_auc: 0.9425\n",
      "Epoch 84/100\n",
      "879/879 [==============================] - 0s 117us/step - loss: 0.1005 - acc: 0.9556 - auc: 0.9430 - val_loss: 0.0088 - val_acc: 1.0000 - val_auc: 0.9435\n",
      "Epoch 85/100\n",
      "879/879 [==============================] - 0s 114us/step - loss: 0.0764 - acc: 0.9738 - auc: 0.9441 - val_loss: 0.0083 - val_acc: 1.0000 - val_auc: 0.9446\n",
      "Epoch 86/100\n",
      "879/879 [==============================] - 0s 135us/step - loss: 0.0915 - acc: 0.9670 - auc: 0.9451 - val_loss: 0.0082 - val_acc: 1.0000 - val_auc: 0.9456\n",
      "Epoch 87/100\n",
      "879/879 [==============================] - 0s 115us/step - loss: 0.0728 - acc: 0.9772 - auc: 0.9461 - val_loss: 0.0081 - val_acc: 1.0000 - val_auc: 0.9466\n",
      "Epoch 88/100\n",
      "879/879 [==============================] - 0s 100us/step - loss: 0.0844 - acc: 0.9727 - auc: 0.9471 - val_loss: 0.0078 - val_acc: 1.0000 - val_auc: 0.9476\n",
      "Epoch 89/100\n",
      "879/879 [==============================] - 0s 105us/step - loss: 0.0833 - acc: 0.9750 - auc: 0.9481 - val_loss: 0.0076 - val_acc: 1.0000 - val_auc: 0.9486\n",
      "Epoch 90/100\n",
      "879/879 [==============================] - 0s 122us/step - loss: 0.0834 - acc: 0.9681 - auc: 0.9490 - val_loss: 0.0074 - val_acc: 1.0000 - val_auc: 0.9495\n",
      "Epoch 91/100\n",
      "879/879 [==============================] - 0s 123us/step - loss: 0.0882 - acc: 0.9681 - auc: 0.9499 - val_loss: 0.0072 - val_acc: 1.0000 - val_auc: 0.9503\n",
      "Epoch 92/100\n",
      "879/879 [==============================] - 0s 118us/step - loss: 0.0888 - acc: 0.9693 - auc: 0.9507 - val_loss: 0.0069 - val_acc: 1.0000 - val_auc: 0.9512\n",
      "Epoch 93/100\n",
      "879/879 [==============================] - 0s 102us/step - loss: 0.0818 - acc: 0.9750 - auc: 0.9516 - val_loss: 0.0065 - val_acc: 1.0000 - val_auc: 0.9520\n",
      "Epoch 94/100\n",
      "879/879 [==============================] - 0s 99us/step - loss: 0.0587 - acc: 0.9818 - auc: 0.9525 - val_loss: 0.0063 - val_acc: 1.0000 - val_auc: 0.9529\n",
      "Epoch 95/100\n",
      "879/879 [==============================] - 0s 96us/step - loss: 0.0751 - acc: 0.9716 - auc: 0.9533 - val_loss: 0.0062 - val_acc: 1.0000 - val_auc: 0.9537\n",
      "Epoch 96/100\n",
      "879/879 [==============================] - 0s 120us/step - loss: 0.0884 - acc: 0.9704 - auc: 0.9541 - val_loss: 0.0059 - val_acc: 1.0000 - val_auc: 0.9544\n",
      "Epoch 97/100\n",
      "879/879 [==============================] - 0s 121us/step - loss: 0.0677 - acc: 0.9784 - auc: 0.9548 - val_loss: 0.0057 - val_acc: 1.0000 - val_auc: 0.9552\n",
      "Epoch 98/100\n",
      "879/879 [==============================] - 0s 119us/step - loss: 0.0706 - acc: 0.9795 - auc: 0.9556 - val_loss: 0.0057 - val_acc: 1.0000 - val_auc: 0.9559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100\n",
      "879/879 [==============================] - 0s 107us/step - loss: 0.0773 - acc: 0.9761 - auc: 0.9563 - val_loss: 0.0055 - val_acc: 1.0000 - val_auc: 0.9566\n",
      "Epoch 100/100\n",
      "879/879 [==============================] - 0s 104us/step - loss: 0.0743 - acc: 0.9761 - auc: 0.9570 - val_loss: 0.0055 - val_acc: 1.0000 - val_auc: 0.9573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c2aff98>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(df_test_x.shape[1])\n",
    "model.fit(df_train_x, df_train_y,validation_split=0.2,\n",
    "          batch_size=32, epochs=100, verbose=1,\n",
    "         class_weight={0:1,1:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 [==============================] - 0s 55us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.011901789281706689, 0.9957627118644068, 0.9575908547740871]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=df_test_x,y=df_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Exercise 6:\n",
    "(OPTIONAL)\n",
    "\n",
    "Predict the alcohol consumption of the person?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tmp = get_training_data(data_root)\n",
    "df[\"Alcohol\"] = df_tmp[\"Alcohol\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>D1Sugar</th>\n",
       "      <th>D1Carb</th>\n",
       "      <th>D2Sugar</th>\n",
       "      <th>D2Carb</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>SleepHours</th>\n",
       "      <th>Sleep_less</th>\n",
       "      <th>Diabetes_1.0</th>\n",
       "      <th>Diabetes_2.0</th>\n",
       "      <th>Race_1.0</th>\n",
       "      <th>Race_2.0</th>\n",
       "      <th>Race_3.0</th>\n",
       "      <th>Race_4.0</th>\n",
       "      <th>Race_6.0</th>\n",
       "      <th>Race_7.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.322153</td>\n",
       "      <td>-0.315899</td>\n",
       "      <td>-1.538426</td>\n",
       "      <td>-0.110637</td>\n",
       "      <td>-0.248231</td>\n",
       "      <td>0.433807</td>\n",
       "      <td>0.390910</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.813810</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.378850</td>\n",
       "      <td>-0.302716</td>\n",
       "      <td>0.331616</td>\n",
       "      <td>-0.308236</td>\n",
       "      <td>-0.604989</td>\n",
       "      <td>-0.050182</td>\n",
       "      <td>-0.368307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.536940</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.698484</td>\n",
       "      <td>0.096874</td>\n",
       "      <td>-0.491202</td>\n",
       "      <td>0.220393</td>\n",
       "      <td>0.336995</td>\n",
       "      <td>-0.614385</td>\n",
       "      <td>-0.481870</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.536940</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.285919</td>\n",
       "      <td>-0.405910</td>\n",
       "      <td>-0.491202</td>\n",
       "      <td>-0.358812</td>\n",
       "      <td>-0.223206</td>\n",
       "      <td>-0.248151</td>\n",
       "      <td>-0.266317</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.813810</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.074814</td>\n",
       "      <td>-0.304535</td>\n",
       "      <td>0.356550</td>\n",
       "      <td>0.660546</td>\n",
       "      <td>2.150216</td>\n",
       "      <td>-0.120293</td>\n",
       "      <td>0.504474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090679</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age   Insulin  Cholesterol   D1Sugar    D1Carb   D2Sugar    D2Carb  \\\n",
       "0  1.322153 -0.315899    -1.538426 -0.110637 -0.248231  0.433807  0.390910   \n",
       "1  1.378850 -0.302716     0.331616 -0.308236 -0.604989 -0.050182 -0.368307   \n",
       "2  0.698484  0.096874    -0.491202  0.220393  0.336995 -0.614385 -0.481870   \n",
       "3 -1.285919 -0.405910    -0.491202 -0.358812 -0.223206 -0.248151 -0.266317   \n",
       "6  0.074814 -0.304535     0.356550  0.660546  2.150216 -0.120293  0.504474   \n",
       "\n",
       "   Alcohol  SleepHours  Sleep_less  Diabetes_1.0  Diabetes_2.0  Race_1.0  \\\n",
       "0      0.0    0.813810           0             1             0         0   \n",
       "1      0.0    1.536940           0             0             1         0   \n",
       "2      2.0    1.536940           0             0             1         0   \n",
       "3      2.0    0.813810           0             0             1         0   \n",
       "6      0.0    0.090679           0             0             1         0   \n",
       "\n",
       "   Race_2.0  Race_3.0  Race_4.0  Race_6.0  Race_7.0  \n",
       "0         0         1         0         0         0  \n",
       "1         0         1         0         0         0  \n",
       "2         0         1         0         0         0  \n",
       "3         0         1         0         0         0  \n",
       "6         0         0         0         1         0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_train,df_test = train_test_split(df,train_size=0.7)\n",
    "df_train_x = df_train.drop(\"Alcohol\",axis=1)\n",
    "df_train_y = df_train[\"Alcohol\"]\n",
    "\n",
    "df_test_x = df_test.drop(\"Alcohol\",axis=1)\n",
    "df_test_y = df_test[\"Alcohol\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=32,activation='relu',input_dim=input_dim))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=20,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=10,activation='relu'))\n",
    "\n",
    "    # Build your final \"readout\" layer\n",
    "    # 1 line\n",
    "    model.add(Dense(units=1,activation=\"relu\"))\n",
    "    ######################\n",
    "    # The loss function and optimizer are provided for you.\n",
    "    sgd = SGD(lr=0.001, decay=1e-7, momentum=0.9)  #Stochastic gradient descent\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=sgd,metrics=[\"mae\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 219 samples, validate on 880 samples\n",
      "Epoch 1/50\n",
      "219/219 [==============================] - 1s 4ms/step - loss: 12.9128 - mean_absolute_error: 2.0677 - val_loss: 213.2904 - val_mean_absolute_error: 3.1037\n",
      "Epoch 2/50\n",
      "219/219 [==============================] - 0s 157us/step - loss: 11.1072 - mean_absolute_error: 2.0482 - val_loss: 210.4224 - val_mean_absolute_error: 3.2251\n",
      "Epoch 3/50\n",
      "219/219 [==============================] - 0s 142us/step - loss: 10.3664 - mean_absolute_error: 2.0007 - val_loss: 210.4285 - val_mean_absolute_error: 3.0166\n",
      "Epoch 4/50\n",
      "219/219 [==============================] - 0s 180us/step - loss: 9.7998 - mean_absolute_error: 1.7918 - val_loss: 210.7042 - val_mean_absolute_error: 2.9688\n",
      "Epoch 5/50\n",
      "219/219 [==============================] - 0s 218us/step - loss: 9.6761 - mean_absolute_error: 1.8062 - val_loss: 210.0288 - val_mean_absolute_error: 3.0065\n",
      "Epoch 6/50\n",
      "219/219 [==============================] - 0s 170us/step - loss: 9.4742 - mean_absolute_error: 1.8518 - val_loss: 209.9497 - val_mean_absolute_error: 3.0168\n",
      "Epoch 7/50\n",
      "219/219 [==============================] - 0s 159us/step - loss: 9.3743 - mean_absolute_error: 1.8580 - val_loss: 210.1118 - val_mean_absolute_error: 3.0084\n",
      "Epoch 8/50\n",
      "219/219 [==============================] - 0s 189us/step - loss: 9.2629 - mean_absolute_error: 1.8330 - val_loss: 210.1810 - val_mean_absolute_error: 3.0104\n",
      "Epoch 9/50\n",
      "219/219 [==============================] - 0s 204us/step - loss: 9.1522 - mean_absolute_error: 1.8294 - val_loss: 210.1379 - val_mean_absolute_error: 3.0254\n",
      "Epoch 10/50\n",
      "219/219 [==============================] - 0s 222us/step - loss: 9.0435 - mean_absolute_error: 1.8424 - val_loss: 210.2744 - val_mean_absolute_error: 3.0215\n",
      "Epoch 11/50\n",
      "219/219 [==============================] - 0s 144us/step - loss: 8.9421 - mean_absolute_error: 1.8285 - val_loss: 210.1030 - val_mean_absolute_error: 3.0438\n",
      "Epoch 12/50\n",
      "219/219 [==============================] - 0s 161us/step - loss: 8.9082 - mean_absolute_error: 1.8326 - val_loss: 210.4550 - val_mean_absolute_error: 3.0148\n",
      "Epoch 13/50\n",
      "219/219 [==============================] - 0s 150us/step - loss: 8.6890 - mean_absolute_error: 1.8121 - val_loss: 210.2140 - val_mean_absolute_error: 3.0459\n",
      "Epoch 14/50\n",
      "219/219 [==============================] - 0s 172us/step - loss: 8.7026 - mean_absolute_error: 1.8727 - val_loss: 210.1319 - val_mean_absolute_error: 3.0802\n",
      "Epoch 15/50\n",
      "219/219 [==============================] - 0s 172us/step - loss: 8.6421 - mean_absolute_error: 1.8969 - val_loss: 210.6029 - val_mean_absolute_error: 3.0509\n",
      "Epoch 16/50\n",
      "219/219 [==============================] - 0s 163us/step - loss: 8.4421 - mean_absolute_error: 1.7724 - val_loss: 210.9842 - val_mean_absolute_error: 3.0177\n",
      "Epoch 17/50\n",
      "219/219 [==============================] - 0s 176us/step - loss: 8.3747 - mean_absolute_error: 1.7400 - val_loss: 210.5095 - val_mean_absolute_error: 3.0347\n",
      "Epoch 18/50\n",
      "219/219 [==============================] - 0s 156us/step - loss: 8.3043 - mean_absolute_error: 1.8125 - val_loss: 210.2949 - val_mean_absolute_error: 3.0665\n",
      "Epoch 19/50\n",
      "219/219 [==============================] - 0s 163us/step - loss: 8.1564 - mean_absolute_error: 1.7907 - val_loss: 210.6106 - val_mean_absolute_error: 3.0462\n",
      "Epoch 20/50\n",
      "219/219 [==============================] - 0s 145us/step - loss: 8.1622 - mean_absolute_error: 1.7274 - val_loss: 210.8963 - val_mean_absolute_error: 3.0206\n",
      "Epoch 21/50\n",
      "219/219 [==============================] - 0s 184us/step - loss: 7.9874 - mean_absolute_error: 1.7720 - val_loss: 210.3193 - val_mean_absolute_error: 3.0877\n",
      "Epoch 22/50\n",
      "219/219 [==============================] - 0s 189us/step - loss: 7.8727 - mean_absolute_error: 1.8014 - val_loss: 210.8121 - val_mean_absolute_error: 3.0677\n",
      "Epoch 23/50\n",
      "219/219 [==============================] - 0s 175us/step - loss: 7.9584 - mean_absolute_error: 1.8083 - val_loss: 211.1640 - val_mean_absolute_error: 3.0607\n",
      "Epoch 24/50\n",
      "219/219 [==============================] - 0s 194us/step - loss: 7.7778 - mean_absolute_error: 1.6918 - val_loss: 211.4332 - val_mean_absolute_error: 3.0298\n",
      "Epoch 25/50\n",
      "219/219 [==============================] - 0s 174us/step - loss: 7.5175 - mean_absolute_error: 1.6898 - val_loss: 210.4218 - val_mean_absolute_error: 3.1033\n",
      "Epoch 26/50\n",
      "219/219 [==============================] - 0s 186us/step - loss: 7.4689 - mean_absolute_error: 1.7654 - val_loss: 210.8551 - val_mean_absolute_error: 3.0840\n",
      "Epoch 27/50\n",
      "219/219 [==============================] - 0s 176us/step - loss: 7.2666 - mean_absolute_error: 1.7032 - val_loss: 211.0965 - val_mean_absolute_error: 3.0751\n",
      "Epoch 28/50\n",
      "219/219 [==============================] - 0s 180us/step - loss: 7.1975 - mean_absolute_error: 1.7059 - val_loss: 210.9681 - val_mean_absolute_error: 3.1085\n",
      "Epoch 29/50\n",
      "219/219 [==============================] - 0s 140us/step - loss: 7.0708 - mean_absolute_error: 1.7286 - val_loss: 211.5043 - val_mean_absolute_error: 3.0689\n",
      "Epoch 30/50\n",
      "219/219 [==============================] - 0s 150us/step - loss: 6.8947 - mean_absolute_error: 1.6624 - val_loss: 211.3927 - val_mean_absolute_error: 3.0730\n",
      "Epoch 31/50\n",
      "219/219 [==============================] - 0s 149us/step - loss: 6.8290 - mean_absolute_error: 1.6934 - val_loss: 211.4039 - val_mean_absolute_error: 3.1047\n",
      "Epoch 32/50\n",
      "219/219 [==============================] - 0s 149us/step - loss: 6.9077 - mean_absolute_error: 1.6262 - val_loss: 211.4803 - val_mean_absolute_error: 3.0543\n",
      "Epoch 33/50\n",
      "219/219 [==============================] - 0s 153us/step - loss: 6.5824 - mean_absolute_error: 1.6920 - val_loss: 210.9455 - val_mean_absolute_error: 3.1409\n",
      "Epoch 34/50\n",
      "219/219 [==============================] - 0s 146us/step - loss: 6.6222 - mean_absolute_error: 1.6142 - val_loss: 211.8569 - val_mean_absolute_error: 3.0481\n",
      "Epoch 35/50\n",
      "219/219 [==============================] - 0s 146us/step - loss: 5.9540 - mean_absolute_error: 1.5624 - val_loss: 211.0739 - val_mean_absolute_error: 3.1759\n",
      "Epoch 36/50\n",
      "219/219 [==============================] - 0s 155us/step - loss: 6.2075 - mean_absolute_error: 1.7076 - val_loss: 212.4779 - val_mean_absolute_error: 3.0840\n",
      "Epoch 37/50\n",
      "219/219 [==============================] - 0s 126us/step - loss: 6.0782 - mean_absolute_error: 1.5372 - val_loss: 211.6488 - val_mean_absolute_error: 3.1024\n",
      "Epoch 38/50\n",
      "219/219 [==============================] - 0s 142us/step - loss: 5.8440 - mean_absolute_error: 1.6700 - val_loss: 211.7283 - val_mean_absolute_error: 3.1570\n",
      "Epoch 39/50\n",
      "219/219 [==============================] - 0s 159us/step - loss: 5.1351 - mean_absolute_error: 1.4812 - val_loss: 212.6444 - val_mean_absolute_error: 3.1141\n",
      "Epoch 40/50\n",
      "219/219 [==============================] - 0s 156us/step - loss: 5.0487 - mean_absolute_error: 1.4465 - val_loss: 211.8501 - val_mean_absolute_error: 3.1596\n",
      "Epoch 41/50\n",
      "219/219 [==============================] - 0s 160us/step - loss: 5.1540 - mean_absolute_error: 1.5475 - val_loss: 213.0232 - val_mean_absolute_error: 3.1999\n",
      "Epoch 42/50\n",
      "219/219 [==============================] - 0s 153us/step - loss: 5.1462 - mean_absolute_error: 1.4749 - val_loss: 212.9550 - val_mean_absolute_error: 3.1293\n",
      "Epoch 43/50\n",
      "219/219 [==============================] - 0s 142us/step - loss: 4.4962 - mean_absolute_error: 1.4305 - val_loss: 212.0426 - val_mean_absolute_error: 3.2118\n",
      "Epoch 44/50\n",
      "219/219 [==============================] - 0s 178us/step - loss: 4.2508 - mean_absolute_error: 1.4682 - val_loss: 213.6148 - val_mean_absolute_error: 3.2236\n",
      "Epoch 45/50\n",
      "219/219 [==============================] - 0s 271us/step - loss: 3.8643 - mean_absolute_error: 1.4426 - val_loss: 213.6253 - val_mean_absolute_error: 3.1895\n",
      "Epoch 46/50\n",
      "219/219 [==============================] - 0s 309us/step - loss: 3.8180 - mean_absolute_error: 1.3162 - val_loss: 212.9645 - val_mean_absolute_error: 3.2745\n",
      "Epoch 47/50\n",
      "219/219 [==============================] - 0s 286us/step - loss: 3.1548 - mean_absolute_error: 1.3012 - val_loss: 214.2730 - val_mean_absolute_error: 3.2319\n",
      "Epoch 48/50\n",
      "219/219 [==============================] - 0s 260us/step - loss: 2.7997 - mean_absolute_error: 1.2177 - val_loss: 214.1556 - val_mean_absolute_error: 3.3091\n",
      "Epoch 49/50\n",
      "219/219 [==============================] - 0s 171us/step - loss: 2.6159 - mean_absolute_error: 1.1851 - val_loss: 214.4596 - val_mean_absolute_error: 3.2557\n",
      "Epoch 50/50\n",
      "219/219 [==============================] - 0s 134us/step - loss: 2.3031 - mean_absolute_error: 1.1285 - val_loss: 214.4961 - val_mean_absolute_error: 3.3595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11eef87f0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(df_train_x.shape[1])\n",
    "model.fit(df_train_x,df_train_y,validation_split=0.8,batch_size=32,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 [==============================] - 0s 23us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[377.7863880416094, 4.009580402050988]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(df_test_x,df_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
