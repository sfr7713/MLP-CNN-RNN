{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "\n",
    "Predicting diabetes using the NHANES dataset\n",
    "\n",
    "About our dataset\n",
    "The National Health and Nutrition Examination Survey (NHANES), administered annually by the National Center for Health Statistics, is designed to assess the general health and nutritional status of adults and children in the United States.\n",
    "\n",
    "\n",
    "Data:\n",
    "\n",
    "https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=2013\n",
    "\n",
    "\n",
    "Goals:\n",
    "\n",
    "- refresh general machine learning principles (train/dev/test)\n",
    "- refresh neural network implementation\n",
    "- handle an imbalanced dataset\n",
    "\n",
    "Keras:\n",
    "\n",
    "The Python Deep Learning Library Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook supports automated reloading of packages. So once you import a file as a module, any saved changes you make to that file will be automatically changed in this notebook. For example, go to exercise_1.py and toggle the comment the second \"print\" statement in helloworld(), rerunning the next cell several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from exercise_1 import helloworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "helloworld()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing the dataset\n",
    "We provide a helper function for you to read the SAS files into a pandas dataframe. \n",
    "\n",
    "To load it, you will need to install xport (a SAS interface to Python) \n",
    "\n",
    "```pip install xport```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import merge_xpt, get_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFY THIS CELL BY POINTING IT TO WHERE YOU EXTRACTED YOUR DATA ###\n",
    "data_root =\"/Users/furon/Desktop/Project 1/Data/nhanes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/anbangwu/Downloads/Project1/Data/nhanes\\\\2013-2014\\\\questionnaire\\\\DIQ_H.XPT'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7b22b3e92371>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_training_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\Project 1\\utils.py\u001b[0m in \u001b[0;36mget_training_data\u001b[1;34m(data_root)\u001b[0m\n\u001b[0;32m     39\u001b[0m     ]\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_xpt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     df = merged[[\n\u001b[0;32m     43\u001b[0m         \u001b[1;34m'DIQ010'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RIDAGEYR'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'LBDINSI'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'RIDRETH3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Project 1\\utils.py\u001b[0m in \u001b[0;36mmerge_xpt\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m                 \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mall_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# store all datasets in a np.array\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/anbangwu/Downloads/Project1/Data/nhanes\\\\2013-2014\\\\questionnaire\\\\DIQ_H.XPT'"
     ]
    }
   ],
   "source": [
    "df = get_training_data(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two commands above are useful for quickly getting a feel for the dataset. We definitely want to know the shape of the dataframe so we know how many features we're dealing with, and we can see the number of missing values in each column, as well as a few descriptive statistics.\n",
    "\n",
    "# Exercise 1: \n",
    "\n",
    "The Diabetes column is coded as 1.0: yes and 2.0: no, and for some reason there are a few rows with a 3.0. We don't know what 3.0 means, so we will drop it. Also, we will remove all of the samples that have a NaN (for easy training later- also it seems like we'll still have enough data). \n",
    "\n",
    "As most of you are familiar with SQL, these references may be helpful:\n",
    "https://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html\n",
    "\n",
    "Use it to complete clean_data_and_labels() in exercise_1.py(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_1 import clean_data_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_data_and_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check your code. \n",
    "# If you see no output, you have completed the exercise correctly.\n",
    "assert np.all(df.Diabetes < 3), \"Not all labels are < 3.0\"\n",
    "assert np.all(df.count() == len(df)), \"There are still NaNs in your data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2:\n",
    "\n",
    "With the clean dataset we are ready to build and train the model.\n",
    "\n",
    "Please build your neural network by completing build_model() in exercise_1.py. The architecture choice is up to you, but please only use fully connected (Dense) layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_1 import build_model, split_x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if the model works. If it runs your model is functioning properly\n",
    "x_sm, y_sm = split_x_y(df[0:10])  # load the first 10 samples\n",
    "model.fit(x_sm, y_sm, batch_size=1, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3:\n",
    "Model tuning\n",
    "\n",
    "Optimize your neural network by modifying the cells below. Here are a few hints:\n",
    "\n",
    "- You are not provided with a validation set, so it may be helpful to make your own. \n",
    "- Check the distribution of the labels using df.Diabetes.hist(). What should you do about this? (make any changes to preprocess_dataset() in exercise_1.py- OPTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/Users/anbangwu/Downloads/Project1/Data/nhanes\"\n",
    "df = get_training_data(data_root)\n",
    "df = clean_data_and_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from exercise_1 import preprocess_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Diabetes\", axis=1)\n",
    "y = df[\"Diabetes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 10.,\n",
    "                1: 1.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y = pd.get_dummies(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x= X_scaled, y = y.values, epochs = 200, verbose = 0, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are happy with your model training, run the below cell to generate the final labels for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_testing_data\n",
    "test_data, _ = get_testing_data(data_root)\n",
    "test_data = pd.DataFrame(test_data)\n",
    "test_data\n",
    "test_data = scaler.transform(test_data)\n",
    "predicted = model.predict_classes(test_data)\n",
    "with open(\"exercise_1_output.txt\", \"w\") as f:\n",
    "    [f.write(\"{}\\n\".format(p+1)) for p in predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(predicted==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Exercise 4:\n",
    "(OPTIONAL)\n",
    "\n",
    "Do you find any relationship between Age and whether a person has Diabetes or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Exercise 5:\n",
    "(OPTIONAL)\n",
    "\n",
    "Predict whether the person sleeps for less than mean sleeping hours across the dataset or more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Exercise 6:\n",
    "(OPTIONAL)\n",
    "\n",
    "Predict the alcohol consumption of the person?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
