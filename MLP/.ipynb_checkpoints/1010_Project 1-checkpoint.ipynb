{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "\n",
    "Predicting diabetes using the NHANES dataset\n",
    "\n",
    "About our dataset\n",
    "The National Health and Nutrition Examination Survey (NHANES), administered annually by the National Center for Health Statistics, is designed to assess the general health and nutritional status of adults and children in the United States.\n",
    "\n",
    "\n",
    "Data:\n",
    "\n",
    "https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=2013\n",
    "\n",
    "\n",
    "Goals:\n",
    "\n",
    "- refresh general machine learning principles (train/dev/test)\n",
    "- refresh neural network implementation\n",
    "- handle an imbalanced dataset\n",
    "\n",
    "Keras:\n",
    "\n",
    "The Python Deep Learning Library Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection as ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook supports automated reloading of packages. So once you import a file as a module, any saved changes you make to that file will be automatically changed in this notebook. For example, go to exercise_1.py and toggle the comment the second \"print\" statement in helloworld(), rerunning the next cell several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from exercise_1 import helloworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "helloworld()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing the dataset\n",
    "We provide a helper function for you to read the SAS files into a pandas dataframe. \n",
    "\n",
    "To load it, you will need to install xport (a SAS interface to Python) \n",
    "\n",
    "```pip install xport```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import merge_xpt, get_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFY THIS CELL BY POINTING IT TO WHERE YOU EXTRACTED YOUR DATA ###\n",
    "data_root = \"/Users/furon/Desktop/Project 1/Data/nhanes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_training_data(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2507, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Age</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>Race</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>D1Sugar</th>\n",
       "      <th>D1Carb</th>\n",
       "      <th>D2Sugar</th>\n",
       "      <th>D2Carb</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>SleepHours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2507.000000</td>\n",
       "      <td>2507.000000</td>\n",
       "      <td>2444.000000</td>\n",
       "      <td>2507.000000</td>\n",
       "      <td>2483.000000</td>\n",
       "      <td>2318.000000</td>\n",
       "      <td>2318.000000</td>\n",
       "      <td>2067.000000</td>\n",
       "      <td>2067.000000</td>\n",
       "      <td>1930.000000</td>\n",
       "      <td>2506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.919426</td>\n",
       "      <td>48.035899</td>\n",
       "      <td>75.344239</td>\n",
       "      <td>3.286797</td>\n",
       "      <td>187.024970</td>\n",
       "      <td>108.484292</td>\n",
       "      <td>251.030453</td>\n",
       "      <td>101.030982</td>\n",
       "      <td>237.012772</td>\n",
       "      <td>3.496891</td>\n",
       "      <td>6.922985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.364925</td>\n",
       "      <td>18.252254</td>\n",
       "      <td>113.087851</td>\n",
       "      <td>1.490164</td>\n",
       "      <td>41.496019</td>\n",
       "      <td>75.073624</td>\n",
       "      <td>126.535489</td>\n",
       "      <td>65.456716</td>\n",
       "      <td>116.708696</td>\n",
       "      <td>13.739268</td>\n",
       "      <td>1.403750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>8.670000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>34.305000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>57.390000</td>\n",
       "      <td>164.557500</td>\n",
       "      <td>54.570000</td>\n",
       "      <td>158.240000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>53.460000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>93.450000</td>\n",
       "      <td>231.080000</td>\n",
       "      <td>88.420000</td>\n",
       "      <td>220.880000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>86.280000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>141.022500</td>\n",
       "      <td>311.895000</td>\n",
       "      <td>132.055000</td>\n",
       "      <td>296.170000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4094.880000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>612.000000</td>\n",
       "      <td>719.580000</td>\n",
       "      <td>1300.720000</td>\n",
       "      <td>457.410000</td>\n",
       "      <td>1041.560000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Diabetes          Age      Insulin         Race  Cholesterol  \\\n",
       "count  2507.000000  2507.000000  2444.000000  2507.000000  2483.000000   \n",
       "mean      1.919426    48.035899    75.344239     3.286797   187.024970   \n",
       "std       0.364925    18.252254   113.087851     1.490164    41.496019   \n",
       "min       1.000000    18.000000     0.840000     1.000000    69.000000   \n",
       "25%       2.000000    33.000000    34.305000     3.000000   159.000000   \n",
       "50%       2.000000    48.000000    53.460000     3.000000   183.000000   \n",
       "75%       2.000000    63.000000    86.280000     4.000000   211.000000   \n",
       "max       3.000000    80.000000  4094.880000     7.000000   612.000000   \n",
       "\n",
       "           D1Sugar       D1Carb      D2Sugar       D2Carb      Alcohol  \\\n",
       "count  2318.000000  2318.000000  2067.000000  2067.000000  1930.000000   \n",
       "mean    108.484292   251.030453   101.030982   237.012772     3.496891   \n",
       "std      75.073624   126.535489    65.456716   116.708696    13.739268   \n",
       "min       0.130000     8.670000     0.000000     0.000000     0.000000   \n",
       "25%      57.390000   164.557500    54.570000   158.240000     1.000000   \n",
       "50%      93.450000   231.080000    88.420000   220.880000     2.000000   \n",
       "75%     141.022500   311.895000   132.055000   296.170000     4.000000   \n",
       "max     719.580000  1300.720000   457.410000  1041.560000   365.000000   \n",
       "\n",
       "        SleepHours  \n",
       "count  2506.000000  \n",
       "mean      6.922985  \n",
       "std       1.403750  \n",
       "min       2.000000  \n",
       "25%       6.000000  \n",
       "50%       7.000000  \n",
       "75%       8.000000  \n",
       "max      12.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    2157\n",
       "1.0     276\n",
       "3.0      74\n",
       "Name: Diabetes, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Diabetes\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two commands above are useful for quickly getting a feel for the dataset. We definitely want to know the shape of the dataframe so we know how many features we're dealing with, and we can see the number of missing values in each column, as well as a few descriptive statistics.\n",
    "\n",
    "# Exercise 1: \n",
    "\n",
    "The Diabetes column is coded as 1.0: yes and 2.0: no, and for some reason there are a few rows with a 3.0. We don't know what 3.0 means, so we will drop it. Also, we will remove all of the samples that have a NaN (for easy training later- also it seems like we'll still have enough data). \n",
    "\n",
    "As most of you are familiar with SQL, these references may be helpful:\n",
    "https://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html\n",
    "\n",
    "Use it to complete clean_data_and_labels() in exercise_1.py(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_1 import clean_data_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_data_and_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check your code. \n",
    "# If you see no output, you have completed the exercise correctly.\n",
    "assert np.all(df.Diabetes < 3), \"Not all labels are < 3.0\"\n",
    "assert np.all(df.count() == len(df)), \"There are still NaNs in your data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2:\n",
    "\n",
    "With the clean dataset we are ready to build and train the model.\n",
    "\n",
    "Please build your neural network by completing build_model() in exercise_1.py. The architecture choice is up to you, but please only use fully connected (Dense) layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_1 import build_model, split_x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(df.shape[1]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.6874 - acc: 0.8000 - auc: 0.6289   \n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6476 - acc: 0.9000 - auc: 0.7707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1786a51c780>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if the model works. If it runs your model is functioning properly\n",
    "x_sm, y_sm = split_x_y(df[0:10])  # load the first 10 samples\n",
    "model.fit(x_sm, y_sm, batch_size=1, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3:\n",
    "Model tuning\n",
    "\n",
    "Optimize your neural network by modifying the cells below. Here are a few hints:\n",
    "\n",
    "- You are not provided with a validation set, so it may be helpful to make your own. \n",
    "- Check the distribution of the labels using df.Diabetes.hist(). What should you do about this? (make any changes to preprocess_dataset() in exercise_1.py- OPTIONAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from exercise_1 import preprocess_dataset\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.shape[1] == 11, \\\n",
    "\"Number of features is off, make sure you didn't remove or add any\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>Age</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>D1Sugar</th>\n",
       "      <th>D1Carb</th>\n",
       "      <th>D2Sugar</th>\n",
       "      <th>D2Carb</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>SleepHours</th>\n",
       "      <th>Race_1.0</th>\n",
       "      <th>Race_2.0</th>\n",
       "      <th>Race_3.0</th>\n",
       "      <th>Race_4.0</th>\n",
       "      <th>Race_6.0</th>\n",
       "      <th>Race_7.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1571.000000</td>\n",
       "      <td>1.571000e+03</td>\n",
       "      <td>1.571000e+03</td>\n",
       "      <td>1.571000e+03</td>\n",
       "      <td>1.571000e+03</td>\n",
       "      <td>1.571000e+03</td>\n",
       "      <td>1.571000e+03</td>\n",
       "      <td>1.571000e+03</td>\n",
       "      <td>1.571000e+03</td>\n",
       "      <td>1.571000e+03</td>\n",
       "      <td>1571.000000</td>\n",
       "      <td>1571.000000</td>\n",
       "      <td>1571.000000</td>\n",
       "      <td>1571.000000</td>\n",
       "      <td>1571.000000</td>\n",
       "      <td>1571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.886696</td>\n",
       "      <td>3.957510e-17</td>\n",
       "      <td>3.180142e-19</td>\n",
       "      <td>2.953027e-16</td>\n",
       "      <td>-5.144764e-17</td>\n",
       "      <td>-1.639540e-16</td>\n",
       "      <td>9.893776e-19</td>\n",
       "      <td>-3.136327e-16</td>\n",
       "      <td>9.178244e-17</td>\n",
       "      <td>3.083943e-16</td>\n",
       "      <td>0.129217</td>\n",
       "      <td>0.090388</td>\n",
       "      <td>0.478676</td>\n",
       "      <td>0.185869</td>\n",
       "      <td>0.092934</td>\n",
       "      <td>0.022915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.317065</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>1.000318e+00</td>\n",
       "      <td>0.335547</td>\n",
       "      <td>0.286829</td>\n",
       "      <td>0.499704</td>\n",
       "      <td>0.389125</td>\n",
       "      <td>0.290433</td>\n",
       "      <td>0.149681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.739497e+00</td>\n",
       "      <td>-5.745647e-01</td>\n",
       "      <td>-2.635517e+00</td>\n",
       "      <td>-1.450440e+00</td>\n",
       "      <td>-1.914392e+00</td>\n",
       "      <td>-1.546781e+00</td>\n",
       "      <td>-2.011794e+00</td>\n",
       "      <td>-2.410468e-01</td>\n",
       "      <td>-3.524974e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>-8.323412e-01</td>\n",
       "      <td>-3.234003e-01</td>\n",
       "      <td>-6.906737e-01</td>\n",
       "      <td>-6.755969e-01</td>\n",
       "      <td>-6.733057e-01</td>\n",
       "      <td>-7.154043e-01</td>\n",
       "      <td>-6.813619e-01</td>\n",
       "      <td>-1.749010e-01</td>\n",
       "      <td>-6.324515e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.811713e-02</td>\n",
       "      <td>-1.781568e-01</td>\n",
       "      <td>-9.226017e-02</td>\n",
       "      <td>-2.055161e-01</td>\n",
       "      <td>-1.494444e-01</td>\n",
       "      <td>-1.953792e-01</td>\n",
       "      <td>-1.421857e-01</td>\n",
       "      <td>-1.087553e-01</td>\n",
       "      <td>9.067900e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.118782e-01</td>\n",
       "      <td>6.096074e-02</td>\n",
       "      <td>6.058889e-01</td>\n",
       "      <td>4.108687e-01</td>\n",
       "      <td>4.596095e-01</td>\n",
       "      <td>4.954747e-01</td>\n",
       "      <td>5.034671e-01</td>\n",
       "      <td>-9.536612e-03</td>\n",
       "      <td>8.138095e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.775731e+00</td>\n",
       "      <td>3.044435e+01</td>\n",
       "      <td>3.622890e+00</td>\n",
       "      <td>7.948563e+00</td>\n",
       "      <td>8.065041e+00</td>\n",
       "      <td>5.349847e+00</td>\n",
       "      <td>6.724060e+00</td>\n",
       "      <td>2.390216e+01</td>\n",
       "      <td>3.706332e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Diabetes           Age       Insulin   Cholesterol       D1Sugar  \\\n",
       "count  1571.000000  1.571000e+03  1.571000e+03  1.571000e+03  1.571000e+03   \n",
       "mean      1.886696  3.957510e-17  3.180142e-19  2.953027e-16 -5.144764e-17   \n",
       "std       0.317065  1.000318e+00  1.000318e+00  1.000318e+00  1.000318e+00   \n",
       "min       1.000000 -1.739497e+00 -5.745647e-01 -2.635517e+00 -1.450440e+00   \n",
       "25%       2.000000 -8.323412e-01 -3.234003e-01 -6.906737e-01 -6.755969e-01   \n",
       "50%       2.000000  1.811713e-02 -1.781568e-01 -9.226017e-02 -2.055161e-01   \n",
       "75%       2.000000  8.118782e-01  6.096074e-02  6.058889e-01  4.108687e-01   \n",
       "max       2.000000  1.775731e+00  3.044435e+01  3.622890e+00  7.948563e+00   \n",
       "\n",
       "             D1Carb       D2Sugar        D2Carb       Alcohol    SleepHours  \\\n",
       "count  1.571000e+03  1.571000e+03  1.571000e+03  1.571000e+03  1.571000e+03   \n",
       "mean  -1.639540e-16  9.893776e-19 -3.136327e-16  9.178244e-17  3.083943e-16   \n",
       "std    1.000318e+00  1.000318e+00  1.000318e+00  1.000318e+00  1.000318e+00   \n",
       "min   -1.914392e+00 -1.546781e+00 -2.011794e+00 -2.410468e-01 -3.524974e+00   \n",
       "25%   -6.733057e-01 -7.154043e-01 -6.813619e-01 -1.749010e-01 -6.324515e-01   \n",
       "50%   -1.494444e-01 -1.953792e-01 -1.421857e-01 -1.087553e-01  9.067900e-02   \n",
       "75%    4.596095e-01  4.954747e-01  5.034671e-01 -9.536612e-03  8.138095e-01   \n",
       "max    8.065041e+00  5.349847e+00  6.724060e+00  2.390216e+01  3.706332e+00   \n",
       "\n",
       "          Race_1.0     Race_2.0     Race_3.0     Race_4.0     Race_6.0  \\\n",
       "count  1571.000000  1571.000000  1571.000000  1571.000000  1571.000000   \n",
       "mean      0.129217     0.090388     0.478676     0.185869     0.092934   \n",
       "std       0.335547     0.286829     0.499704     0.389125     0.290433   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     1.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "          Race_7.0  \n",
       "count  1571.000000  \n",
       "mean      0.022915  \n",
       "std       0.149681  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       0.000000  \n",
       "max       1.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocess_dataset(df)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cells below to fit your model and as scratch (perhaps to view the dataset distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    1393\n",
       "1.0     178\n",
       "Name: Diabetes, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Diabetes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_x_y(df)  \n",
    "X_train, X_vali, y_train, y_vali = train_test_split(X, y, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1256 samples, validate on 315 samples\n",
      "Epoch 1/200\n",
      "1256/1256 [==============================] - 1s 878us/step - loss: 1.5830 - acc: 0.5318 - auc: 0.5193 - val_loss: 0.6612 - val_acc: 0.7270 - val_auc: 0.5307\n",
      "Epoch 2/200\n",
      "1256/1256 [==============================] - 0s 115us/step - loss: 1.3977 - acc: 0.5303 - auc: 0.5454 - val_loss: 0.6734 - val_acc: 0.7492 - val_auc: 0.5475\n",
      "Epoch 3/200\n",
      "1256/1256 [==============================] - 0s 118us/step - loss: 1.3024 - acc: 0.5127 - auc: 0.5519 - val_loss: 0.6844 - val_acc: 0.7460 - val_auc: 0.5464\n",
      "Epoch 4/200\n",
      "1256/1256 [==============================] - 0s 114us/step - loss: 1.2531 - acc: 0.5358 - auc: 0.5488 - val_loss: 0.6902 - val_acc: 0.6857 - val_auc: 0.5477\n",
      "Epoch 5/200\n",
      "1256/1256 [==============================] - 0s 114us/step - loss: 1.2330 - acc: 0.5342 - auc: 0.5479 - val_loss: 0.6791 - val_acc: 0.7429 - val_auc: 0.5513\n",
      "Epoch 6/200\n",
      "1256/1256 [==============================] - 0s 109us/step - loss: 1.2686 - acc: 0.5422 - auc: 0.5532 - val_loss: 0.6941 - val_acc: 0.7429 - val_auc: 0.5536\n",
      "Epoch 7/200\n",
      "1256/1256 [==============================] - 0s 121us/step - loss: 1.2642 - acc: 0.5518 - auc: 0.5553 - val_loss: 0.6845 - val_acc: 0.7905 - val_auc: 0.5570\n",
      "Epoch 8/200\n",
      "1256/1256 [==============================] - 0s 118us/step - loss: 1.2393 - acc: 0.5438 - auc: 0.5583 - val_loss: 0.6868 - val_acc: 0.7556 - val_auc: 0.5593\n",
      "Epoch 9/200\n",
      "1256/1256 [==============================] - 0s 121us/step - loss: 1.2640 - acc: 0.5191 - auc: 0.5588 - val_loss: 0.6878 - val_acc: 0.7238 - val_auc: 0.5581\n",
      "Epoch 10/200\n",
      "1256/1256 [==============================] - 0s 117us/step - loss: 1.2196 - acc: 0.5406 - auc: 0.5597 - val_loss: 0.6783 - val_acc: 0.7397 - val_auc: 0.5601\n",
      "Epoch 11/200\n",
      "1256/1256 [==============================] - 0s 112us/step - loss: 1.2611 - acc: 0.5589 - auc: 0.5613 - val_loss: 0.6677 - val_acc: 0.8190 - val_auc: 0.5625\n",
      "Epoch 12/200\n",
      "1256/1256 [==============================] - 0s 115us/step - loss: 1.2425 - acc: 0.5804 - auc: 0.5652 - val_loss: 0.6716 - val_acc: 0.7968 - val_auc: 0.5675\n",
      "Epoch 13/200\n",
      "1256/1256 [==============================] - 0s 118us/step - loss: 1.1993 - acc: 0.6099 - auc: 0.5709 - val_loss: 0.6565 - val_acc: 0.8222 - val_auc: 0.5746\n",
      "Epoch 14/200\n",
      "1256/1256 [==============================] - 0s 116us/step - loss: 1.2490 - acc: 0.5947 - auc: 0.5778 - val_loss: 0.6628 - val_acc: 0.7841 - val_auc: 0.5799\n",
      "Epoch 15/200\n",
      "1256/1256 [==============================] - 0s 112us/step - loss: 1.2147 - acc: 0.5900 - auc: 0.5827 - val_loss: 0.6831 - val_acc: 0.7111 - val_auc: 0.5832\n",
      "Epoch 16/200\n",
      "1256/1256 [==============================] - 0s 110us/step - loss: 1.2260 - acc: 0.5446 - auc: 0.5833 - val_loss: 0.6832 - val_acc: 0.6952 - val_auc: 0.5829\n",
      "Epoch 17/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.2003 - acc: 0.5836 - auc: 0.5843 - val_loss: 0.6590 - val_acc: 0.7524 - val_auc: 0.5856\n",
      "Epoch 18/200\n",
      "1256/1256 [==============================] - 0s 117us/step - loss: 1.2187 - acc: 0.5852 - auc: 0.5870 - val_loss: 0.6634 - val_acc: 0.7397 - val_auc: 0.5883\n",
      "Epoch 19/200\n",
      "1256/1256 [==============================] - 0s 110us/step - loss: 1.2118 - acc: 0.5916 - auc: 0.5897 - val_loss: 0.6491 - val_acc: 0.7810 - val_auc: 0.5913\n",
      "Epoch 20/200\n",
      "1256/1256 [==============================] - 0s 103us/step - loss: 1.2043 - acc: 0.6123 - auc: 0.5933 - val_loss: 0.6643 - val_acc: 0.7524 - val_auc: 0.5946\n",
      "Epoch 21/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.2314 - acc: 0.6067 - auc: 0.5957 - val_loss: 0.6557 - val_acc: 0.7683 - val_auc: 0.5972\n",
      "Epoch 22/200\n",
      "1256/1256 [==============================] - 0s 105us/step - loss: 1.1938 - acc: 0.5884 - auc: 0.5982 - val_loss: 0.6467 - val_acc: 0.7778 - val_auc: 0.5993\n",
      "Epoch 23/200\n",
      "1256/1256 [==============================] - 0s 119us/step - loss: 1.1870 - acc: 0.6322 - auc: 0.6012 - val_loss: 0.6485 - val_acc: 0.7587 - val_auc: 0.6028\n",
      "Epoch 24/200\n",
      "1256/1256 [==============================] - 0s 106us/step - loss: 1.1950 - acc: 0.6115 - auc: 0.6043 - val_loss: 0.6487 - val_acc: 0.7429 - val_auc: 0.6052\n",
      "Epoch 25/200\n",
      "1256/1256 [==============================] - 0s 103us/step - loss: 1.1872 - acc: 0.6314 - auc: 0.6069 - val_loss: 0.6305 - val_acc: 0.7714 - val_auc: 0.6085\n",
      "Epoch 26/200\n",
      "1256/1256 [==============================] - 0s 101us/step - loss: 1.1757 - acc: 0.6791 - auc: 0.6110 - val_loss: 0.6204 - val_acc: 0.8095 - val_auc: 0.6134\n",
      "Epoch 27/200\n",
      "1256/1256 [==============================] - 0s 104us/step - loss: 1.1438 - acc: 0.6744 - auc: 0.6158 - val_loss: 0.6138 - val_acc: 0.7905 - val_auc: 0.6179\n",
      "Epoch 28/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.1786 - acc: 0.6346 - auc: 0.6196 - val_loss: 0.6101 - val_acc: 0.7714 - val_auc: 0.6208\n",
      "Epoch 29/200\n",
      "1256/1256 [==============================] - 0s 114us/step - loss: 1.1756 - acc: 0.6361 - auc: 0.6225 - val_loss: 0.6078 - val_acc: 0.7778 - val_auc: 0.6236\n",
      "Epoch 30/200\n",
      "1256/1256 [==============================] - 0s 124us/step - loss: 1.1862 - acc: 0.6290 - auc: 0.6248 - val_loss: 0.6200 - val_acc: 0.7556 - val_auc: 0.6257\n",
      "Epoch 31/200\n",
      "1256/1256 [==============================] - 0s 121us/step - loss: 1.2249 - acc: 0.6489 - auc: 0.6272 - val_loss: 0.6055 - val_acc: 0.7937 - val_auc: 0.6284\n",
      "Epoch 32/200\n",
      "1256/1256 [==============================] - 0s 110us/step - loss: 1.1560 - acc: 0.6823 - auc: 0.6305 - val_loss: 0.5928 - val_acc: 0.8095 - val_auc: 0.6323\n",
      "Epoch 33/200\n",
      "1256/1256 [==============================] - 0s 116us/step - loss: 1.1862 - acc: 0.6592 - auc: 0.6340 - val_loss: 0.6210 - val_acc: 0.7587 - val_auc: 0.6349\n",
      "Epoch 34/200\n",
      "1256/1256 [==============================] - 0s 119us/step - loss: 1.1948 - acc: 0.6505 - auc: 0.6359 - val_loss: 0.6020 - val_acc: 0.7778 - val_auc: 0.6368\n",
      "Epoch 35/200\n",
      "1256/1256 [==============================] - 0s 107us/step - loss: 1.1274 - acc: 0.6354 - auc: 0.6380 - val_loss: 0.6289 - val_acc: 0.6984 - val_auc: 0.6382\n",
      "Epoch 36/200\n",
      "1256/1256 [==============================] - 0s 104us/step - loss: 1.1388 - acc: 0.6170 - auc: 0.6387 - val_loss: 0.6052 - val_acc: 0.7397 - val_auc: 0.6392\n",
      "Epoch 37/200\n",
      "1256/1256 [==============================] - 0s 106us/step - loss: 1.1457 - acc: 0.6640 - auc: 0.6404 - val_loss: 0.5756 - val_acc: 0.8032 - val_auc: 0.6419\n",
      "Epoch 38/200\n",
      "1256/1256 [==============================] - 0s 115us/step - loss: 1.1482 - acc: 0.6943 - auc: 0.6437 - val_loss: 0.5696 - val_acc: 0.8032 - val_auc: 0.6454\n",
      "Epoch 39/200\n",
      "1256/1256 [==============================] - 0s 110us/step - loss: 1.1385 - acc: 0.6807 - auc: 0.6470 - val_loss: 0.5896 - val_acc: 0.7714 - val_auc: 0.6479\n",
      "Epoch 40/200\n",
      "1256/1256 [==============================] - 0s 118us/step - loss: 1.1314 - acc: 0.6871 - auc: 0.6491 - val_loss: 0.5843 - val_acc: 0.7683 - val_auc: 0.6504\n",
      "Epoch 41/200\n",
      "1256/1256 [==============================] - 0s 115us/step - loss: 1.1517 - acc: 0.6473 - auc: 0.6512 - val_loss: 0.5956 - val_acc: 0.7365 - val_auc: 0.6516\n",
      "Epoch 42/200\n",
      "1256/1256 [==============================] - 0s 112us/step - loss: 1.1248 - acc: 0.6497 - auc: 0.6523 - val_loss: 0.5849 - val_acc: 0.7365 - val_auc: 0.6529\n",
      "Epoch 43/200\n",
      "1256/1256 [==============================] - 0s 114us/step - loss: 1.1468 - acc: 0.6449 - auc: 0.6535 - val_loss: 0.5914 - val_acc: 0.7365 - val_auc: 0.6541\n",
      "Epoch 44/200\n",
      "1256/1256 [==============================] - 0s 121us/step - loss: 1.1006 - acc: 0.6656 - auc: 0.6550 - val_loss: 0.5800 - val_acc: 0.7429 - val_auc: 0.6557\n",
      "Epoch 45/200\n",
      "1256/1256 [==============================] - 0s 116us/step - loss: 1.0939 - acc: 0.6720 - auc: 0.6566 - val_loss: 0.5709 - val_acc: 0.7492 - val_auc: 0.6576\n",
      "Epoch 46/200\n",
      "1256/1256 [==============================] - 0s 118us/step - loss: 1.1311 - acc: 0.6791 - auc: 0.6586 - val_loss: 0.5811 - val_acc: 0.7333 - val_auc: 0.6593\n",
      "Epoch 47/200\n",
      "1256/1256 [==============================] - 0s 111us/step - loss: 1.1239 - acc: 0.6720 - auc: 0.6602 - val_loss: 0.5750 - val_acc: 0.7460 - val_auc: 0.6609\n",
      "Epoch 48/200\n",
      "1256/1256 [==============================] - 0s 111us/step - loss: 1.0771 - acc: 0.6736 - auc: 0.6619 - val_loss: 0.5638 - val_acc: 0.7524 - val_auc: 0.6628\n",
      "Epoch 49/200\n",
      "1256/1256 [==============================] - 0s 112us/step - loss: 1.0831 - acc: 0.6696 - auc: 0.6636 - val_loss: 0.5718 - val_acc: 0.7079 - val_auc: 0.6643\n",
      "Epoch 50/200\n",
      "1256/1256 [==============================] - 0s 111us/step - loss: 1.1159 - acc: 0.6592 - auc: 0.6648 - val_loss: 0.5609 - val_acc: 0.7175 - val_auc: 0.6655\n",
      "Epoch 51/200\n",
      "1256/1256 [==============================] - 0s 113us/step - loss: 1.0773 - acc: 0.6680 - auc: 0.6664 - val_loss: 0.5601 - val_acc: 0.7143 - val_auc: 0.6670\n",
      "Epoch 52/200\n",
      "1256/1256 [==============================] - 0s 118us/step - loss: 1.1004 - acc: 0.6632 - auc: 0.6676 - val_loss: 0.5581 - val_acc: 0.7143 - val_auc: 0.6683\n",
      "Epoch 53/200\n",
      "1256/1256 [==============================] - 0s 114us/step - loss: 1.0384 - acc: 0.6975 - auc: 0.6692 - val_loss: 0.5393 - val_acc: 0.7587 - val_auc: 0.6702\n",
      "Epoch 54/200\n",
      "1256/1256 [==============================] - 0s 106us/step - loss: 1.0869 - acc: 0.6839 - auc: 0.6711 - val_loss: 0.5327 - val_acc: 0.7683 - val_auc: 0.6720\n",
      "Epoch 55/200\n",
      "1256/1256 [==============================] - 0s 109us/step - loss: 1.0802 - acc: 0.6975 - auc: 0.6731 - val_loss: 0.5459 - val_acc: 0.7302 - val_auc: 0.6739\n",
      "Epoch 56/200\n",
      "1256/1256 [==============================] - 0s 104us/step - loss: 1.1251 - acc: 0.6815 - auc: 0.6746 - val_loss: 0.5331 - val_acc: 0.7619 - val_auc: 0.6752\n",
      "Epoch 57/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.0634 - acc: 0.6839 - auc: 0.6761 - val_loss: 0.5317 - val_acc: 0.7619 - val_auc: 0.6769\n",
      "Epoch 58/200\n",
      "1256/1256 [==============================] - 0s 97us/step - loss: 1.0681 - acc: 0.6879 - auc: 0.6778 - val_loss: 0.5359 - val_acc: 0.7524 - val_auc: 0.6784\n",
      "Epoch 59/200\n",
      "1256/1256 [==============================] - 0s 101us/step - loss: 1.1201 - acc: 0.6903 - auc: 0.6791 - val_loss: 0.5345 - val_acc: 0.7683 - val_auc: 0.6800\n",
      "Epoch 60/200\n",
      "1256/1256 [==============================] - 0s 101us/step - loss: 1.0724 - acc: 0.6688 - auc: 0.6806 - val_loss: 0.5338 - val_acc: 0.7714 - val_auc: 0.6814\n",
      "Epoch 61/200\n",
      "1256/1256 [==============================] - 0s 98us/step - loss: 1.0475 - acc: 0.6895 - auc: 0.6821 - val_loss: 0.5398 - val_acc: 0.7302 - val_auc: 0.6828\n",
      "Epoch 62/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.0806 - acc: 0.6656 - auc: 0.6833 - val_loss: 0.5349 - val_acc: 0.7492 - val_auc: 0.6838\n",
      "Epoch 63/200\n",
      "1256/1256 [==============================] - 0s 110us/step - loss: 1.0304 - acc: 0.7054 - auc: 0.6847 - val_loss: 0.5291 - val_acc: 0.7619 - val_auc: 0.6854\n",
      "Epoch 64/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.1116 - acc: 0.6879 - auc: 0.6860 - val_loss: 0.5409 - val_acc: 0.7333 - val_auc: 0.6866\n",
      "Epoch 65/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 1.0450 - acc: 0.6975 - auc: 0.6873 - val_loss: 0.5391 - val_acc: 0.7365 - val_auc: 0.6880\n",
      "Epoch 66/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.0900 - acc: 0.6545 - auc: 0.6884 - val_loss: 0.5365 - val_acc: 0.7556 - val_auc: 0.6888\n",
      "Epoch 67/200\n",
      "1256/1256 [==============================] - 0s 103us/step - loss: 1.0401 - acc: 0.6975 - auc: 0.6896 - val_loss: 0.5278 - val_acc: 0.7746 - val_auc: 0.6902\n",
      "Epoch 68/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 1.0435 - acc: 0.6871 - auc: 0.6908 - val_loss: 0.5391 - val_acc: 0.6952 - val_auc: 0.6913\n",
      "Epoch 69/200\n",
      "1256/1256 [==============================] - 0s 101us/step - loss: 1.0420 - acc: 0.6871 - auc: 0.6917 - val_loss: 0.5351 - val_acc: 0.6984 - val_auc: 0.6921\n",
      "Epoch 70/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.0099 - acc: 0.7014 - auc: 0.6927 - val_loss: 0.5271 - val_acc: 0.7079 - val_auc: 0.6933\n",
      "Epoch 71/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 1.0726 - acc: 0.6799 - auc: 0.6937 - val_loss: 0.5259 - val_acc: 0.7397 - val_auc: 0.6943\n",
      "Epoch 72/200\n",
      "1256/1256 [==============================] - 0s 117us/step - loss: 1.0957 - acc: 0.6783 - auc: 0.6949 - val_loss: 0.5326 - val_acc: 0.7175 - val_auc: 0.6953\n",
      "Epoch 73/200\n",
      "1256/1256 [==============================] - 0s 110us/step - loss: 1.0748 - acc: 0.6720 - auc: 0.6957 - val_loss: 0.5364 - val_acc: 0.6984 - val_auc: 0.6960\n",
      "Epoch 74/200\n",
      "1256/1256 [==============================] - 0s 108us/step - loss: 1.0189 - acc: 0.6871 - auc: 0.6966 - val_loss: 0.5380 - val_acc: 0.6952 - val_auc: 0.6969\n",
      "Epoch 75/200\n",
      "1256/1256 [==============================] - 0s 114us/step - loss: 1.0251 - acc: 0.6616 - auc: 0.6971 - val_loss: 0.5344 - val_acc: 0.6889 - val_auc: 0.6974\n",
      "Epoch 76/200\n",
      "1256/1256 [==============================] - 0s 114us/step - loss: 1.0457 - acc: 0.6823 - auc: 0.6978 - val_loss: 0.5156 - val_acc: 0.7175 - val_auc: 0.6982\n",
      "Epoch 77/200\n",
      "1256/1256 [==============================] - 0s 110us/step - loss: 1.0191 - acc: 0.6855 - auc: 0.6987 - val_loss: 0.5236 - val_acc: 0.7206 - val_auc: 0.6991\n",
      "Epoch 78/200\n",
      "1256/1256 [==============================] - 0s 104us/step - loss: 1.0616 - acc: 0.6672 - auc: 0.6994 - val_loss: 0.5310 - val_acc: 0.6889 - val_auc: 0.6997\n",
      "Epoch 79/200\n",
      "1256/1256 [==============================] - 0s 106us/step - loss: 1.0348 - acc: 0.6728 - auc: 0.7001 - val_loss: 0.5397 - val_acc: 0.6730 - val_auc: 0.7003\n",
      "Epoch 80/200\n",
      "1256/1256 [==============================] - 0s 106us/step - loss: 1.0107 - acc: 0.6760 - auc: 0.7006 - val_loss: 0.5216 - val_acc: 0.6889 - val_auc: 0.7010\n",
      "Epoch 81/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 1.0389 - acc: 0.6656 - auc: 0.7013 - val_loss: 0.5249 - val_acc: 0.6698 - val_auc: 0.7015\n",
      "Epoch 82/200\n",
      "1256/1256 [==============================] - 0s 108us/step - loss: 1.0590 - acc: 0.6545 - auc: 0.7017 - val_loss: 0.5482 - val_acc: 0.6254 - val_auc: 0.7018\n",
      "Epoch 83/200\n",
      "1256/1256 [==============================] - 0s 121us/step - loss: 1.0291 - acc: 0.6616 - auc: 0.7020 - val_loss: 0.5394 - val_acc: 0.6349 - val_auc: 0.7022\n",
      "Epoch 84/200\n",
      "1256/1256 [==============================] - 0s 113us/step - loss: 1.0193 - acc: 0.6648 - auc: 0.7024 - val_loss: 0.5357 - val_acc: 0.6476 - val_auc: 0.7027\n",
      "Epoch 85/200\n",
      "1256/1256 [==============================] - 0s 118us/step - loss: 1.0221 - acc: 0.6807 - auc: 0.7030 - val_loss: 0.5312 - val_acc: 0.6540 - val_auc: 0.7033\n",
      "Epoch 86/200\n",
      "1256/1256 [==============================] - 0s 114us/step - loss: 1.0354 - acc: 0.6664 - auc: 0.7036 - val_loss: 0.5344 - val_acc: 0.6571 - val_auc: 0.7038\n",
      "Epoch 87/200\n",
      "1256/1256 [==============================] - 0s 112us/step - loss: 1.0524 - acc: 0.6696 - auc: 0.7041 - val_loss: 0.5310 - val_acc: 0.6698 - val_auc: 0.7043\n",
      "Epoch 88/200\n",
      "1256/1256 [==============================] - 0s 106us/step - loss: 1.0460 - acc: 0.6704 - auc: 0.7047 - val_loss: 0.5276 - val_acc: 0.6762 - val_auc: 0.7048\n",
      "Epoch 89/200\n",
      "1256/1256 [==============================] - 0s 107us/step - loss: 1.0275 - acc: 0.6664 - auc: 0.7051 - val_loss: 0.5270 - val_acc: 0.6794 - val_auc: 0.7053\n",
      "Epoch 90/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.0183 - acc: 0.6847 - auc: 0.7056 - val_loss: 0.5210 - val_acc: 0.6921 - val_auc: 0.7060\n",
      "Epoch 91/200\n",
      "1256/1256 [==============================] - 0s 99us/step - loss: 1.0389 - acc: 0.6807 - auc: 0.7063 - val_loss: 0.5165 - val_acc: 0.7016 - val_auc: 0.7066\n",
      "Epoch 92/200\n",
      "1256/1256 [==============================] - 0s 113us/step - loss: 1.0252 - acc: 0.6863 - auc: 0.7070 - val_loss: 0.5019 - val_acc: 0.7587 - val_auc: 0.7073\n",
      "Epoch 93/200\n",
      "1256/1256 [==============================] - 0s 111us/step - loss: 1.0634 - acc: 0.6871 - auc: 0.7077 - val_loss: 0.5145 - val_acc: 0.7302 - val_auc: 0.7080\n",
      "Epoch 94/200\n",
      "1256/1256 [==============================] - 0s 109us/step - loss: 1.0129 - acc: 0.6911 - auc: 0.7084 - val_loss: 0.5230 - val_acc: 0.7048 - val_auc: 0.7088\n",
      "Epoch 95/200\n",
      "1256/1256 [==============================] - 0s 114us/step - loss: 1.0118 - acc: 0.6887 - auc: 0.7091 - val_loss: 0.5339 - val_acc: 0.6857 - val_auc: 0.7094\n",
      "Epoch 96/200\n",
      "1256/1256 [==============================] - 0s 116us/step - loss: 0.9768 - acc: 0.7134 - auc: 0.7099 - val_loss: 0.5184 - val_acc: 0.7302 - val_auc: 0.7103\n",
      "Epoch 97/200\n",
      "1256/1256 [==============================] - 0s 104us/step - loss: 1.0284 - acc: 0.6760 - auc: 0.7105 - val_loss: 0.5301 - val_acc: 0.6730 - val_auc: 0.7108\n",
      "Epoch 98/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.0202 - acc: 0.6903 - auc: 0.7112 - val_loss: 0.5212 - val_acc: 0.6762 - val_auc: 0.7115\n",
      "Epoch 99/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.0550 - acc: 0.6783 - auc: 0.7117 - val_loss: 0.5468 - val_acc: 0.6317 - val_auc: 0.7119\n",
      "Epoch 100/200\n",
      "1256/1256 [==============================] - 0s 105us/step - loss: 0.9688 - acc: 0.7030 - auc: 0.7121 - val_loss: 0.5459 - val_acc: 0.6444 - val_auc: 0.7124\n",
      "Epoch 101/200\n",
      "1256/1256 [==============================] - 0s 105us/step - loss: 1.0229 - acc: 0.6648 - auc: 0.7125 - val_loss: 0.5230 - val_acc: 0.6825 - val_auc: 0.7127\n",
      "Epoch 102/200\n",
      "1256/1256 [==============================] - 0s 97us/step - loss: 0.9956 - acc: 0.6863 - auc: 0.7129 - val_loss: 0.5186 - val_acc: 0.6889 - val_auc: 0.7132\n",
      "Epoch 103/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.0112 - acc: 0.6919 - auc: 0.7135 - val_loss: 0.5211 - val_acc: 0.6921 - val_auc: 0.7137\n",
      "Epoch 104/200\n",
      "1256/1256 [==============================] - 0s 111us/step - loss: 1.0484 - acc: 0.6744 - auc: 0.7140 - val_loss: 0.5263 - val_acc: 0.6730 - val_auc: 0.7141\n",
      "Epoch 105/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.0407 - acc: 0.6919 - auc: 0.7144 - val_loss: 0.5213 - val_acc: 0.6730 - val_auc: 0.7147\n",
      "Epoch 106/200\n",
      "1256/1256 [==============================] - 0s 98us/step - loss: 1.0193 - acc: 0.6744 - auc: 0.7149 - val_loss: 0.5161 - val_acc: 0.6889 - val_auc: 0.7151\n",
      "Epoch 107/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.0176 - acc: 0.6982 - auc: 0.7154 - val_loss: 0.5033 - val_acc: 0.6984 - val_auc: 0.7157\n",
      "Epoch 108/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 1.0151 - acc: 0.6791 - auc: 0.7159 - val_loss: 0.5143 - val_acc: 0.6857 - val_auc: 0.7161\n",
      "Epoch 109/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 1.0233 - acc: 0.6752 - auc: 0.7163 - val_loss: 0.5238 - val_acc: 0.6571 - val_auc: 0.7165\n",
      "Epoch 110/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.0458 - acc: 0.6688 - auc: 0.7167 - val_loss: 0.5212 - val_acc: 0.6825 - val_auc: 0.7168\n",
      "Epoch 111/200\n",
      "1256/1256 [==============================] - 0s 99us/step - loss: 0.9757 - acc: 0.6982 - auc: 0.7171 - val_loss: 0.5151 - val_acc: 0.6889 - val_auc: 0.7174\n",
      "Epoch 112/200\n",
      "1256/1256 [==============================] - 0s 109us/step - loss: 0.9903 - acc: 0.6799 - auc: 0.7176 - val_loss: 0.5078 - val_acc: 0.7048 - val_auc: 0.7179\n",
      "Epoch 113/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.0529 - acc: 0.6895 - auc: 0.7182 - val_loss: 0.5186 - val_acc: 0.6698 - val_auc: 0.7184\n",
      "Epoch 114/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 0.9835 - acc: 0.6712 - auc: 0.7186 - val_loss: 0.5186 - val_acc: 0.6698 - val_auc: 0.7188\n",
      "Epoch 115/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 0.9966 - acc: 0.6879 - auc: 0.7190 - val_loss: 0.5202 - val_acc: 0.6730 - val_auc: 0.7192\n",
      "Epoch 116/200\n",
      "1256/1256 [==============================] - 0s 110us/step - loss: 1.0251 - acc: 0.6624 - auc: 0.7194 - val_loss: 0.5261 - val_acc: 0.6571 - val_auc: 0.7194\n",
      "Epoch 117/200\n",
      "1256/1256 [==============================] - 0s 111us/step - loss: 0.9933 - acc: 0.6696 - auc: 0.7196 - val_loss: 0.5185 - val_acc: 0.6603 - val_auc: 0.7198\n",
      "Epoch 118/200\n",
      "1256/1256 [==============================] - 0s 108us/step - loss: 0.9861 - acc: 0.6791 - auc: 0.7200 - val_loss: 0.5074 - val_acc: 0.6730 - val_auc: 0.7201\n",
      "Epoch 119/200\n",
      "1256/1256 [==============================] - 0s 114us/step - loss: 1.0389 - acc: 0.6807 - auc: 0.7203 - val_loss: 0.5173 - val_acc: 0.6698 - val_auc: 0.7205\n",
      "Epoch 120/200\n",
      "1256/1256 [==============================] - 0s 117us/step - loss: 0.9876 - acc: 0.6990 - auc: 0.7207 - val_loss: 0.5111 - val_acc: 0.6889 - val_auc: 0.7210\n",
      "Epoch 121/200\n",
      "1256/1256 [==============================] - 0s 118us/step - loss: 1.0281 - acc: 0.6664 - auc: 0.7211 - val_loss: 0.5221 - val_acc: 0.6825 - val_auc: 0.7213\n",
      "Epoch 122/200\n",
      "1256/1256 [==============================] - 0s 101us/step - loss: 0.9732 - acc: 0.6799 - auc: 0.7214 - val_loss: 0.5087 - val_acc: 0.7048 - val_auc: 0.7216\n",
      "Epoch 123/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 1.0201 - acc: 0.7086 - auc: 0.7219 - val_loss: 0.5222 - val_acc: 0.6825 - val_auc: 0.7221\n",
      "Epoch 124/200\n",
      "1256/1256 [==============================] - 0s 103us/step - loss: 1.0244 - acc: 0.6903 - auc: 0.7223 - val_loss: 0.5168 - val_acc: 0.7111 - val_auc: 0.7225\n",
      "Epoch 125/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 0.9817 - acc: 0.6895 - auc: 0.7227 - val_loss: 0.5188 - val_acc: 0.7016 - val_auc: 0.7230\n",
      "Epoch 126/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.0878 - acc: 0.6736 - auc: 0.7231 - val_loss: 0.5310 - val_acc: 0.6794 - val_auc: 0.7232\n",
      "Epoch 127/200\n",
      "1256/1256 [==============================] - 0s 124us/step - loss: 1.0185 - acc: 0.6640 - auc: 0.7233 - val_loss: 0.5459 - val_acc: 0.6635 - val_auc: 0.7234\n",
      "Epoch 128/200\n",
      "1256/1256 [==============================] - 0s 114us/step - loss: 1.0087 - acc: 0.6616 - auc: 0.7235 - val_loss: 0.5376 - val_acc: 0.6667 - val_auc: 0.7236\n",
      "Epoch 129/200\n",
      "1256/1256 [==============================] - 0s 112us/step - loss: 0.9977 - acc: 0.6688 - auc: 0.7237 - val_loss: 0.5199 - val_acc: 0.6921 - val_auc: 0.7238\n",
      "Epoch 130/200\n",
      "1256/1256 [==============================] - 0s 114us/step - loss: 0.9963 - acc: 0.6775 - auc: 0.7241 - val_loss: 0.5257 - val_acc: 0.6857 - val_auc: 0.7242\n",
      "Epoch 131/200\n",
      "1256/1256 [==============================] - 0s 117us/step - loss: 0.9990 - acc: 0.6799 - auc: 0.7243 - val_loss: 0.5306 - val_acc: 0.6762 - val_auc: 0.7245\n",
      "Epoch 132/200\n",
      "1256/1256 [==============================] - 0s 109us/step - loss: 0.9848 - acc: 0.6592 - auc: 0.7246 - val_loss: 0.5152 - val_acc: 0.6921 - val_auc: 0.7247\n",
      "Epoch 133/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.0226 - acc: 0.6688 - auc: 0.7249 - val_loss: 0.5101 - val_acc: 0.6921 - val_auc: 0.7250\n",
      "Epoch 134/200\n",
      "1256/1256 [==============================] - 0s 105us/step - loss: 1.0117 - acc: 0.6775 - auc: 0.7252 - val_loss: 0.5159 - val_acc: 0.6952 - val_auc: 0.7253\n",
      "Epoch 135/200\n",
      "1256/1256 [==============================] - 0s 111us/step - loss: 0.9499 - acc: 0.6959 - auc: 0.7255 - val_loss: 0.4979 - val_acc: 0.7206 - val_auc: 0.7258\n",
      "Epoch 136/200\n",
      "1256/1256 [==============================] - 0s 110us/step - loss: 1.0024 - acc: 0.6879 - auc: 0.7260 - val_loss: 0.5013 - val_acc: 0.7143 - val_auc: 0.7262\n",
      "Epoch 137/200\n",
      "1256/1256 [==============================] - 0s 115us/step - loss: 0.9990 - acc: 0.6895 - auc: 0.7264 - val_loss: 0.5110 - val_acc: 0.6952 - val_auc: 0.7266\n",
      "Epoch 138/200\n",
      "1256/1256 [==============================] - 0s 114us/step - loss: 0.9523 - acc: 0.6863 - auc: 0.7268 - val_loss: 0.5109 - val_acc: 0.6984 - val_auc: 0.7270\n",
      "Epoch 139/200\n",
      "1256/1256 [==============================] - 0s 122us/step - loss: 0.9902 - acc: 0.6911 - auc: 0.7271 - val_loss: 0.4965 - val_acc: 0.7079 - val_auc: 0.7274\n",
      "Epoch 140/200\n",
      "1256/1256 [==============================] - 0s 110us/step - loss: 0.9873 - acc: 0.6967 - auc: 0.7276 - val_loss: 0.5052 - val_acc: 0.6921 - val_auc: 0.7278\n",
      "Epoch 141/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 0.9658 - acc: 0.7014 - auc: 0.7280 - val_loss: 0.5060 - val_acc: 0.6921 - val_auc: 0.7282\n",
      "Epoch 142/200\n",
      "1256/1256 [==============================] - 0s 108us/step - loss: 1.0263 - acc: 0.6728 - auc: 0.7284 - val_loss: 0.5144 - val_acc: 0.6762 - val_auc: 0.7285\n",
      "Epoch 143/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.0112 - acc: 0.6911 - auc: 0.7287 - val_loss: 0.5037 - val_acc: 0.7016 - val_auc: 0.7288\n",
      "Epoch 144/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 0.9978 - acc: 0.6863 - auc: 0.7290 - val_loss: 0.5075 - val_acc: 0.6952 - val_auc: 0.7292\n",
      "Epoch 145/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.0265 - acc: 0.6879 - auc: 0.7293 - val_loss: 0.5217 - val_acc: 0.6730 - val_auc: 0.7295\n",
      "Epoch 146/200\n",
      "1256/1256 [==============================] - 0s 101us/step - loss: 0.9686 - acc: 0.6943 - auc: 0.7297 - val_loss: 0.5260 - val_acc: 0.6667 - val_auc: 0.7298\n",
      "Epoch 147/200\n",
      "1256/1256 [==============================] - 0s 97us/step - loss: 0.9750 - acc: 0.6760 - auc: 0.7300 - val_loss: 0.5267 - val_acc: 0.6635 - val_auc: 0.7301\n",
      "Epoch 148/200\n",
      "1256/1256 [==============================] - 0s 99us/step - loss: 1.0369 - acc: 0.6696 - auc: 0.7302 - val_loss: 0.5261 - val_acc: 0.6603 - val_auc: 0.7303\n",
      "Epoch 149/200\n",
      "1256/1256 [==============================] - 0s 99us/step - loss: 0.9733 - acc: 0.6831 - auc: 0.7304 - val_loss: 0.5154 - val_acc: 0.6762 - val_auc: 0.7306\n",
      "Epoch 150/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 0.9423 - acc: 0.6807 - auc: 0.7307 - val_loss: 0.5376 - val_acc: 0.6635 - val_auc: 0.7309\n",
      "Epoch 151/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 1.0052 - acc: 0.6768 - auc: 0.7309 - val_loss: 0.5398 - val_acc: 0.6571 - val_auc: 0.7310\n",
      "Epoch 152/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 0.9786 - acc: 0.6728 - auc: 0.7311 - val_loss: 0.5362 - val_acc: 0.6571 - val_auc: 0.7312\n",
      "Epoch 153/200\n",
      "1256/1256 [==============================] - 0s 101us/step - loss: 0.9619 - acc: 0.6831 - auc: 0.7313 - val_loss: 0.5344 - val_acc: 0.6571 - val_auc: 0.7314\n",
      "Epoch 154/200\n",
      "1256/1256 [==============================] - 0s 108us/step - loss: 1.0015 - acc: 0.6775 - auc: 0.7315 - val_loss: 0.5215 - val_acc: 0.6698 - val_auc: 0.7316\n",
      "Epoch 155/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 0.9636 - acc: 0.6911 - auc: 0.7317 - val_loss: 0.5169 - val_acc: 0.6794 - val_auc: 0.7319\n",
      "Epoch 156/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 0.9609 - acc: 0.6871 - auc: 0.7320 - val_loss: 0.5071 - val_acc: 0.6762 - val_auc: 0.7322\n",
      "Epoch 157/200\n",
      "1256/1256 [==============================] - 0s 95us/step - loss: 0.9445 - acc: 0.7062 - auc: 0.7324 - val_loss: 0.5047 - val_acc: 0.6952 - val_auc: 0.7326\n",
      "Epoch 158/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 0.9913 - acc: 0.6863 - auc: 0.7328 - val_loss: 0.5127 - val_acc: 0.6794 - val_auc: 0.7329\n",
      "Epoch 159/200\n",
      "1256/1256 [==============================] - 0s 113us/step - loss: 0.9782 - acc: 0.7014 - auc: 0.7331 - val_loss: 0.5027 - val_acc: 0.6921 - val_auc: 0.7332\n",
      "Epoch 160/200\n",
      "1256/1256 [==============================] - 0s 106us/step - loss: 0.9663 - acc: 0.6927 - auc: 0.7334 - val_loss: 0.5133 - val_acc: 0.6667 - val_auc: 0.7335\n",
      "Epoch 161/200\n",
      "1256/1256 [==============================] - 0s 113us/step - loss: 0.9906 - acc: 0.6831 - auc: 0.7336 - val_loss: 0.5097 - val_acc: 0.6794 - val_auc: 0.7337\n",
      "Epoch 162/200\n",
      "1256/1256 [==============================] - 0s 118us/step - loss: 0.9714 - acc: 0.6744 - auc: 0.7339 - val_loss: 0.5047 - val_acc: 0.6825 - val_auc: 0.7339\n",
      "Epoch 163/200\n",
      "1256/1256 [==============================] - 0s 116us/step - loss: 0.9813 - acc: 0.6847 - auc: 0.7341 - val_loss: 0.5071 - val_acc: 0.6762 - val_auc: 0.7342\n",
      "Epoch 164/200\n",
      "1256/1256 [==============================] - 0s 105us/step - loss: 0.9949 - acc: 0.6823 - auc: 0.7343 - val_loss: 0.5094 - val_acc: 0.6698 - val_auc: 0.7345\n",
      "Epoch 165/200\n",
      "1256/1256 [==============================] - 0s 98us/step - loss: 0.9418 - acc: 0.6887 - auc: 0.7346 - val_loss: 0.5105 - val_acc: 0.6667 - val_auc: 0.7347\n",
      "Epoch 166/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 0.9732 - acc: 0.6831 - auc: 0.7349 - val_loss: 0.5081 - val_acc: 0.6698 - val_auc: 0.7350\n",
      "Epoch 167/200\n",
      "1256/1256 [==============================] - 0s 98us/step - loss: 1.0301 - acc: 0.6855 - auc: 0.7351 - val_loss: 0.5149 - val_acc: 0.6603 - val_auc: 0.7352\n",
      "Epoch 168/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 0.9897 - acc: 0.6768 - auc: 0.7353 - val_loss: 0.5135 - val_acc: 0.6603 - val_auc: 0.7354\n",
      "Epoch 169/200\n",
      "1256/1256 [==============================] - 0s 101us/step - loss: 0.9430 - acc: 0.6791 - auc: 0.7355 - val_loss: 0.5131 - val_acc: 0.6476 - val_auc: 0.7356\n",
      "Epoch 170/200\n",
      "1256/1256 [==============================] - 0s 114us/step - loss: 0.9668 - acc: 0.6927 - auc: 0.7357 - val_loss: 0.5041 - val_acc: 0.6635 - val_auc: 0.7358\n",
      "Epoch 171/200\n",
      "1256/1256 [==============================] - 0s 114us/step - loss: 1.0652 - acc: 0.6648 - auc: 0.7359 - val_loss: 0.5140 - val_acc: 0.6349 - val_auc: 0.7359\n",
      "Epoch 172/200\n",
      "1256/1256 [==============================] - 0s 108us/step - loss: 0.9564 - acc: 0.6839 - auc: 0.7360 - val_loss: 0.5040 - val_acc: 0.6762 - val_auc: 0.7361\n",
      "Epoch 173/200\n",
      "1256/1256 [==============================] - 0s 110us/step - loss: 0.9755 - acc: 0.6863 - auc: 0.7363 - val_loss: 0.4925 - val_acc: 0.6857 - val_auc: 0.7364\n",
      "Epoch 174/200\n",
      "1256/1256 [==============================] - 0s 111us/step - loss: 1.0093 - acc: 0.6688 - auc: 0.7365 - val_loss: 0.5164 - val_acc: 0.6603 - val_auc: 0.7365\n",
      "Epoch 175/200\n",
      "1256/1256 [==============================] - 0s 115us/step - loss: 1.0443 - acc: 0.6672 - auc: 0.7366 - val_loss: 0.5094 - val_acc: 0.6889 - val_auc: 0.7367\n",
      "Epoch 176/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 0.9574 - acc: 0.6839 - auc: 0.7368 - val_loss: 0.5007 - val_acc: 0.7016 - val_auc: 0.7370\n",
      "Epoch 177/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 0.9528 - acc: 0.6807 - auc: 0.7371 - val_loss: 0.5038 - val_acc: 0.6825 - val_auc: 0.7372\n",
      "Epoch 178/200\n",
      "1256/1256 [==============================] - 0s 114us/step - loss: 0.9557 - acc: 0.6823 - auc: 0.7373 - val_loss: 0.5041 - val_acc: 0.6794 - val_auc: 0.7374\n",
      "Epoch 179/200\n",
      "1256/1256 [==============================] - 0s 118us/step - loss: 0.9832 - acc: 0.6775 - auc: 0.7375 - val_loss: 0.5015 - val_acc: 0.6762 - val_auc: 0.7376\n",
      "Epoch 180/200\n",
      "1256/1256 [==============================] - 0s 106us/step - loss: 0.9833 - acc: 0.6736 - auc: 0.7377 - val_loss: 0.5050 - val_acc: 0.6698 - val_auc: 0.7378\n",
      "Epoch 181/200\n",
      "1256/1256 [==============================] - 0s 109us/step - loss: 0.9856 - acc: 0.6624 - auc: 0.7379 - val_loss: 0.5139 - val_acc: 0.6476 - val_auc: 0.7379\n",
      "Epoch 182/200\n",
      "1256/1256 [==============================] - 0s 110us/step - loss: 0.9493 - acc: 0.6871 - auc: 0.7381 - val_loss: 0.5150 - val_acc: 0.6508 - val_auc: 0.7381\n",
      "Epoch 183/200\n",
      "1256/1256 [==============================] - 0s 106us/step - loss: 0.9942 - acc: 0.6839 - auc: 0.7382 - val_loss: 0.4999 - val_acc: 0.6698 - val_auc: 0.7383\n",
      "Epoch 184/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 0.9635 - acc: 0.6688 - auc: 0.7384 - val_loss: 0.5144 - val_acc: 0.6349 - val_auc: 0.7385\n",
      "Epoch 185/200\n",
      "1256/1256 [==============================] - 0s 100us/step - loss: 0.9574 - acc: 0.6847 - auc: 0.7386 - val_loss: 0.5095 - val_acc: 0.6571 - val_auc: 0.7387\n",
      "Epoch 186/200\n",
      "1256/1256 [==============================] - 0s 106us/step - loss: 0.9218 - acc: 0.6839 - auc: 0.7388 - val_loss: 0.5035 - val_acc: 0.6667 - val_auc: 0.7389\n",
      "Epoch 187/200\n",
      "1256/1256 [==============================] - 0s 104us/step - loss: 0.9553 - acc: 0.6879 - auc: 0.7390 - val_loss: 0.5068 - val_acc: 0.6381 - val_auc: 0.7391\n",
      "Epoch 188/200\n",
      "1256/1256 [==============================] - 0s 98us/step - loss: 0.9824 - acc: 0.6783 - auc: 0.7392 - val_loss: 0.5146 - val_acc: 0.6381 - val_auc: 0.7393\n",
      "Epoch 189/200\n",
      "1256/1256 [==============================] - 0s 96us/step - loss: 0.9887 - acc: 0.6815 - auc: 0.7394 - val_loss: 0.5147 - val_acc: 0.6540 - val_auc: 0.7395\n",
      "Epoch 190/200\n",
      "1256/1256 [==============================] - 0s 99us/step - loss: 0.9936 - acc: 0.6608 - auc: 0.7396 - val_loss: 0.5253 - val_acc: 0.6254 - val_auc: 0.7396\n",
      "Epoch 191/200\n",
      "1256/1256 [==============================] - 0s 101us/step - loss: 1.0300 - acc: 0.6505 - auc: 0.7396 - val_loss: 0.5347 - val_acc: 0.6190 - val_auc: 0.7396\n",
      "Epoch 192/200\n",
      "1256/1256 [==============================] - 0s 103us/step - loss: 0.9382 - acc: 0.6911 - auc: 0.7397 - val_loss: 0.5202 - val_acc: 0.6508 - val_auc: 0.7398\n",
      "Epoch 193/200\n",
      "1256/1256 [==============================] - 0s 97us/step - loss: 0.9722 - acc: 0.6783 - auc: 0.7399 - val_loss: 0.5257 - val_acc: 0.6413 - val_auc: 0.7400\n",
      "Epoch 194/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 0.9797 - acc: 0.6704 - auc: 0.7400 - val_loss: 0.5308 - val_acc: 0.6159 - val_auc: 0.7401\n",
      "Epoch 195/200\n",
      "1256/1256 [==============================] - 0s 110us/step - loss: 0.9164 - acc: 0.6736 - auc: 0.7401 - val_loss: 0.5418 - val_acc: 0.6095 - val_auc: 0.7402\n",
      "Epoch 196/200\n",
      "1256/1256 [==============================] - 0s 107us/step - loss: 0.9916 - acc: 0.6783 - auc: 0.7402 - val_loss: 0.5377 - val_acc: 0.6127 - val_auc: 0.7403\n",
      "Epoch 197/200\n",
      "1256/1256 [==============================] - 0s 113us/step - loss: 0.9117 - acc: 0.6903 - auc: 0.7403 - val_loss: 0.5211 - val_acc: 0.6540 - val_auc: 0.7405\n",
      "Epoch 198/200\n",
      "1256/1256 [==============================] - 0s 102us/step - loss: 0.9358 - acc: 0.7038 - auc: 0.7406 - val_loss: 0.5249 - val_acc: 0.6635 - val_auc: 0.7407\n",
      "Epoch 199/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 0.9625 - acc: 0.6799 - auc: 0.7408 - val_loss: 0.5180 - val_acc: 0.6476 - val_auc: 0.7408\n",
      "Epoch 200/200\n",
      "1256/1256 [==============================] - 0s 94us/step - loss: 0.9806 - acc: 0.6768 - auc: 0.7409 - val_loss: 0.5315 - val_acc: 0.6063 - val_auc: 0.7410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1786cd5be48>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight = { 0: 8.4, 1: 1}\n",
    "model.fit(X_train, y_train, batch_size= 32, epochs= 200, verbose=1, validation_data=(X_vali, y_vali), class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are happy with your model training, run the below cell to generate the final labels for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_testing_data\n",
    "test_data, _ = get_testing_data(data_root)\n",
    "test_data = preprocess_dataset(test_data)\n",
    "predicted = model.predict_classes(test_data)\n",
    "with open(\"exercise_1_output.txt\", \"w\") as f:\n",
    "    [f.write(\"{}\\n\".format(p)) for p in predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7053571428571429"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy2 = accuracy_score(pd.get_dummies(_).iloc[:,1], predicted)\n",
    "accuracy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Exercise 4:\n",
    "(OPTIONAL)\n",
    "\n",
    "Do you find any relationship between Age and whether a person has Diabetes or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24729294365805007, 2.5529994557745299e-23)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "pearsonr(df['Age'], y[1])\n",
    "\n",
    "## The pearsonr correlation between age and Diabetes is possitive(0.24729294365805007)\n",
    "## and it is statistically significant, with p-value 2.5529994557745299e-23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = get_training_data(data_root)\n",
    "df2 = clean_data_and_labels(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEaCAYAAAAL7cBuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFAsEERFJsUAQEZEUCwQREUn9fyJnbqagvTq0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17861c182b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "plt.scatter(df2['Age'],df2['Diabetes'],alpha=0.5)\n",
    "plt.title('Diabetes vs. Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Diabetes ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Exercise 5:\n",
    "(OPTIONAL)\n",
    "\n",
    "Predict whether the person sleeps for less than mean sleeping hours across the dataset or more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from sklearn import preprocessing\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Dropout, Activation, Input, BatchNormalization\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = df2.drop(\"SleepHours\", axis=1)\n",
    "cols = X_s.columns.tolist()\n",
    "cols.remove('Diabetes')\n",
    "cols.remove('Race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_s[cols])\n",
    "X_s[cols]  = scaler.transform(X_s[cols])\n",
    "X_s = pd.get_dummies(X_s, prefix = ['Diabetes', 'Race'], columns = ['Diabetes', 'Race'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_s = pd.get_dummies(df2['SleepHours'] < df2['SleepHours'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s= build_model(X_s.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 769 samples, validate on 330 samples\n",
      "Epoch 1/200\n",
      "769/769 [==============================] - 1s 1ms/step - loss: 1.5075 - acc: 0.5228 - auc: 0.4733 - val_loss: 0.8826 - val_acc: 0.4818 - val_auc: 0.5092\n",
      "Epoch 2/200\n",
      "769/769 [==============================] - 0s 122us/step - loss: 1.2792 - acc: 0.5254 - auc: 0.5037 - val_loss: 0.8072 - val_acc: 0.5182 - val_auc: 0.5132\n",
      "Epoch 3/200\n",
      "769/769 [==============================] - 0s 122us/step - loss: 1.1291 - acc: 0.5449 - auc: 0.5172 - val_loss: 0.7592 - val_acc: 0.5303 - val_auc: 0.5219\n",
      "Epoch 4/200\n",
      "769/769 [==============================] - 0s 112us/step - loss: 0.9396 - acc: 0.5436 - auc: 0.5275 - val_loss: 0.7379 - val_acc: 0.5727 - val_auc: 0.5315\n",
      "Epoch 5/200\n",
      "769/769 [==============================] - 0s 117us/step - loss: 0.9111 - acc: 0.5397 - auc: 0.5348 - val_loss: 0.7247 - val_acc: 0.5667 - val_auc: 0.5377\n",
      "Epoch 6/200\n",
      "769/769 [==============================] - 0s 114us/step - loss: 0.8892 - acc: 0.5449 - auc: 0.5377 - val_loss: 0.7160 - val_acc: 0.5697 - val_auc: 0.5387\n",
      "Epoch 7/200\n",
      "769/769 [==============================] - 0s 131us/step - loss: 0.7886 - acc: 0.6047 - auc: 0.5433 - val_loss: 0.7055 - val_acc: 0.5879 - val_auc: 0.5473\n",
      "Epoch 8/200\n",
      "769/769 [==============================] - 0s 127us/step - loss: 0.7892 - acc: 0.5592 - auc: 0.5506 - val_loss: 0.6997 - val_acc: 0.5909 - val_auc: 0.5511\n",
      "Epoch 9/200\n",
      "769/769 [==============================] - 0s 134us/step - loss: 0.7676 - acc: 0.5670 - auc: 0.5520 - val_loss: 0.6939 - val_acc: 0.5879 - val_auc: 0.5531\n",
      "Epoch 10/200\n",
      "769/769 [==============================] - 0s 134us/step - loss: 0.7332 - acc: 0.5722 - auc: 0.5549 - val_loss: 0.6910 - val_acc: 0.5939 - val_auc: 0.5561\n",
      "Epoch 11/200\n",
      "769/769 [==============================] - 0s 131us/step - loss: 0.7200 - acc: 0.6021 - auc: 0.5581 - val_loss: 0.6883 - val_acc: 0.5970 - val_auc: 0.5601\n",
      "Epoch 12/200\n",
      "769/769 [==============================] - 0s 139us/step - loss: 0.7064 - acc: 0.5904 - auc: 0.5620 - val_loss: 0.6857 - val_acc: 0.6000 - val_auc: 0.5635\n",
      "Epoch 13/200\n",
      "769/769 [==============================] - 0s 137us/step - loss: 0.7149 - acc: 0.5761 - auc: 0.5641 - val_loss: 0.6839 - val_acc: 0.5970 - val_auc: 0.5653\n",
      "Epoch 14/200\n",
      "769/769 [==============================] - 0s 141us/step - loss: 0.6920 - acc: 0.6164 - auc: 0.5672 - val_loss: 0.6829 - val_acc: 0.5970 - val_auc: 0.5678\n",
      "Epoch 15/200\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.6942 - acc: 0.6021 - auc: 0.5689 - val_loss: 0.6817 - val_acc: 0.5970 - val_auc: 0.5697\n",
      "Epoch 16/200\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.6793 - acc: 0.6177 - auc: 0.5713 - val_loss: 0.6810 - val_acc: 0.5970 - val_auc: 0.5720\n",
      "Epoch 17/200\n",
      "769/769 [==============================] - 0s 128us/step - loss: 0.6768 - acc: 0.5969 - auc: 0.5734 - val_loss: 0.6794 - val_acc: 0.5970 - val_auc: 0.5740\n",
      "Epoch 18/200\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.6640 - acc: 0.6190 - auc: 0.5755 - val_loss: 0.6791 - val_acc: 0.5970 - val_auc: 0.5766\n",
      "Epoch 19/200\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.6643 - acc: 0.6281 - auc: 0.5777 - val_loss: 0.6785 - val_acc: 0.5939 - val_auc: 0.5790\n",
      "Epoch 20/200\n",
      "769/769 [==============================] - 0s 121us/step - loss: 0.6620 - acc: 0.6333 - auc: 0.5803 - val_loss: 0.6781 - val_acc: 0.5939 - val_auc: 0.5814\n",
      "Epoch 21/200\n",
      "769/769 [==============================] - 0s 127us/step - loss: 0.6764 - acc: 0.6086 - auc: 0.5820 - val_loss: 0.6778 - val_acc: 0.5939 - val_auc: 0.5825\n",
      "Epoch 22/200\n",
      "769/769 [==============================] - 0s 128us/step - loss: 0.6649 - acc: 0.6177 - auc: 0.5834 - val_loss: 0.6776 - val_acc: 0.5939 - val_auc: 0.5841\n",
      "Epoch 23/200\n",
      "769/769 [==============================] - 0s 150us/step - loss: 0.6602 - acc: 0.6229 - auc: 0.5851 - val_loss: 0.6770 - val_acc: 0.5939 - val_auc: 0.5859\n",
      "Epoch 24/200\n",
      "769/769 [==============================] - 0s 144us/step - loss: 0.6571 - acc: 0.6307 - auc: 0.5868 - val_loss: 0.6767 - val_acc: 0.5939 - val_auc: 0.5878\n",
      "Epoch 25/200\n",
      "769/769 [==============================] - 0s 136us/step - loss: 0.6714 - acc: 0.6164 - auc: 0.5882 - val_loss: 0.6769 - val_acc: 0.5939 - val_auc: 0.5887\n",
      "Epoch 26/200\n",
      "769/769 [==============================] - 0s 130us/step - loss: 0.6676 - acc: 0.6307 - auc: 0.5895 - val_loss: 0.6771 - val_acc: 0.5970 - val_auc: 0.5898\n",
      "Epoch 27/200\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.6586 - acc: 0.6346 - auc: 0.5905 - val_loss: 0.6772 - val_acc: 0.5939 - val_auc: 0.5912\n",
      "Epoch 28/200\n",
      "769/769 [==============================] - 0s 137us/step - loss: 0.6599 - acc: 0.6346 - auc: 0.5920 - val_loss: 0.6765 - val_acc: 0.5939 - val_auc: 0.5925\n",
      "Epoch 29/200\n",
      "769/769 [==============================] - 0s 140us/step - loss: 0.6556 - acc: 0.6320 - auc: 0.5934 - val_loss: 0.6765 - val_acc: 0.5939 - val_auc: 0.5938\n",
      "Epoch 30/200\n",
      "769/769 [==============================] - 0s 127us/step - loss: 0.6645 - acc: 0.6281 - auc: 0.5943 - val_loss: 0.6767 - val_acc: 0.5939 - val_auc: 0.5947\n",
      "Epoch 31/200\n",
      "769/769 [==============================] - 0s 113us/step - loss: 0.6546 - acc: 0.6333 - auc: 0.5953 - val_loss: 0.6762 - val_acc: 0.5939 - val_auc: 0.5960\n",
      "Epoch 32/200\n",
      "769/769 [==============================] - 0s 115us/step - loss: 0.6599 - acc: 0.6164 - auc: 0.5966 - val_loss: 0.6766 - val_acc: 0.5970 - val_auc: 0.5970\n",
      "Epoch 33/200\n",
      "769/769 [==============================] - 0s 115us/step - loss: 0.6612 - acc: 0.6307 - auc: 0.5975 - val_loss: 0.6766 - val_acc: 0.5939 - val_auc: 0.5978\n",
      "Epoch 34/200\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.6545 - acc: 0.6294 - auc: 0.5983 - val_loss: 0.6762 - val_acc: 0.5939 - val_auc: 0.5989\n",
      "Epoch 35/200\n",
      "769/769 [==============================] - 0s 130us/step - loss: 0.6555 - acc: 0.6346 - auc: 0.5995 - val_loss: 0.6758 - val_acc: 0.5909 - val_auc: 0.5999\n",
      "Epoch 36/200\n",
      "769/769 [==============================] - 0s 139us/step - loss: 0.6693 - acc: 0.6177 - auc: 0.6003 - val_loss: 0.6759 - val_acc: 0.5970 - val_auc: 0.6003\n",
      "Epoch 37/200\n",
      "769/769 [==============================] - 0s 136us/step - loss: 0.6602 - acc: 0.6385 - auc: 0.6007 - val_loss: 0.6752 - val_acc: 0.5939 - val_auc: 0.6009\n",
      "Epoch 38/200\n",
      "769/769 [==============================] - 0s 143us/step - loss: 0.6633 - acc: 0.6307 - auc: 0.6013 - val_loss: 0.6745 - val_acc: 0.5909 - val_auc: 0.6015\n",
      "Epoch 39/200\n",
      "769/769 [==============================] - 0s 127us/step - loss: 0.6646 - acc: 0.6294 - auc: 0.6017 - val_loss: 0.6747 - val_acc: 0.5970 - val_auc: 0.6019\n",
      "Epoch 40/200\n",
      "769/769 [==============================] - 0s 130us/step - loss: 0.6566 - acc: 0.6307 - auc: 0.6024 - val_loss: 0.6752 - val_acc: 0.5970 - val_auc: 0.6026\n",
      "Epoch 41/200\n",
      "769/769 [==============================] - 0s 130us/step - loss: 0.6517 - acc: 0.6359 - auc: 0.6032 - val_loss: 0.6762 - val_acc: 0.6000 - val_auc: 0.6034\n",
      "Epoch 42/200\n",
      "769/769 [==============================] - 0s 134us/step - loss: 0.6514 - acc: 0.6359 - auc: 0.6040 - val_loss: 0.6770 - val_acc: 0.6000 - val_auc: 0.6042\n",
      "Epoch 43/200\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.6594 - acc: 0.6333 - auc: 0.6045 - val_loss: 0.6777 - val_acc: 0.5970 - val_auc: 0.6047\n",
      "Epoch 44/200\n",
      "769/769 [==============================] - 0s 127us/step - loss: 0.6553 - acc: 0.6268 - auc: 0.6051 - val_loss: 0.6777 - val_acc: 0.5970 - val_auc: 0.6053\n",
      "Epoch 45/200\n",
      "769/769 [==============================] - 0s 114us/step - loss: 0.6621 - acc: 0.6294 - auc: 0.6057 - val_loss: 0.6778 - val_acc: 0.6000 - val_auc: 0.6057\n",
      "Epoch 46/200\n",
      "769/769 [==============================] - 0s 136us/step - loss: 0.6563 - acc: 0.6320 - auc: 0.6058 - val_loss: 0.6775 - val_acc: 0.5970 - val_auc: 0.6061\n",
      "Epoch 47/200\n",
      "769/769 [==============================] - 0s 130us/step - loss: 0.6577 - acc: 0.6320 - auc: 0.6065 - val_loss: 0.6775 - val_acc: 0.6000 - val_auc: 0.6066\n",
      "Epoch 48/200\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.6603 - acc: 0.6268 - auc: 0.6067 - val_loss: 0.6764 - val_acc: 0.6000 - val_auc: 0.6069\n",
      "Epoch 49/200\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.6609 - acc: 0.6294 - auc: 0.6072 - val_loss: 0.6755 - val_acc: 0.6000 - val_auc: 0.6072\n",
      "Epoch 50/200\n",
      "769/769 [==============================] - 0s 118us/step - loss: 0.6540 - acc: 0.6346 - auc: 0.6075 - val_loss: 0.6754 - val_acc: 0.5970 - val_auc: 0.6077\n",
      "Epoch 51/200\n",
      "769/769 [==============================] - 0s 121us/step - loss: 0.6572 - acc: 0.6385 - auc: 0.6079 - val_loss: 0.6751 - val_acc: 0.6000 - val_auc: 0.6081\n",
      "Epoch 52/200\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.6546 - acc: 0.6372 - auc: 0.6085 - val_loss: 0.6750 - val_acc: 0.6000 - val_auc: 0.6086\n",
      "Epoch 53/200\n",
      "769/769 [==============================] - 0s 113us/step - loss: 0.6546 - acc: 0.6372 - auc: 0.6089 - val_loss: 0.6742 - val_acc: 0.6000 - val_auc: 0.6092\n",
      "Epoch 54/200\n",
      "769/769 [==============================] - 0s 106us/step - loss: 0.6529 - acc: 0.6359 - auc: 0.6096 - val_loss: 0.6747 - val_acc: 0.6000 - val_auc: 0.6097\n",
      "Epoch 55/200\n",
      "769/769 [==============================] - 0s 117us/step - loss: 0.6550 - acc: 0.6307 - auc: 0.6100 - val_loss: 0.6747 - val_acc: 0.6000 - val_auc: 0.6102\n",
      "Epoch 56/200\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.6534 - acc: 0.6346 - auc: 0.6106 - val_loss: 0.6741 - val_acc: 0.5970 - val_auc: 0.6108\n",
      "Epoch 57/200\n",
      "769/769 [==============================] - 0s 114us/step - loss: 0.6554 - acc: 0.6372 - auc: 0.6111 - val_loss: 0.6744 - val_acc: 0.5970 - val_auc: 0.6112\n",
      "Epoch 58/200\n",
      "769/769 [==============================] - 0s 121us/step - loss: 0.6551 - acc: 0.6398 - auc: 0.6115 - val_loss: 0.6747 - val_acc: 0.5970 - val_auc: 0.6116\n",
      "Epoch 59/200\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.6609 - acc: 0.6268 - auc: 0.6117 - val_loss: 0.6749 - val_acc: 0.5970 - val_auc: 0.6118\n",
      "Epoch 60/200\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.6558 - acc: 0.6333 - auc: 0.6120 - val_loss: 0.6746 - val_acc: 0.5970 - val_auc: 0.6121\n",
      "Epoch 61/200\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.6529 - acc: 0.6398 - auc: 0.6124 - val_loss: 0.6739 - val_acc: 0.5970 - val_auc: 0.6125\n",
      "Epoch 62/200\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.6554 - acc: 0.6450 - auc: 0.6127 - val_loss: 0.6734 - val_acc: 0.6030 - val_auc: 0.6128\n",
      "Epoch 63/200\n",
      "769/769 [==============================] - 0s 118us/step - loss: 0.6504 - acc: 0.6359 - auc: 0.6130 - val_loss: 0.6737 - val_acc: 0.6030 - val_auc: 0.6133\n",
      "Epoch 64/200\n",
      "769/769 [==============================] - 0s 115us/step - loss: 0.6523 - acc: 0.6385 - auc: 0.6135 - val_loss: 0.6736 - val_acc: 0.6030 - val_auc: 0.6137\n",
      "Epoch 65/200\n",
      "769/769 [==============================] - 0s 110us/step - loss: 0.6559 - acc: 0.6294 - auc: 0.6139 - val_loss: 0.6746 - val_acc: 0.6030 - val_auc: 0.6140\n",
      "Epoch 66/200\n",
      "769/769 [==============================] - 0s 114us/step - loss: 0.6564 - acc: 0.6333 - auc: 0.6142 - val_loss: 0.6752 - val_acc: 0.5970 - val_auc: 0.6143\n",
      "Epoch 67/200\n",
      "769/769 [==============================] - 0s 118us/step - loss: 0.6554 - acc: 0.6385 - auc: 0.6145 - val_loss: 0.6757 - val_acc: 0.5970 - val_auc: 0.6146\n",
      "Epoch 68/200\n",
      "769/769 [==============================] - 0s 127us/step - loss: 0.6597 - acc: 0.6268 - auc: 0.6148 - val_loss: 0.6759 - val_acc: 0.5970 - val_auc: 0.6148\n",
      "Epoch 69/200\n",
      "769/769 [==============================] - 0s 128us/step - loss: 0.6536 - acc: 0.6294 - auc: 0.6152 - val_loss: 0.6769 - val_acc: 0.5970 - val_auc: 0.6152\n",
      "Epoch 70/200\n",
      "769/769 [==============================] - 0s 143us/step - loss: 0.6569 - acc: 0.6281 - auc: 0.6154 - val_loss: 0.6768 - val_acc: 0.5970 - val_auc: 0.6154\n",
      "Epoch 71/200\n",
      "769/769 [==============================] - 0s 134us/step - loss: 0.6496 - acc: 0.6424 - auc: 0.6155 - val_loss: 0.6756 - val_acc: 0.5970 - val_auc: 0.6158\n",
      "Epoch 72/200\n",
      "769/769 [==============================] - 0s 121us/step - loss: 0.6544 - acc: 0.6359 - auc: 0.6159 - val_loss: 0.6764 - val_acc: 0.5970 - val_auc: 0.6160\n",
      "Epoch 73/200\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.6527 - acc: 0.6398 - auc: 0.6164 - val_loss: 0.6764 - val_acc: 0.5970 - val_auc: 0.6164\n",
      "Epoch 74/200\n",
      "769/769 [==============================] - 0s 121us/step - loss: 0.6585 - acc: 0.6281 - auc: 0.6165 - val_loss: 0.6756 - val_acc: 0.5970 - val_auc: 0.6165\n",
      "Epoch 75/200\n",
      "769/769 [==============================] - 0s 128us/step - loss: 0.6559 - acc: 0.6320 - auc: 0.6166 - val_loss: 0.6752 - val_acc: 0.5970 - val_auc: 0.6167\n",
      "Epoch 76/200\n",
      "769/769 [==============================] - 0s 117us/step - loss: 0.6557 - acc: 0.6320 - auc: 0.6169 - val_loss: 0.6752 - val_acc: 0.5970 - val_auc: 0.6170\n",
      "Epoch 77/200\n",
      "769/769 [==============================] - 0s 112us/step - loss: 0.6566 - acc: 0.6294 - auc: 0.6172 - val_loss: 0.6752 - val_acc: 0.5970 - val_auc: 0.6172\n",
      "Epoch 78/200\n",
      "769/769 [==============================] - 0s 110us/step - loss: 0.6542 - acc: 0.6359 - auc: 0.6173 - val_loss: 0.6744 - val_acc: 0.5970 - val_auc: 0.6174\n",
      "Epoch 79/200\n",
      "769/769 [==============================] - 0s 113us/step - loss: 0.6505 - acc: 0.6385 - auc: 0.6177 - val_loss: 0.6750 - val_acc: 0.5970 - val_auc: 0.6177\n",
      "Epoch 80/200\n",
      "769/769 [==============================] - 0s 114us/step - loss: 0.6507 - acc: 0.6333 - auc: 0.6180 - val_loss: 0.6751 - val_acc: 0.6030 - val_auc: 0.6180\n",
      "Epoch 81/200\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.6566 - acc: 0.6229 - auc: 0.6182 - val_loss: 0.6749 - val_acc: 0.6030 - val_auc: 0.6183\n",
      "Epoch 82/200\n",
      "769/769 [==============================] - 0s 117us/step - loss: 0.6515 - acc: 0.6333 - auc: 0.6185 - val_loss: 0.6748 - val_acc: 0.6030 - val_auc: 0.6187\n",
      "Epoch 83/200\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.6539 - acc: 0.6346 - auc: 0.6188 - val_loss: 0.6745 - val_acc: 0.6030 - val_auc: 0.6189\n",
      "Epoch 84/200\n",
      "769/769 [==============================] - 0s 118us/step - loss: 0.6596 - acc: 0.6281 - auc: 0.6191 - val_loss: 0.6749 - val_acc: 0.6000 - val_auc: 0.6190\n",
      "Epoch 85/200\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.6487 - acc: 0.6450 - auc: 0.6192 - val_loss: 0.6757 - val_acc: 0.6000 - val_auc: 0.6193\n",
      "Epoch 86/200\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.6505 - acc: 0.6372 - auc: 0.6194 - val_loss: 0.6761 - val_acc: 0.5970 - val_auc: 0.6196\n",
      "Epoch 87/200\n",
      "769/769 [==============================] - 0s 118us/step - loss: 0.6540 - acc: 0.6359 - auc: 0.6199 - val_loss: 0.6757 - val_acc: 0.5970 - val_auc: 0.6199\n",
      "Epoch 88/200\n",
      "769/769 [==============================] - 0s 119us/step - loss: 0.6574 - acc: 0.6333 - auc: 0.6201 - val_loss: 0.6747 - val_acc: 0.6000 - val_auc: 0.6200\n",
      "Epoch 89/200\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.6509 - acc: 0.6372 - auc: 0.6202 - val_loss: 0.6746 - val_acc: 0.6000 - val_auc: 0.6203\n",
      "Epoch 90/200\n",
      "769/769 [==============================] - 0s 119us/step - loss: 0.6579 - acc: 0.6320 - auc: 0.6205 - val_loss: 0.6751 - val_acc: 0.6000 - val_auc: 0.6204\n",
      "Epoch 91/200\n",
      "769/769 [==============================] - 0s 121us/step - loss: 0.6552 - acc: 0.6320 - auc: 0.6205 - val_loss: 0.6754 - val_acc: 0.5970 - val_auc: 0.6206\n",
      "Epoch 92/200\n",
      "769/769 [==============================] - 0s 118us/step - loss: 0.6571 - acc: 0.6372 - auc: 0.6207 - val_loss: 0.6758 - val_acc: 0.5970 - val_auc: 0.6207\n",
      "Epoch 93/200\n",
      "769/769 [==============================] - 0s 117us/step - loss: 0.6539 - acc: 0.6307 - auc: 0.6209 - val_loss: 0.6757 - val_acc: 0.5970 - val_auc: 0.6209\n",
      "Epoch 94/200\n",
      "769/769 [==============================] - 0s 112us/step - loss: 0.6543 - acc: 0.6307 - auc: 0.6211 - val_loss: 0.6763 - val_acc: 0.5970 - val_auc: 0.6211\n",
      "Epoch 95/200\n",
      "769/769 [==============================] - 0s 115us/step - loss: 0.6541 - acc: 0.6411 - auc: 0.6212 - val_loss: 0.6769 - val_acc: 0.5970 - val_auc: 0.6212\n",
      "Epoch 96/200\n",
      "769/769 [==============================] - 0s 119us/step - loss: 0.6584 - acc: 0.6333 - auc: 0.6213 - val_loss: 0.6770 - val_acc: 0.5970 - val_auc: 0.6213\n",
      "Epoch 97/200\n",
      "769/769 [==============================] - 0s 131us/step - loss: 0.6510 - acc: 0.6307 - auc: 0.6214 - val_loss: 0.6776 - val_acc: 0.5970 - val_auc: 0.6215\n",
      "Epoch 98/200\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.6526 - acc: 0.6346 - auc: 0.6216 - val_loss: 0.6772 - val_acc: 0.6000 - val_auc: 0.6217\n",
      "Epoch 99/200\n",
      "769/769 [==============================] - 0s 121us/step - loss: 0.6507 - acc: 0.6359 - auc: 0.6219 - val_loss: 0.6774 - val_acc: 0.5970 - val_auc: 0.6220\n",
      "Epoch 100/200\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.6501 - acc: 0.6398 - auc: 0.6221 - val_loss: 0.6773 - val_acc: 0.5970 - val_auc: 0.6222\n",
      "Epoch 101/200\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.6574 - acc: 0.6242 - auc: 0.6223 - val_loss: 0.6779 - val_acc: 0.5970 - val_auc: 0.6223\n",
      "Epoch 102/200\n",
      "769/769 [==============================] - 0s 131us/step - loss: 0.6541 - acc: 0.6333 - auc: 0.6224 - val_loss: 0.6770 - val_acc: 0.5970 - val_auc: 0.6225\n",
      "Epoch 103/200\n",
      "769/769 [==============================] - 0s 127us/step - loss: 0.6526 - acc: 0.6320 - auc: 0.6226 - val_loss: 0.6758 - val_acc: 0.5970 - val_auc: 0.6226\n",
      "Epoch 104/200\n",
      "769/769 [==============================] - 0s 134us/step - loss: 0.6500 - acc: 0.6385 - auc: 0.6227 - val_loss: 0.6757 - val_acc: 0.5970 - val_auc: 0.6228\n",
      "Epoch 105/200\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.6531 - acc: 0.6346 - auc: 0.6229 - val_loss: 0.6761 - val_acc: 0.5970 - val_auc: 0.6230\n",
      "Epoch 106/200\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.6609 - acc: 0.6255 - auc: 0.6231 - val_loss: 0.6758 - val_acc: 0.5970 - val_auc: 0.6230\n",
      "Epoch 107/200\n",
      "769/769 [==============================] - 0s 109us/step - loss: 0.6540 - acc: 0.6385 - auc: 0.6231 - val_loss: 0.6752 - val_acc: 0.5970 - val_auc: 0.6231\n",
      "Epoch 108/200\n",
      "769/769 [==============================] - 0s 108us/step - loss: 0.6564 - acc: 0.6294 - auc: 0.6233 - val_loss: 0.6751 - val_acc: 0.6000 - val_auc: 0.6233\n",
      "Epoch 109/200\n",
      "769/769 [==============================] - 0s 110us/step - loss: 0.6589 - acc: 0.6281 - auc: 0.6233 - val_loss: 0.6755 - val_acc: 0.6000 - val_auc: 0.6233\n",
      "Epoch 110/200\n",
      "769/769 [==============================] - 0s 119us/step - loss: 0.6576 - acc: 0.6320 - auc: 0.6233 - val_loss: 0.6757 - val_acc: 0.6000 - val_auc: 0.6233\n",
      "Epoch 111/200\n",
      "769/769 [==============================] - 0s 112us/step - loss: 0.6518 - acc: 0.6359 - auc: 0.6235 - val_loss: 0.6758 - val_acc: 0.6000 - val_auc: 0.6235\n",
      "Epoch 112/200\n",
      "769/769 [==============================] - 0s 112us/step - loss: 0.6504 - acc: 0.6359 - auc: 0.6237 - val_loss: 0.6751 - val_acc: 0.5970 - val_auc: 0.6237\n",
      "Epoch 113/200\n",
      "769/769 [==============================] - 0s 109us/step - loss: 0.6510 - acc: 0.6359 - auc: 0.6238 - val_loss: 0.6753 - val_acc: 0.5970 - val_auc: 0.6239\n",
      "Epoch 114/200\n",
      "769/769 [==============================] - 0s 112us/step - loss: 0.6512 - acc: 0.6372 - auc: 0.6241 - val_loss: 0.6749 - val_acc: 0.5970 - val_auc: 0.6240\n",
      "Epoch 115/200\n",
      "769/769 [==============================] - 0s 106us/step - loss: 0.6468 - acc: 0.6450 - auc: 0.6242 - val_loss: 0.6747 - val_acc: 0.5970 - val_auc: 0.6243\n",
      "Epoch 116/200\n",
      "769/769 [==============================] - 0s 105us/step - loss: 0.6533 - acc: 0.6372 - auc: 0.6244 - val_loss: 0.6755 - val_acc: 0.5970 - val_auc: 0.6244\n",
      "Epoch 117/200\n",
      "769/769 [==============================] - 0s 106us/step - loss: 0.6516 - acc: 0.6346 - auc: 0.6245 - val_loss: 0.6758 - val_acc: 0.5970 - val_auc: 0.6246\n",
      "Epoch 118/200\n",
      "769/769 [==============================] - 0s 148us/step - loss: 0.6544 - acc: 0.6333 - auc: 0.6247 - val_loss: 0.6753 - val_acc: 0.5970 - val_auc: 0.6247\n",
      "Epoch 119/200\n",
      "769/769 [==============================] - 0s 105us/step - loss: 0.6469 - acc: 0.6450 - auc: 0.6248 - val_loss: 0.6752 - val_acc: 0.5970 - val_auc: 0.6248\n",
      "Epoch 120/200\n",
      "769/769 [==============================] - 0s 117us/step - loss: 0.6516 - acc: 0.6346 - auc: 0.6250 - val_loss: 0.6754 - val_acc: 0.6000 - val_auc: 0.6250\n",
      "Epoch 121/200\n",
      "769/769 [==============================] - 0s 113us/step - loss: 0.6564 - acc: 0.6281 - auc: 0.6250 - val_loss: 0.6758 - val_acc: 0.5970 - val_auc: 0.6251\n",
      "Epoch 122/200\n",
      "769/769 [==============================] - 0s 105us/step - loss: 0.6574 - acc: 0.6281 - auc: 0.6252 - val_loss: 0.6760 - val_acc: 0.5970 - val_auc: 0.6252\n",
      "Epoch 123/200\n",
      "769/769 [==============================] - 0s 109us/step - loss: 0.6602 - acc: 0.6333 - auc: 0.6253 - val_loss: 0.6753 - val_acc: 0.5970 - val_auc: 0.6252\n",
      "Epoch 124/200\n",
      "769/769 [==============================] - 0s 128us/step - loss: 0.6557 - acc: 0.6255 - auc: 0.6253 - val_loss: 0.6748 - val_acc: 0.5970 - val_auc: 0.6253\n",
      "Epoch 125/200\n",
      "769/769 [==============================] - 0s 114us/step - loss: 0.6493 - acc: 0.6294 - auc: 0.6255 - val_loss: 0.6749 - val_acc: 0.5970 - val_auc: 0.6255\n",
      "Epoch 126/200\n",
      "769/769 [==============================] - 0s 112us/step - loss: 0.6590 - acc: 0.6216 - auc: 0.6256 - val_loss: 0.6742 - val_acc: 0.5970 - val_auc: 0.6256\n",
      "Epoch 127/200\n",
      "769/769 [==============================] - 0s 114us/step - loss: 0.6499 - acc: 0.6385 - auc: 0.6258 - val_loss: 0.6738 - val_acc: 0.5970 - val_auc: 0.6258\n",
      "Epoch 128/200\n",
      "769/769 [==============================] - 0s 106us/step - loss: 0.6508 - acc: 0.6502 - auc: 0.6259 - val_loss: 0.6735 - val_acc: 0.6000 - val_auc: 0.6259\n",
      "Epoch 129/200\n",
      "769/769 [==============================] - 0s 108us/step - loss: 0.6556 - acc: 0.6385 - auc: 0.6260 - val_loss: 0.6736 - val_acc: 0.6000 - val_auc: 0.6260\n",
      "Epoch 130/200\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.6540 - acc: 0.6307 - auc: 0.6261 - val_loss: 0.6731 - val_acc: 0.6000 - val_auc: 0.6261\n",
      "Epoch 131/200\n",
      "769/769 [==============================] - 0s 130us/step - loss: 0.6574 - acc: 0.6242 - auc: 0.6261 - val_loss: 0.6723 - val_acc: 0.6030 - val_auc: 0.6261\n",
      "Epoch 132/200\n",
      "769/769 [==============================] - 0s 127us/step - loss: 0.6497 - acc: 0.6424 - auc: 0.6262 - val_loss: 0.6724 - val_acc: 0.6030 - val_auc: 0.6263\n",
      "Epoch 133/200\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.6526 - acc: 0.6294 - auc: 0.6264 - val_loss: 0.6738 - val_acc: 0.6000 - val_auc: 0.6264\n",
      "Epoch 134/200\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.6536 - acc: 0.6320 - auc: 0.6265 - val_loss: 0.6745 - val_acc: 0.5970 - val_auc: 0.6265\n",
      "Epoch 135/200\n",
      "769/769 [==============================] - 0s 136us/step - loss: 0.6517 - acc: 0.6476 - auc: 0.6267 - val_loss: 0.6756 - val_acc: 0.5970 - val_auc: 0.6266\n",
      "Epoch 136/200\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.6532 - acc: 0.6411 - auc: 0.6267 - val_loss: 0.6761 - val_acc: 0.5970 - val_auc: 0.6267\n",
      "Epoch 137/200\n",
      "769/769 [==============================] - 0s 119us/step - loss: 0.6523 - acc: 0.6359 - auc: 0.6268 - val_loss: 0.6760 - val_acc: 0.5970 - val_auc: 0.6269\n",
      "Epoch 138/200\n",
      "769/769 [==============================] - 0s 115us/step - loss: 0.6471 - acc: 0.6437 - auc: 0.6270 - val_loss: 0.6761 - val_acc: 0.5970 - val_auc: 0.6271\n",
      "Epoch 139/200\n",
      "769/769 [==============================] - 0s 113us/step - loss: 0.6479 - acc: 0.6398 - auc: 0.6272 - val_loss: 0.6767 - val_acc: 0.5970 - val_auc: 0.6272\n",
      "Epoch 140/200\n",
      "769/769 [==============================] - 0s 112us/step - loss: 0.6532 - acc: 0.6359 - auc: 0.6273 - val_loss: 0.6770 - val_acc: 0.5970 - val_auc: 0.6273\n",
      "Epoch 141/200\n",
      "769/769 [==============================] - 0s 117us/step - loss: 0.6520 - acc: 0.6372 - auc: 0.6274 - val_loss: 0.6767 - val_acc: 0.6000 - val_auc: 0.6274\n",
      "Epoch 142/200\n",
      "769/769 [==============================] - 0s 118us/step - loss: 0.6587 - acc: 0.6424 - auc: 0.6274 - val_loss: 0.6763 - val_acc: 0.5970 - val_auc: 0.6274\n",
      "Epoch 143/200\n",
      "769/769 [==============================] - 0s 112us/step - loss: 0.6520 - acc: 0.6359 - auc: 0.6275 - val_loss: 0.6763 - val_acc: 0.5970 - val_auc: 0.6275\n",
      "Epoch 144/200\n",
      "769/769 [==============================] - 0s 102us/step - loss: 0.6591 - acc: 0.6346 - auc: 0.6276 - val_loss: 0.6764 - val_acc: 0.5970 - val_auc: 0.6276\n",
      "Epoch 145/200\n",
      "769/769 [==============================] - 0s 105us/step - loss: 0.6533 - acc: 0.6346 - auc: 0.6276 - val_loss: 0.6767 - val_acc: 0.5970 - val_auc: 0.6277\n",
      "Epoch 146/200\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.6524 - acc: 0.6411 - auc: 0.6278 - val_loss: 0.6758 - val_acc: 0.5970 - val_auc: 0.6278\n",
      "Epoch 147/200\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.6535 - acc: 0.6294 - auc: 0.6278 - val_loss: 0.6747 - val_acc: 0.5970 - val_auc: 0.6279\n",
      "Epoch 148/200\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.6480 - acc: 0.6372 - auc: 0.6279 - val_loss: 0.6744 - val_acc: 0.5970 - val_auc: 0.6280\n",
      "Epoch 149/200\n",
      "769/769 [==============================] - 0s 113us/step - loss: 0.6514 - acc: 0.6359 - auc: 0.6281 - val_loss: 0.6748 - val_acc: 0.5970 - val_auc: 0.6281\n",
      "Epoch 150/200\n",
      "769/769 [==============================] - 0s 121us/step - loss: 0.6541 - acc: 0.6372 - auc: 0.6282 - val_loss: 0.6750 - val_acc: 0.5970 - val_auc: 0.6282\n",
      "Epoch 151/200\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.6497 - acc: 0.6333 - auc: 0.6283 - val_loss: 0.6751 - val_acc: 0.5970 - val_auc: 0.6283\n",
      "Epoch 152/200\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.6516 - acc: 0.6294 - auc: 0.6284 - val_loss: 0.6749 - val_acc: 0.5970 - val_auc: 0.6284\n",
      "Epoch 153/200\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.6513 - acc: 0.6346 - auc: 0.6285 - val_loss: 0.6747 - val_acc: 0.5970 - val_auc: 0.6286\n",
      "Epoch 154/200\n",
      "769/769 [==============================] - 0s 130us/step - loss: 0.6543 - acc: 0.6411 - auc: 0.6287 - val_loss: 0.6748 - val_acc: 0.5970 - val_auc: 0.6286\n",
      "Epoch 155/200\n",
      "769/769 [==============================] - 0s 109us/step - loss: 0.6487 - acc: 0.6307 - auc: 0.6288 - val_loss: 0.6745 - val_acc: 0.5970 - val_auc: 0.6288\n",
      "Epoch 156/200\n",
      "769/769 [==============================] - 0s 119us/step - loss: 0.6488 - acc: 0.6385 - auc: 0.6289 - val_loss: 0.6738 - val_acc: 0.6000 - val_auc: 0.6289\n",
      "Epoch 157/200\n",
      "769/769 [==============================] - 0s 110us/step - loss: 0.6523 - acc: 0.6333 - auc: 0.6290 - val_loss: 0.6742 - val_acc: 0.6000 - val_auc: 0.6291\n",
      "Epoch 158/200\n",
      "769/769 [==============================] - 0s 108us/step - loss: 0.6537 - acc: 0.6333 - auc: 0.6291 - val_loss: 0.6742 - val_acc: 0.6000 - val_auc: 0.6291\n",
      "Epoch 159/200\n",
      "769/769 [==============================] - 0s 114us/step - loss: 0.6512 - acc: 0.6333 - auc: 0.6292 - val_loss: 0.6741 - val_acc: 0.6000 - val_auc: 0.6292\n",
      "Epoch 160/200\n",
      "769/769 [==============================] - 0s 139us/step - loss: 0.6601 - acc: 0.6359 - auc: 0.6293 - val_loss: 0.6742 - val_acc: 0.6000 - val_auc: 0.6292\n",
      "Epoch 161/200\n",
      "769/769 [==============================] - 0s 118us/step - loss: 0.6525 - acc: 0.6320 - auc: 0.6293 - val_loss: 0.6751 - val_acc: 0.5970 - val_auc: 0.6294\n",
      "Epoch 162/200\n",
      "769/769 [==============================] - 0s 128us/step - loss: 0.6484 - acc: 0.6411 - auc: 0.6295 - val_loss: 0.6746 - val_acc: 0.6000 - val_auc: 0.6295\n",
      "Epoch 163/200\n",
      "769/769 [==============================] - 0s 115us/step - loss: 0.6532 - acc: 0.6307 - auc: 0.6295 - val_loss: 0.6754 - val_acc: 0.5970 - val_auc: 0.6296\n",
      "Epoch 164/200\n",
      "769/769 [==============================] - 0s 127us/step - loss: 0.6548 - acc: 0.6372 - auc: 0.6296 - val_loss: 0.6756 - val_acc: 0.5970 - val_auc: 0.6296\n",
      "Epoch 165/200\n",
      "769/769 [==============================] - 0s 125us/step - loss: 0.6508 - acc: 0.6242 - auc: 0.6297 - val_loss: 0.6748 - val_acc: 0.5970 - val_auc: 0.6298\n",
      "Epoch 166/200\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.6506 - acc: 0.6320 - auc: 0.6299 - val_loss: 0.6746 - val_acc: 0.5970 - val_auc: 0.6299\n",
      "Epoch 167/200\n",
      "769/769 [==============================] - 0s 128us/step - loss: 0.6496 - acc: 0.6281 - auc: 0.6300 - val_loss: 0.6743 - val_acc: 0.5970 - val_auc: 0.6300\n",
      "Epoch 168/200\n",
      "769/769 [==============================] - 0s 118us/step - loss: 0.6521 - acc: 0.6346 - auc: 0.6301 - val_loss: 0.6742 - val_acc: 0.5970 - val_auc: 0.6301\n",
      "Epoch 169/200\n",
      "769/769 [==============================] - 0s 112us/step - loss: 0.6481 - acc: 0.6476 - auc: 0.6302 - val_loss: 0.6744 - val_acc: 0.5970 - val_auc: 0.6302\n",
      "Epoch 170/200\n",
      "769/769 [==============================] - 0s 113us/step - loss: 0.6588 - acc: 0.6255 - auc: 0.6303 - val_loss: 0.6745 - val_acc: 0.5970 - val_auc: 0.6303\n",
      "Epoch 171/200\n",
      "769/769 [==============================] - 0s 113us/step - loss: 0.6561 - acc: 0.6320 - auc: 0.6303 - val_loss: 0.6747 - val_acc: 0.5970 - val_auc: 0.6303\n",
      "Epoch 172/200\n",
      "769/769 [==============================] - 0s 115us/step - loss: 0.6519 - acc: 0.6385 - auc: 0.6304 - val_loss: 0.6744 - val_acc: 0.5970 - val_auc: 0.6304\n",
      "Epoch 173/200\n",
      "769/769 [==============================] - 0s 113us/step - loss: 0.6486 - acc: 0.6320 - auc: 0.6305 - val_loss: 0.6747 - val_acc: 0.5970 - val_auc: 0.6305\n",
      "Epoch 174/200\n",
      "769/769 [==============================] - 0s 106us/step - loss: 0.6510 - acc: 0.6398 - auc: 0.6306 - val_loss: 0.6744 - val_acc: 0.5970 - val_auc: 0.6306\n",
      "Epoch 175/200\n",
      "769/769 [==============================] - 0s 110us/step - loss: 0.6523 - acc: 0.6333 - auc: 0.6307 - val_loss: 0.6739 - val_acc: 0.5970 - val_auc: 0.6307\n",
      "Epoch 176/200\n",
      "769/769 [==============================] - 0s 106us/step - loss: 0.6480 - acc: 0.6333 - auc: 0.6308 - val_loss: 0.6739 - val_acc: 0.5970 - val_auc: 0.6309\n",
      "Epoch 177/200\n",
      "769/769 [==============================] - 0s 110us/step - loss: 0.6498 - acc: 0.6333 - auc: 0.6309 - val_loss: 0.6742 - val_acc: 0.5970 - val_auc: 0.6310\n",
      "Epoch 178/200\n",
      "769/769 [==============================] - 0s 119us/step - loss: 0.6495 - acc: 0.6320 - auc: 0.6310 - val_loss: 0.6748 - val_acc: 0.5970 - val_auc: 0.6311\n",
      "Epoch 179/200\n",
      "769/769 [==============================] - 0s 106us/step - loss: 0.6578 - acc: 0.6307 - auc: 0.6311 - val_loss: 0.6749 - val_acc: 0.5970 - val_auc: 0.6311\n",
      "Epoch 180/200\n",
      "769/769 [==============================] - 0s 106us/step - loss: 0.6506 - acc: 0.6372 - auc: 0.6311 - val_loss: 0.6753 - val_acc: 0.5970 - val_auc: 0.6312\n",
      "Epoch 181/200\n",
      "769/769 [==============================] - 0s 112us/step - loss: 0.6552 - acc: 0.6333 - auc: 0.6313 - val_loss: 0.6752 - val_acc: 0.5970 - val_auc: 0.6312\n",
      "Epoch 182/200\n",
      "769/769 [==============================] - 0s 110us/step - loss: 0.6483 - acc: 0.6385 - auc: 0.6313 - val_loss: 0.6747 - val_acc: 0.5970 - val_auc: 0.6313\n",
      "Epoch 183/200\n",
      "769/769 [==============================] - 0s 102us/step - loss: 0.6528 - acc: 0.6411 - auc: 0.6314 - val_loss: 0.6745 - val_acc: 0.5970 - val_auc: 0.6314\n",
      "Epoch 184/200\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.6498 - acc: 0.6385 - auc: 0.6315 - val_loss: 0.6746 - val_acc: 0.5970 - val_auc: 0.6315\n",
      "Epoch 185/200\n",
      "769/769 [==============================] - 0s 109us/step - loss: 0.6508 - acc: 0.6294 - auc: 0.6316 - val_loss: 0.6747 - val_acc: 0.6000 - val_auc: 0.6316\n",
      "Epoch 186/200\n",
      "769/769 [==============================] - 0s 104us/step - loss: 0.6489 - acc: 0.6294 - auc: 0.6317 - val_loss: 0.6748 - val_acc: 0.5970 - val_auc: 0.6318\n",
      "Epoch 187/200\n",
      "769/769 [==============================] - 0s 109us/step - loss: 0.6430 - acc: 0.6372 - auc: 0.6319 - val_loss: 0.6761 - val_acc: 0.5970 - val_auc: 0.6320\n",
      "Epoch 188/200\n",
      "769/769 [==============================] - 0s 108us/step - loss: 0.6480 - acc: 0.6359 - auc: 0.6321 - val_loss: 0.6769 - val_acc: 0.5970 - val_auc: 0.6321\n",
      "Epoch 189/200\n",
      "769/769 [==============================] - 0s 104us/step - loss: 0.6501 - acc: 0.6359 - auc: 0.6322 - val_loss: 0.6772 - val_acc: 0.5970 - val_auc: 0.6322\n",
      "Epoch 190/200\n",
      "769/769 [==============================] - 0s 123us/step - loss: 0.6556 - acc: 0.6294 - auc: 0.6322 - val_loss: 0.6764 - val_acc: 0.5970 - val_auc: 0.6322\n",
      "Epoch 191/200\n",
      "769/769 [==============================] - 0s 109us/step - loss: 0.6470 - acc: 0.6346 - auc: 0.6323 - val_loss: 0.6766 - val_acc: 0.5970 - val_auc: 0.6323\n",
      "Epoch 192/200\n",
      "769/769 [==============================] - 0s 112us/step - loss: 0.6505 - acc: 0.6320 - auc: 0.6324 - val_loss: 0.6769 - val_acc: 0.5970 - val_auc: 0.6324\n",
      "Epoch 193/200\n",
      "769/769 [==============================] - 0s 110us/step - loss: 0.6483 - acc: 0.6385 - auc: 0.6325 - val_loss: 0.6771 - val_acc: 0.5970 - val_auc: 0.6325\n",
      "Epoch 194/200\n",
      "769/769 [==============================] - 0s 104us/step - loss: 0.6538 - acc: 0.6372 - auc: 0.6325 - val_loss: 0.6765 - val_acc: 0.5970 - val_auc: 0.6326\n",
      "Epoch 195/200\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.6436 - acc: 0.6385 - auc: 0.6327 - val_loss: 0.6765 - val_acc: 0.5970 - val_auc: 0.6327\n",
      "Epoch 196/200\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.6547 - acc: 0.6411 - auc: 0.6327 - val_loss: 0.6754 - val_acc: 0.5970 - val_auc: 0.6327\n",
      "Epoch 197/200\n",
      "769/769 [==============================] - 0s 118us/step - loss: 0.6556 - acc: 0.6268 - auc: 0.6328 - val_loss: 0.6753 - val_acc: 0.5970 - val_auc: 0.6328\n",
      "Epoch 198/200\n",
      "769/769 [==============================] - 0s 122us/step - loss: 0.6524 - acc: 0.6320 - auc: 0.6328 - val_loss: 0.6751 - val_acc: 0.6000 - val_auc: 0.6329\n",
      "Epoch 199/200\n",
      "769/769 [==============================] - 0s 126us/step - loss: 0.6552 - acc: 0.6307 - auc: 0.6329 - val_loss: 0.6748 - val_acc: 0.6000 - val_auc: 0.6329\n",
      "Epoch 200/200\n",
      "769/769 [==============================] - 0s 118us/step - loss: 0.6565 - acc: 0.6307 - auc: 0.6329 - val_loss: 0.6746 - val_acc: 0.6000 - val_auc: 0.6329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17871a95a90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight2 = {1:1, 0:1}\n",
    "X_s_train, X_s_test, y_s_train, y_s_test = train_test_split(X_s, y_s, test_size=0.3, random_state=42)\n",
    "model_s.fit(X_s_train, y_s_train, batch_size= 32, epochs= 200, verbose=1,  validation_split=0.3, class_weight = class_weight2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 [==============================] - 0s 55us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.66183768187539049, 0.61864406779661019, 0.63295923249196195]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_s.evaluate(X_s_test,y_s_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Exercise 6:\n",
    "(OPTIONAL)\n",
    "\n",
    "Predict the alcohol consumption of the person?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_a = df2['Alcohol']\n",
    "X_a = df2.drop(\"Alcohol\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2 = ['Age', 'Insulin', 'Cholesterol', 'D1Sugar', 'D1Carb', 'D2Sugar' , 'D2Carb','SleepHours']\n",
    "scaler2 = preprocessing.StandardScaler().fit(X_a[cols2] )\n",
    "X_a[cols2]  = scaler2.fit_transform(X_a[cols2])\n",
    "X_a = pd.get_dummies(X_a, prefix = ['Diabetes', 'Race'], columns = ['Diabetes', 'Race'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = Sequential()\n",
    "\n",
    "model_a.add(Dense(input_dim= 16 , units= 32, activation=\"relu\"))  \n",
    "model_a.add(Dropout(0.5))\n",
    "\n",
    "model_a.add(Dense( units= 16, activation=\"relu\"))  \n",
    "model_a.add(Dropout(0.5))\n",
    "\n",
    "model_a.add(Dense( units= 8, activation=\"relu\"))  \n",
    "model_a.add(Dropout(0.5))\n",
    "\n",
    "model_a.add(Dense(units= 1, activation=\"sigmoid\"))\n",
    "\n",
    "model_a.compile(loss=\"mean_squared_error\", optimizer= 'adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 769 samples, validate on 330 samples\n",
      "Epoch 1/50\n",
      "769/769 [==============================] - 1s 1ms/step - loss: 398.7812 - mean_absolute_error: 3.7785 - val_loss: 34.9597 - val_mean_absolute_error: 2.3811\n",
      "Epoch 2/50\n",
      "769/769 [==============================] - 0s 64us/step - loss: 398.4813 - mean_absolute_error: 3.7526 - val_loss: 34.7115 - val_mean_absolute_error: 2.3492\n",
      "Epoch 3/50\n",
      "769/769 [==============================] - 0s 69us/step - loss: 398.0758 - mean_absolute_error: 3.7147 - val_loss: 34.4551 - val_mean_absolute_error: 2.3160\n",
      "Epoch 4/50\n",
      "769/769 [==============================] - 0s 60us/step - loss: 397.8389 - mean_absolute_error: 3.6952 - val_loss: 34.1921 - val_mean_absolute_error: 2.2828\n",
      "Epoch 5/50\n",
      "769/769 [==============================] - 0s 71us/step - loss: 397.5781 - mean_absolute_error: 3.6686 - val_loss: 33.9751 - val_mean_absolute_error: 2.2523\n",
      "Epoch 6/50\n",
      "769/769 [==============================] - 0s 67us/step - loss: 396.9019 - mean_absolute_error: 3.6395 - val_loss: 33.7604 - val_mean_absolute_error: 2.2213\n",
      "Epoch 7/50\n",
      "769/769 [==============================] - 0s 67us/step - loss: 397.0756 - mean_absolute_error: 3.6067 - val_loss: 33.5945 - val_mean_absolute_error: 2.1958\n",
      "Epoch 8/50\n",
      "769/769 [==============================] - 0s 71us/step - loss: 397.0532 - mean_absolute_error: 3.6005 - val_loss: 33.4643 - val_mean_absolute_error: 2.1744\n",
      "Epoch 9/50\n",
      "769/769 [==============================] - 0s 73us/step - loss: 396.9032 - mean_absolute_error: 3.5852 - val_loss: 33.3578 - val_mean_absolute_error: 2.1567\n",
      "Epoch 10/50\n",
      "769/769 [==============================] - 0s 67us/step - loss: 396.3968 - mean_absolute_error: 3.5630 - val_loss: 33.2712 - val_mean_absolute_error: 2.1421\n",
      "Epoch 11/50\n",
      "769/769 [==============================] - 0s 77us/step - loss: 396.5622 - mean_absolute_error: 3.5614 - val_loss: 33.2065 - val_mean_absolute_error: 2.1312\n",
      "Epoch 12/50\n",
      "769/769 [==============================] - 0s 67us/step - loss: 396.4524 - mean_absolute_error: 3.5489 - val_loss: 33.1574 - val_mean_absolute_error: 2.1230\n",
      "Epoch 13/50\n",
      "769/769 [==============================] - 0s 74us/step - loss: 396.2810 - mean_absolute_error: 3.5407 - val_loss: 33.1092 - val_mean_absolute_error: 2.1151\n",
      "Epoch 14/50\n",
      "769/769 [==============================] - 0s 78us/step - loss: 396.0080 - mean_absolute_error: 3.5345 - val_loss: 33.0821 - val_mean_absolute_error: 2.1107\n",
      "Epoch 15/50\n",
      "769/769 [==============================] - 0s 73us/step - loss: 396.0280 - mean_absolute_error: 3.5225 - val_loss: 33.0613 - val_mean_absolute_error: 2.1075\n",
      "Epoch 16/50\n",
      "769/769 [==============================] - 0s 75us/step - loss: 395.7154 - mean_absolute_error: 3.5201 - val_loss: 33.0457 - val_mean_absolute_error: 2.1052\n",
      "Epoch 17/50\n",
      "769/769 [==============================] - 0s 75us/step - loss: 395.8435 - mean_absolute_error: 3.5130 - val_loss: 33.0352 - val_mean_absolute_error: 2.1037\n",
      "Epoch 18/50\n",
      "769/769 [==============================] - 0s 74us/step - loss: 395.7545 - mean_absolute_error: 3.5103 - val_loss: 33.0288 - val_mean_absolute_error: 2.1028\n",
      "Epoch 19/50\n",
      "769/769 [==============================] - 0s 75us/step - loss: 395.7674 - mean_absolute_error: 3.5161 - val_loss: 33.0235 - val_mean_absolute_error: 2.1020\n",
      "Epoch 20/50\n",
      "769/769 [==============================] - 0s 71us/step - loss: 395.5827 - mean_absolute_error: 3.5025 - val_loss: 33.0205 - val_mean_absolute_error: 2.1016\n",
      "Epoch 21/50\n",
      "769/769 [==============================] - 0s 73us/step - loss: 395.8867 - mean_absolute_error: 3.5045 - val_loss: 33.0173 - val_mean_absolute_error: 2.1012\n",
      "Epoch 22/50\n",
      "769/769 [==============================] - 0s 70us/step - loss: 395.5871 - mean_absolute_error: 3.5018 - val_loss: 33.0158 - val_mean_absolute_error: 2.1009\n",
      "Epoch 23/50\n",
      "769/769 [==============================] - 0s 75us/step - loss: 395.6751 - mean_absolute_error: 3.5008 - val_loss: 33.0140 - val_mean_absolute_error: 2.1007\n",
      "Epoch 24/50\n",
      "769/769 [==============================] - 0s 74us/step - loss: 395.5729 - mean_absolute_error: 3.5026 - val_loss: 33.0129 - val_mean_absolute_error: 2.1005\n",
      "Epoch 25/50\n",
      "769/769 [==============================] - 0s 73us/step - loss: 395.5709 - mean_absolute_error: 3.4928 - val_loss: 33.0122 - val_mean_absolute_error: 2.1004\n",
      "Epoch 26/50\n",
      "769/769 [==============================] - 0s 75us/step - loss: 395.5598 - mean_absolute_error: 3.4971 - val_loss: 33.0115 - val_mean_absolute_error: 2.1003\n",
      "Epoch 27/50\n",
      "769/769 [==============================] - 0s 69us/step - loss: 395.5155 - mean_absolute_error: 3.4974 - val_loss: 33.0111 - val_mean_absolute_error: 2.1002\n",
      "Epoch 28/50\n",
      "769/769 [==============================] - 0s 78us/step - loss: 395.5211 - mean_absolute_error: 3.4895 - val_loss: 33.0106 - val_mean_absolute_error: 2.1002\n",
      "Epoch 29/50\n",
      "769/769 [==============================] - 0s 69us/step - loss: 395.5358 - mean_absolute_error: 3.4923 - val_loss: 33.0103 - val_mean_absolute_error: 2.1001\n",
      "Epoch 30/50\n",
      "769/769 [==============================] - 0s 69us/step - loss: 395.5882 - mean_absolute_error: 3.4944 - val_loss: 33.0101 - val_mean_absolute_error: 2.1001\n",
      "Epoch 31/50\n",
      "769/769 [==============================] - 0s 70us/step - loss: 395.4729 - mean_absolute_error: 3.4876 - val_loss: 33.0099 - val_mean_absolute_error: 2.1001\n",
      "Epoch 32/50\n",
      "769/769 [==============================] - 0s 89us/step - loss: 395.5054 - mean_absolute_error: 3.4928 - val_loss: 33.0098 - val_mean_absolute_error: 2.1001\n",
      "Epoch 33/50\n",
      "769/769 [==============================] - 0s 88us/step - loss: 395.4978 - mean_absolute_error: 3.4906 - val_loss: 33.0097 - val_mean_absolute_error: 2.1001\n",
      "Epoch 34/50\n",
      "769/769 [==============================] - 0s 83us/step - loss: 395.5206 - mean_absolute_error: 3.4859 - val_loss: 33.0096 - val_mean_absolute_error: 2.1001\n",
      "Epoch 35/50\n",
      "769/769 [==============================] - 0s 89us/step - loss: 395.4549 - mean_absolute_error: 3.4872 - val_loss: 33.0095 - val_mean_absolute_error: 2.1000\n",
      "Epoch 36/50\n",
      "769/769 [==============================] - 0s 96us/step - loss: 395.5549 - mean_absolute_error: 3.4883 - val_loss: 33.0094 - val_mean_absolute_error: 2.1000\n",
      "Epoch 37/50\n",
      "769/769 [==============================] - 0s 89us/step - loss: 395.4929 - mean_absolute_error: 3.4859 - val_loss: 33.0093 - val_mean_absolute_error: 2.1000\n",
      "Epoch 38/50\n",
      "769/769 [==============================] - 0s 84us/step - loss: 395.5008 - mean_absolute_error: 3.4905 - val_loss: 33.0093 - val_mean_absolute_error: 2.1000\n",
      "Epoch 39/50\n",
      "769/769 [==============================] - 0s 83us/step - loss: 395.4782 - mean_absolute_error: 3.4867 - val_loss: 33.0093 - val_mean_absolute_error: 2.1000\n",
      "Epoch 40/50\n",
      "769/769 [==============================] - 0s 86us/step - loss: 395.9100 - mean_absolute_error: 3.4882 - val_loss: 33.0092 - val_mean_absolute_error: 2.1000\n",
      "Epoch 41/50\n",
      "769/769 [==============================] - 0s 83us/step - loss: 395.5426 - mean_absolute_error: 3.4855 - val_loss: 33.0092 - val_mean_absolute_error: 2.1000\n",
      "Epoch 42/50\n",
      "769/769 [==============================] - 0s 78us/step - loss: 395.5139 - mean_absolute_error: 3.4872 - val_loss: 33.0092 - val_mean_absolute_error: 2.1000\n",
      "Epoch 43/50\n",
      "769/769 [==============================] - 0s 82us/step - loss: 395.5028 - mean_absolute_error: 3.4863 - val_loss: 33.0092 - val_mean_absolute_error: 2.1000\n",
      "Epoch 44/50\n",
      "769/769 [==============================] - 0s 75us/step - loss: 395.4783 - mean_absolute_error: 3.4797 - val_loss: 33.0092 - val_mean_absolute_error: 2.1000\n",
      "Epoch 45/50\n",
      "769/769 [==============================] - 0s 74us/step - loss: 395.4824 - mean_absolute_error: 3.4812 - val_loss: 33.0091 - val_mean_absolute_error: 2.1000\n",
      "Epoch 46/50\n",
      "769/769 [==============================] - 0s 78us/step - loss: 395.4497 - mean_absolute_error: 3.4818 - val_loss: 33.0091 - val_mean_absolute_error: 2.1000\n",
      "Epoch 47/50\n",
      "769/769 [==============================] - 0s 79us/step - loss: 395.4291 - mean_absolute_error: 3.4810 - val_loss: 33.0091 - val_mean_absolute_error: 2.1000\n",
      "Epoch 48/50\n",
      "769/769 [==============================] - 0s 71us/step - loss: 395.4623 - mean_absolute_error: 3.4819 - val_loss: 33.0091 - val_mean_absolute_error: 2.1000\n",
      "Epoch 49/50\n",
      "769/769 [==============================] - 0s 67us/step - loss: 395.6047 - mean_absolute_error: 3.4787 - val_loss: 33.0091 - val_mean_absolute_error: 2.1000\n",
      "Epoch 50/50\n",
      "769/769 [==============================] - 0s 80us/step - loss: 395.6276 - mean_absolute_error: 3.4846 - val_loss: 33.0091 - val_mean_absolute_error: 2.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1787783de10>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_a_train, X_a_test, y_a_train, y_a_test = train_test_split(X_a, y_a, test_size=0.3, random_state= 101)\n",
    "model_a.fit(X_a_train, y_a_train, batch_size= 32, epochs= 50, verbose=1,  validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 [==============================] - 0s 30us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[116.80105278047465, 2.9745788917703142]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_a.evaluate(X_a_test, y_a_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
