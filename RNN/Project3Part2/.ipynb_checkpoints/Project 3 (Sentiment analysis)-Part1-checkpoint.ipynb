{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Text classification\n",
    "\n",
    "In this assignment, we will be using Neural Networks to classify movie reviews as positive or negative. This is commonly referred to as \"sentiment analysis\". While the actual classification might be unique to different tasks you might encounter, the first few steps to setting up a recurrent neural net (just as the steps to building a convolutional neural network) are common to all RNN structures.\n",
    "\n",
    "Goals:\n",
    "- Understand recurrent neural networks\n",
    "- Understand an end to end NN\n",
    "- Implement a recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data if you don't already have it. \n",
    "# Otherwise, keras will load it in.\n",
    "# The data will be saved in ~/.keras/datasets\n",
    "dim = 300\n",
    "\n",
    "(x, y), _ = imdb.load_data(num_words=5000)\n",
    "y_vectorized = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "words = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "Words cannot be simply passed into a neural network. There are several common approaches for turning words into a format neural networks can understand, the most useful of which is associating each word with an \"embedding\". That is, representing the row as a vector of numbers (that can represent arbitrary features). You are free to train your own word embeddings, but it is common to simply take ones generated from huge text corpora.\n",
    "\n",
    "We will be using the GloVe word embeddings in this exercise, as these vectors have been trained from over 6 billion English language tokens. They are free to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_3 import create_word_embeddings\n",
    "try:\n",
    "    with np.load(\n",
    "        os.path.join(\"glove.6B.{}d.trimmed.npz\".format(dim))) as data:\n",
    "        embeddings = data[\"embeddings\"]\n",
    "except FileNotFoundError:\n",
    "    embeddings = create_word_embeddings(\"glove\", words, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Padding\n",
    "\n",
    "Neural networks tend not to handle inputs of different sizes very well. So, with sequences such as sentences or timeseries of varying length, the common approach is to \"pad\" the sequence with 0's on the end so that all inputs are the same length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 562)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences(x[:10]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "\n",
    "As always, we want to build a dense (fully-connected) model as our baseline. However, this is tricky because the number of units in fully connected layers is hard-coded. So, to pass a sequence into this you need to reshape your embedded words into the right shape. Practice this by implementing build_dense_model() in exercise_3.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_3 import build_dense_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_max_sequence_length = max([len(xi) for xi in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_model = build_dense_model(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure your model is predicting the correct dimension:\n",
    "example_prediction = dense_model.predict(pad_sequences(x[:10], 2494))\n",
    "assert example_prediction.shape == (10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples, validate on 10 samples\n",
      "Epoch 1/10\n",
      "10/10 [==============================] - 27s 3s/step - loss: 8.2141 - acc: 0.3000 - val_loss: 9.6709 - val_acc: 0.4000\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 10s 994ms/step - loss: 9.6709 - acc: 0.4000 - val_loss: 9.6709 - val_acc: 0.4000\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 16s 2s/step - loss: 9.6709 - acc: 0.4000 - val_loss: 9.6709 - val_acc: 0.4000\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 13s 1s/step - loss: 9.6709 - acc: 0.4000 - val_loss: 9.6709 - val_acc: 0.4000\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 13s 1s/step - loss: 9.6709 - acc: 0.4000 - val_loss: 9.6709 - val_acc: 0.4000\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 12s 1s/step - loss: 9.6709 - acc: 0.4000 - val_loss: 9.6709 - val_acc: 0.4000\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 11s 1s/step - loss: 9.6709 - acc: 0.4000 - val_loss: 9.6709 - val_acc: 0.4000\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 10s 962ms/step - loss: 9.6709 - acc: 0.4000 - val_loss: 9.6709 - val_acc: 0.4000\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 10s 1s/step - loss: 9.6709 - acc: 0.4000 - val_loss: 9.6709 - val_acc: 0.4000\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 10s 1s/step - loss: 9.6709 - acc: 0.4000 - val_loss: 9.6709 - val_acc: 0.4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x124bc1b00>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will run model fitting and validation on the dev set\n",
    "dense_model.fit(pad_sequences(x[:10], 2494), y_vectorized[:10],\n",
    "                validation_data=(pad_sequences(x[:10], 2494), y_vectorized[:10]),\n",
    "                batch_size=2, epochs=10, \n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "\n",
    "Clearly, the fully connected approach is not adequate for sequence modeling, especially if we run the risk of a future sentence being longer than our maximally allocated sentence length. Furthermore, if a few sentences in our dataset are especially long and most are short, this leads to a lot of unnecessary computation.\n",
    "\n",
    "Enter the Recurrent neural network architecture (specifically, LSTM). Complete build_lstm_model() in exercise_3.py to compare it with the dense network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_3 import build_lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = build_lstm_model(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples, validate on 10 samples\n",
      "Epoch 1/5\n",
      "10/10 [==============================] - 5s 457ms/step - loss: 0.9422 - acc: 0.6000 - val_loss: 0.8049 - val_acc: 0.6000\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 3s 324ms/step - loss: 0.6839 - acc: 0.5000 - val_loss: 0.8003 - val_acc: 0.4000\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 3s 333ms/step - loss: 0.6907 - acc: 0.6000 - val_loss: 0.4542 - val_acc: 0.8000\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 3s 337ms/step - loss: 0.3970 - acc: 0.9000 - val_loss: 0.2581 - val_acc: 0.9000\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 3s 341ms/step - loss: 0.3413 - acc: 0.9000 - val_loss: 0.1346 - val_acc: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x110343f28>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will run model fitting and validation on the dev set\n",
    "lstm_model.fit(pad_sequences(x[:10]), y_vectorized[:10],\n",
    "                validation_data=(pad_sequences(x[:10]), y_vectorized[:10]),\n",
    "                batch_size=2, epochs=5, \n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_3 import build_bidirectional_lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples, validate on 10 samples\n",
      "Epoch 1/5\n",
      "10/10 [==============================] - 6s 611ms/step - loss: 0.8876 - acc: 0.4000 - val_loss: 0.7560 - val_acc: 0.6000\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 4s 402ms/step - loss: 1.2644 - acc: 0.4000 - val_loss: 0.6901 - val_acc: 0.5000\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 4s 403ms/step - loss: 0.6876 - acc: 0.7000 - val_loss: 0.5678 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 4s 419ms/step - loss: 0.5401 - acc: 0.7000 - val_loss: 0.3705 - val_acc: 0.9000\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 4s 413ms/step - loss: 0.2528 - acc: 1.0000 - val_loss: 0.0719 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1278a94a8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_lstm_model = build_bidirectional_lstm_model(embeddings)\n",
    "# this will run model fitting and validation on the dev set\n",
    "bi_lstm_model.fit(pad_sequences(x[:10]), y_vectorized[:10],\n",
    "                validation_data=(pad_sequences(x[:10]), y_vectorized[:10]),\n",
    "                batch_size=2, epochs=5, \n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4:\n",
    "\n",
    "You should notice that the bidirectional LSTM can perfectly overfit the data in under 5 epochs. Let's fit the full model using it. Implement build_final_model() in exercise_3.py to get the best review classification as you can using any RNN techniques discussed in class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_dev, y_train, y_dev = train_test_split(x, y_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_3 import build_final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18750 samples, validate on 6250 samples\n",
      "Epoch 1/1\n",
      " 1792/18750 [=>............................] - ETA: 1:30:36 - loss: 0.7422 - acc: 0.4872"
     ]
    }
   ],
   "source": [
    "final_model = build_final_model(embeddings)\n",
    "# this will run model fitting and validation on the dev set\n",
    "final_model.fit(pad_sequences(x_train), y_train,\n",
    "                validation_data=(pad_sequences(x_dev), y_dev),\n",
    "                batch_size=256, epochs=1, \n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5:\n",
    "Consider your experience implementing a simple RNN. What advantages and disadvantages do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(double click to edit) Or submit a separate file with your write up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
